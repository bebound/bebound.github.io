<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=content-language content="en"><meta name=author content="KK"><meta name=description content="PyTorch provide a simple DQN implementation to solve the cartpole game. However, the code is incorrect, it diverges after training (It has been discussed here).
The official code&rsquo;s training data is below, it&rsquo;s high score is about 50 and finally diverges.
  There are many reason that lead to divergence.
First it use the difference of two frame as input in the tutorial, not only it loss the cart&rsquo;s absolute information(This information is useful, as game will terminate if cart moves too far from centre), but also confused the agent when difference is the same but the state is varied."><meta name=keywords content="fromkk,blog,kk blog,developer,personal,python,golang,go,linux,machine learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Using Dueling DQN to Play Flappy Bird"><meta name=twitter:description content="PyTorch provide a simple DQN implementation to solve the cartpole game. However, the code is incorrect, it diverges after training (It has been discussed here).
The official code&rsquo;s training data is below, it&rsquo;s high score is about 50 and finally diverges.
  There are many reason that lead to divergence.
First it use the difference of two frame as input in the tutorial, not only it loss the cart&rsquo;s absolute information(This information is useful, as game will terminate if cart moves too far from centre), but also confused the agent when difference is the same but the state is varied."><meta property="og:title" content="Using Dueling DQN to Play Flappy Bird"><meta property="og:description" content="PyTorch provide a simple DQN implementation to solve the cartpole game. However, the code is incorrect, it diverges after training (It has been discussed here).
The official code&rsquo;s training data is below, it&rsquo;s high score is about 50 and finally diverges.
  There are many reason that lead to divergence.
First it use the difference of two frame as input in the tutorial, not only it loss the cart&rsquo;s absolute information(This information is useful, as game will terminate if cart moves too far from centre), but also confused the agent when difference is the same but the state is varied."><meta property="og:type" content="article"><meta property="og:url" content="https://www.fromkk.com/posts/using-ddqn-to-play-flappy-bird/"><meta property="article:published_time" content="2019-04-14T17:10:00+08:00"><meta property="article:modified_time" content="2020-04-04T23:12:56+08:00"><base href=https://www.fromkk.com/posts/using-ddqn-to-play-flappy-bird/><title>Using Dueling DQN to Play Flappy Bird · KK's Blog (fromkk)</title><link rel=canonical href=https://www.fromkk.com/posts/using-ddqn-to-play-flappy-bird/><link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.13.0/css/all.css integrity=sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin=anonymous><link rel=stylesheet href=https://www.fromkk.com/css/coder.min.a4f332213a21ce8eb521670c614470c58923aaaf385e2a73982c31dd7642decb.css integrity="sha256-pPMyITohzo61IWcMYURwxYkjqq84XipzmCwx3XZC3ss=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://www.fromkk.com/css/custom.css><link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-16x16.png sizes=16x16><meta name=generator content="Hugo 0.68.3"></head><body class=colorscheme-light><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://www.fromkk.com/>KK's Blog (fromkk)</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fas fa-bars"></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/tags/>Tags</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/posts/index.xml>RSS</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title>Using Dueling DQN to Play Flappy Bird</h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fas fa-calendar"></i><time datetime=2019-04-14T17:10:00+08:00>04/14/2019</time></span>
<span class=reading-time><i class="fas fa-clock"></i>5-minute read</span></div><div class=tags><i class="fas fa-tag"></i><a href=https://www.fromkk.com/tags/machine-learning/>Machine Learning</a></div></div></header><div><p>PyTorch provide a simple DQN implementation to solve the cartpole game. However, the code is incorrect, it diverges after training (It has been discussed <a href=https://discuss.pytorch.org/t/dqn-example-from-pytorch-diverged/4123>here</a>).</p><p>The official code&rsquo;s training data is below, it&rsquo;s high score is about 50 and finally diverges.</p><figure><img src=https://www.fromkk.com/images/ddqn_official.png width=400></figure><p>There are many reason that lead to divergence.</p><p>First it use the difference of two frame as input in the tutorial, not only it loss the cart&rsquo;s absolute information(This information is useful, as game will terminate if cart moves too far from centre), but also confused the agent when difference is the same but the state is varied.</p><p>Second, small replay memory. If the memory is too small, the agent will forget the strategy it has token in some state. I&rsquo;m not sure whether <code>10000</code> memory is big enough, but I suggest using a higher value.</p><p>Third, the parameters. <code>learning_rate</code>, <code>target_update_interval</code> may cause fluctuation. Here is a example on <a href=https://stackoverflow.com/questions/49837204/performance-fluctuates-as-it-is-trained-with-dqn>Stack Overflow</a>. I also met this problem when training cartpole agent. The reward stops growing after 1000 episode.</p><figure><img src=https://www.fromkk.com/images/ddqn_cartpole_fluctuate.png width=400></figure><p>After doing some research on the cartpole DNQ code, I managed to made a model to play the flappy bird. Here are the changes from the original cartpole code. Most of the technology can be found in these two papers: <a href=https://arxiv.org/abs/1312.5602>Playing Atari with Deep Reinforcement Learning</a> and <a href=https://arxiv.org/abs/1710.02298>Rainbow: Combining Improvements in Deep Reinforcement Learning</a>.</p><p>Here is the model architecture:</p><figure><img src=https://www.fromkk.com/images/ddqn_model.png width=600></figure><p>Here is a trained result:</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/NV82ZUQynuQ style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><ol><li><p>Dueling DQN</p><p>The vanilla DQN has the overestimate problem. As the <code>max</code> function will accumulate the noise when training. This leads to converging at suboptimal point. Two following architectures are submitted to solve this problem.</p><p>\[ Q(s, a) = r + \gamma \max_{a&rsquo;}[Q(s&rsquo;, a&rsquo;)] \]</p><p>Double DQN was published two year later DQN. It has two value function, one is used to choose the action with max Q value, another one is used to calculate the Q value of this action.</p><p>\[ a^{max}(S&rsquo;_j, w) = \arg\max_{a&rsquo;}Q(\phi(S&rsquo;_j),a,w) \]</p><p>\[ Q(s,a) = r + \gamma Q&rsquo;(\phi(S&rsquo;_j),a^{max}(S&rsquo;_j, w),w&rsquo;) \]</p><p>Dueling DQN is another solution. It has two estimator, one estimates the score of current state, another estimates the action score.</p><p>\[Q(s, a) = r + \gamma( \max_{a’}[A(s&rsquo;,a&rsquo;)+V(s&rsquo;)]\]</p><p>In order to distinguish the score of the actions, the return the Q-value will minus the mean action score:</p><p><code>x=val+adv-adv.mean(1,keepdim=True)</code></p><figure><img src=https://www.fromkk.com/images/ddqn_duel_dqn.png width=400></figure><p>In this project, I use dueling DQN.</p></li><li><p>Image processing</p><p>I grayscale and crop the image.</p></li><li><p>Stack frames</p><p>I use the last 4 frame as the input. This should help the agent to know the change of environment.</p></li><li><p>Extra FC before last layer</p><p>I add a FC between the image features and the FC for calculate Q-Value.</p></li><li><p>Frame Skipping</p><p>Frame-skipping means agent sees and selects actions on every k frame instead of every frame, the last action is repeated on skipped frames. This method will accelerate the training procedure. In this project, I use <code>frame_skipping=2</code>, as the more the frame skipping is, the more the bird is likely to hit the pipe. And this method did help the agent to converge faster. More details can be found in this <a href=https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/>post</a>.</p></li><li><p>Prioritized Experience Replay</p><p>This idea was published <a href=https://arxiv.org/abs/1511.05952>here</a>. It&rsquo;s a very simple idea: replay high TD error experience more frequently. My code implementation is not efficient. But in cartpole game, this technology help the agent converge faster.</p></li><li><p>Colab and Kaggle Kernel</p><p>My MacBook doesn&rsquo;t support CUDA, so I use these two website to train the model. Here are the comparison of them. During training, Kaggle seems more stable, Colab usually disconnected after 1h.</p><table><thead><tr><th></th><th>Colab</th><th>Kaggle Kernel</th></tr></thead><tbody><tr><td>GPU</td><td>Tesla T4(16G)</td><td>Tesla P100(16G)</td></tr><tr><td>RAM</td><td>13G</td><td>13G</td></tr><tr><td>Max training time</td><td>12h</td><td>9h</td></tr><tr><td>Export trained model</td><td>Google Drive</td><td>-</td></tr></tbody></table></li></ol><hr><p>The lesson I learnt from this project is patience. It takes a long time(maybe hundreds of thousand steps) to see whether this model works, and there are so many parameters can effect the final performance. It takes me about 3 weeks to build the final model. So if you want to build your own model, be patient and good luck. Here are two articles talking about the debugging and hyperparameter tuning in DQN:</p><ul><li><a href=https://adgefficiency.com/dqn-debugging/>DQN debugging using Open AI gym Cartpole</a></li><li><a href=https://adgefficiency.com/dqn-tuning/>DDQN hyperparameter tuning using Open AI gym Cartpole</a></li></ul><p>Here are something may help with this task.</p><ul><li><p><a href=https://www.tensorflow.org/guide/summaries%5Fand%5Ftensorboard>TensorBoard</a></p><p>It&rsquo;s a visualization tool made by TensorFlow Team. It&rsquo;s more convenient to use it rather than generate graph manually by matplotlib. Besides <code>reward</code> and <code>mean_q</code>, these variable are also useful when debugging: TD-error, loss and action_distribution, avg_priority.</p></li><li><p>Advanced image pre-processing</p><p>In this project, I just grayscalize the image. A more advance technology such as binarize should help agent to filter unimportant detail of game output.</p><figure><img src=https://www.fromkk.com/images/ddqn_binary_preprocessing.png width=100></figure><p>In <a href=https://sarvagyavaish.github.io/FlappyBirdRL/>Flappy Bird RL</a>, the author extract the vertical distance from lower pipe and horizontal distance from next pair of pipes as state. The trained agent can achieve 3000 score.</p><figure><img src=https://www.fromkk.com/images/ddqn_extract_feature.png width=200></figure></li></ul><ul><li><p>Other Improvements</p><p><a href=https://arxiv.org/abs/1710.02298>Rainbow</a> introduce many other extensions to enhance DQN, some of them have been discussed in this post.</p><figure><img src=https://www.fromkk.com/images/ddqn_rainbow.png width=400></figure></li></ul><p>I&rsquo;ve uploaded code to this <a href=https://github.com/bebound/flappy-bird-dqn>repo</a>.</p><h2 id=ref>Ref:</h2><ol><li><a href=https://pytorch.org/tutorials/intermediate/reinforcement%5Fq%5Flearning.html>PyTorch REINFORCEMENT LEARNING (DQN) TUTORIAL</a></li><li><a href=https://www.cnblogs.com/pinard/category/1254674.html>强化学习</a> (A series of Chinese post about reinforcement learning)</li><li><a href=http://cs229.stanford.edu/proj2015/362%5Freport.pdf>Deep Reinforcement Learning for Flappy Bird</a></li><li><a href=https://github.com/ttaoREtw/Flappy-Bird-Double-DQN-Pytorch>Flappy-Bird-Double-DQN-Pytorch</a></li><li><a href=https://github.com/qfettes/DeepRL-Tutorials>DeepRL-Tutorials</a></li><li><a href=https://medium.com/mlreview/speeding-up-dqn-on-pytorch-solving-pong-in-30-minutes-81a1bd2dff55>Speeding up DQN on PyTorch: how to solve Pong in 30 minutes</a></li><li><a href=https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/>Frame Skipping and Pre-Processing for Deep Q-Networks on Atari 2600 Games</a></li><li><a href=https://openai.com/blog/openai-baselines-dqn/>OpenAI Baselines: DQN</a></li><li><a href=https://github.com/susantamoh84/Deep-Reinforcement-Learning-Hands-On/>Deep-Reinforcement-Learning-Hands-On</a></li><li><a href=https://github.com/dennybritz/reinforcement-learning/issues/30>DQN solution results peak at ~35 reward</a></li></ol><hr><ul><li><p>Update 26-04-19</p><p>Colab&rsquo;s GPU has upgrade to Tesla T4 from K80, now it becomes my best bet.</p></li><li><p>Update 07-05-19</p><p>TensorBoard is now natively supported in PyTorch after version 1.1</p></li><li><p>Update 26-07-19</p><p>If you run out of RAM in Colab, it will show up an option to double the RAM.</p></li><li><p>Update 13-08-19</p><p>Upload video, update code.</p></li></ul></div><footer><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"kkblog-1"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></footer></article><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js id=MathJax-script></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}};</script></section></div><footer class=footer><section class=container>©
2020
KK
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-153788833-1','auto');ga('send','pageview');}</script></body></html>