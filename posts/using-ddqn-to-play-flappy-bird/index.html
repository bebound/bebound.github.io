<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Using Dueling DQN to Play Flappy Bird - KK's Blog (fromkk)</title><meta name=Description content="fromkk.com is my personal blog, Explore insightful posts on Python, machine learning, and other stuff. keyword: python, machine learning, programming"><meta property="og:url" content="https://fromkk.com/posts/using-ddqn-to-play-flappy-bird/"><meta property="og:site_name" content="KK's Blog (fromkk)"><meta property="og:title" content="Using Dueling DQN to Play Flappy Bird"><meta property="og:description" content="PyTorch provide a simple DQN implementation to solve the cartpole game. However, the code is incorrect, it diverges after training (It has been discussed here).
The official code’s training data is below, it’s high score is about 50 and finally diverges.
There are many reason that lead to divergence.
First it use the difference of two frame as input in the tutorial, not only it loss the cart’s absolute information(This information is useful, as game will terminate if cart moves too far from centre), but also confused the agent when difference is the same but the state is varied."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-04-14T17:10:00+08:00"><meta property="article:modified_time" content="2025-08-10T18:44:05+08:00"><meta property="article:tag" content="Machine Learning"><meta property="og:image" content="https://fromkk.com/images/avatar.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fromkk.com/images/avatar.png"><meta name=twitter:title content="Using Dueling DQN to Play Flappy Bird"><meta name=twitter:description content="PyTorch provide a simple DQN implementation to solve the cartpole game. However, the code is incorrect, it diverges after training (It has been discussed here).
The official code’s training data is below, it’s high score is about 50 and finally diverges.
There are many reason that lead to divergence.
First it use the difference of two frame as input in the tutorial, not only it loss the cart’s absolute information(This information is useful, as game will terminate if cart moves too far from centre), but also confused the agent when difference is the same but the state is varied."><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://fromkk.com/posts/using-ddqn-to-play-flappy-bird/><link rel=prev href=https://fromkk.com/posts/circular-import-in-python/><link rel=next href=https://fromkk.com/posts/preview-latex-in-org-mode-with-emacs-in-macos/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Using Dueling DQN to Play Flappy Bird","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/fromkk.com\/posts\/using-ddqn-to-play-flappy-bird\/"},"genre":"posts","keywords":"Machine Learning","wordcount":980,"url":"https:\/\/fromkk.com\/posts\/using-ddqn-to-play-flappy-bird\/","datePublished":"2019-04-14T17:10:00+08:00","dateModified":"2025-08-10T18:44:05+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"KK"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="KK's Blog (fromkk)"><span class=header-title-pre><img class='logo lazyautosizes ls-is-cached lazyloaded' src=/images/avatar.png></span>KK's Blog (fromkk)</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/about/>About </a><a class=menu-item href=https://github.com/bebound/bebound.github.io title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="KK's Blog (fromkk)"><span class=header-title-pre><img class='logo lazyautosizes ls-is-cached lazyloaded' src=/images/avatar.png></span>KK's Blog (fromkk)</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/about/ title>About</a><a class=menu-item href=https://github.com/bebound/bebound.github.io title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Using Dueling DQN to Play Flappy Bird</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>KK</a></span>&nbsp;<span class=post-category>included in <a href=/categories/machine-learning/><i class="far fa-folder fa-fw" aria-hidden=true></i>Machine-Learning</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2019-04-14>2019-04-14</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;980 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;2 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#ref>Ref</a></li></ul></nav></div></div><div class=content id=content><p>PyTorch provide a simple DQN implementation to solve the cartpole game. However, the code is incorrect, it diverges after training (It has been discussed <a href=https://discuss.pytorch.org/t/dqn-example-from-pytorch-diverged/4123 target=_blank rel="noopener noreffer">here</a>).</p><p>The official code&rsquo;s training data is below, it&rsquo;s high score is about 50 and finally diverges.</p><figure class=image-size-s><img src=/images/ddqn_official.png></figure><p>There are many reason that lead to divergence.</p><p>First it use the difference of two frame as input in the tutorial, not only it loss the cart&rsquo;s absolute information(This information is useful, as game will terminate if cart moves too far from centre), but also confused the agent when difference is the same but the state is varied.</p><p>Second, small replay memory. If the memory is too small, the agent will forget the strategy it has token in some state. I&rsquo;m not sure whether <code>10000</code> memory is big enough, but I suggest using a higher value.</p><p>Third, the parameters. <code>learning_rate</code>, <code>target_update_interval</code> may cause fluctuation. Here is a example on <a href=https://stackoverflow.com/questions/49837204/performance-fluctuates-as-it-is-trained-with-dqn target=_blank rel="noopener noreffer">Stack Overflow</a>. I also met this problem when training cartpole agent. The reward stops growing after 1000 episode.</p><figure class=image-size-s><img src=/images/ddqn_cartpole_fluctuate.png></figure><p>After doing some research on the cartpole DNQ code, I managed to made a model to play the flappy bird. Here are the changes from the original cartpole code. Most of the technology can be found in these two papers: <a href=https://arxiv.org/abs/1312.5602 target=_blank rel="noopener noreffer">Playing Atari with Deep Reinforcement Learning</a> and <a href=https://arxiv.org/abs/1710.02298 target=_blank rel="noopener noreffer">Rainbow: Combining Improvements in Deep Reinforcement Learning</a>.</p><p>Here is the model architecture:</p><figure><img src=/images/ddqn_model.png width=600></figure><p>Here is a trained result:</p><p>{{&lt; youtube NV82ZUQynuQ >}}</p><ol><li><p>Dueling DQN</p><p>The vanilla DQN has the overestimate problem. As the <code>max</code> function will accumulate the noise when training. This leads to converging at suboptimal point. Two following architectures are submitted to solve this problem.</p><p>\[ Q(s, a) = r + \gamma \max_{a&rsquo;}[Q(s&rsquo;, a&rsquo;)] \]</p><p>Double DQN was published two year later DQN. It has two value function, one is used to choose the action with max Q value, another one is used to calculate the Q value of this action.</p><p>\[ a^{max}(S&rsquo;_j, w) = \arg\max_{a&rsquo;}Q(\phi(S&rsquo;_j),a,w) \]</p><p>\[ Q(s,a) = r + \gamma Q&rsquo;(\phi(S&rsquo;_j),a^{max}(S&rsquo;_j, w),w&rsquo;) \]</p><p>Dueling DQN is another solution. It has two estimator, one estimates the score of current state, another estimates the action score.</p><p>\[Q(s, a) = r + \gamma( \max_{a’}[A(s&rsquo;,a&rsquo;)+V(s&rsquo;)]\]</p><p>In order to distinguish the score of the actions, the return the Q-value will minus the mean action score:</p><p><code>x=val+adv-adv.mean(1,keepdim=True)</code></p><figure class=image-size-s><img src=/images/ddqn_duel_dqn.png></figure><p>In this project, I use dueling DQN.</p></li><li><p>Image processing</p><p>I grayscale and crop the image.</p></li><li><p>Stack frames</p><p>I use the last 4 frame as the input. This should help the agent to know the change of environment.</p></li><li><p>Extra FC before last layer</p><p>I add a FC between the image features and the FC for calculate Q-Value.</p></li><li><p>Frame Skipping</p><p>Frame-skipping means agent sees and selects actions on every k frame instead of every frame, the last action is repeated on skipped frames. This method will accelerate the training procedure. In this project, I use <code>frame_skipping=2</code>, as the more the frame skipping is, the more the bird is likely to hit the pipe. And this method did help the agent to converge faster. More details can be found in this <a href=https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/ target=_blank rel="noopener noreffer">post</a>.</p></li><li><p>Prioritized Experience Replay</p><p>This idea was published <a href=https://arxiv.org/abs/1511.05952 target=_blank rel="noopener noreffer">here</a>. It&rsquo;s a very simple idea: replay high TD error experience more frequently. My code implementation is not efficient. But in cartpole game, this technology help the agent converge faster.</p></li><li><p>Colab and Kaggle Kernel</p><p>My MacBook doesn&rsquo;t support CUDA, so I use these two website to train the model. Here are the comparison of them. During training, Kaggle seems more stable, Colab usually disconnected after 1h.</p><table><thead><tr><th></th><th>Colab</th><th>Kaggle Kernel</th></tr></thead><tbody><tr><td>GPU</td><td>Tesla T4(16G)</td><td>Tesla P100(16G)</td></tr><tr><td>RAM</td><td>13G</td><td>13G</td></tr><tr><td>Max training time</td><td>12h</td><td>9h</td></tr><tr><td>Export trained model</td><td>Google Drive</td><td>-</td></tr></tbody></table></li></ol><hr><p>The lesson I learnt from this project is patience. It takes a long time(maybe hundreds of thousand steps) to see whether this model works, and there are so many parameters can effect the final performance. It takes me about 3 weeks to build the final model. So if you want to build your own model, be patient and good luck. Here are two articles talking about the debugging and hyperparameter tuning in DQN:</p><ul><li><a href=https://adgefficiency.com/dqn-debugging/ target=_blank rel="noopener noreffer">DQN debugging using Open AI gym Cartpole</a></li><li><a href=https://adgefficiency.com/dqn-tuning/ target=_blank rel="noopener noreffer">DDQN hyperparameter tuning using Open AI gym Cartpole</a></li></ul><p>Here are something may help with this task.</p><ul><li><p><a href=https://www.tensorflow.org/guide/summaries_and_tensorboard target=_blank rel="noopener noreffer">TensorBoard</a></p><p>It&rsquo;s a visualization tool made by TensorFlow Team. It&rsquo;s more convenient to use it rather than generate graph manually by matplotlib. Besides <code>reward</code> and <code>mean_q</code>, these variable are also useful when debugging: TD-error, loss and action_distribution, avg_priority.</p></li><li><p>Advanced image pre-processing</p><p>In this project, I just grayscalize the image. A more advance technology such as binarize should help agent to filter unimportant detail of game output.</p><figure><img src=/images/ddqn_binary_preprocessing.png width=100></figure><p>In <a href=https://sarvagyavaish.github.io/FlappyBirdRL/ target=_blank rel="noopener noreffer">Flappy Bird RL</a>, the author extract the vertical distance from lower pipe and horizontal distance from next pair of pipes as state. The trained agent can achieve 3000 score.</p><figure><img src=/images/ddqn_extract_feature.png width=200></figure></li></ul><ul><li><p>Other Improvements</p><p><a href=https://arxiv.org/abs/1710.02298 target=_blank rel="noopener noreffer">Rainbow</a> introduce many other extensions to enhance DQN, some of them have been discussed in this post.</p><figure class=image-size-s><img src=/images/ddqn_rainbow.png></figure></li></ul><p>I&rsquo;ve uploaded code to this <a href=https://github.com/bebound/flappy-bird-dqn target=_blank rel="noopener noreffer">repo</a>.</p><h2 id=ref>Ref</h2><ol><li><a href=https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html target=_blank rel="noopener noreffer">PyTorch REINFORCEMENT LEARNING (DQN) TUTORIAL</a></li><li><a href=https://www.cnblogs.com/pinard/category/1254674.html target=_blank rel="noopener noreffer">强化学习</a> (A series of Chinese post about reinforcement learning)</li><li><a href=http://cs229.stanford.edu/proj2015/362_report.pdf target=_blank rel="noopener noreffer">Deep Reinforcement Learning for Flappy Bird</a></li><li><a href=https://github.com/ttaoREtw/Flappy-Bird-Double-DQN-Pytorch target=_blank rel="noopener noreffer">Flappy-Bird-Double-DQN-Pytorch</a></li><li><a href=https://github.com/qfettes/DeepRL-Tutorials target=_blank rel="noopener noreffer">DeepRL-Tutorials</a></li><li><a href=https://medium.com/mlreview/speeding-up-dqn-on-pytorch-solving-pong-in-30-minutes-81a1bd2dff55 target=_blank rel="noopener noreffer">Speeding up DQN on PyTorch: how to solve Pong in 30 minutes</a></li><li><a href=https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/ target=_blank rel="noopener noreffer">Frame Skipping and Pre-Processing for Deep Q-Networks on Atari 2600 Games</a></li><li><a href=https://openai.com/blog/openai-baselines-dqn/ target=_blank rel="noopener noreffer">OpenAI Baselines: DQN</a></li><li><a href=https://github.com/susantamoh84/Deep-Reinforcement-Learning-Hands-On/ target=_blank rel="noopener noreffer">Deep-Reinforcement-Learning-Hands-On</a></li><li><a href=https://github.com/dennybritz/reinforcement-learning/issues/30 target=_blank rel="noopener noreffer">DQN solution results peak at ~35 reward</a></li></ol><hr><ul><li><p>Update 26-04-19</p><p>Colab&rsquo;s GPU has upgrade to Tesla T4 from K80, now it becomes my best bet.</p></li><li><p>Update 07-05-19</p><p>TensorBoard is now natively supported in PyTorch after version 1.1</p></li><li><p>Update 26-07-19</p><p>If you run out of RAM in Colab, it will show up an option to double the RAM.</p></li><li><p>Update 13-08-19</p><p>Upload video, update code.</p></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2025-08-10</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://fromkk.com/posts/using-ddqn-to-play-flappy-bird/ data-title="Using Dueling DQN to Play Flappy Bird" data-hashtags="Machine Learning"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://fromkk.com/posts/using-ddqn-to-play-flappy-bird/ data-hashtag="Machine Learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://fromkk.com/posts/using-ddqn-to-play-flappy-bird/ data-title="Using Dueling DQN to Play Flappy Bird"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://fromkk.com/posts/using-ddqn-to-play-flappy-bird/ data-title="Using Dueling DQN to Play Flappy Bird"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://fromkk.com/posts/using-ddqn-to-play-flappy-bird/ data-title="Using Dueling DQN to Play Flappy Bird"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/machine-learning/>Machine Learning</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/circular-import-in-python/ class=prev rel=prev title="Circular Import in Python"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Circular Import in Python</a>
<a href=/posts/preview-latex-in-org-mode-with-emacs-in-macos/ class=next rel=next title="Preview LaTeX in Org Mode with Emacs in MacOS">Preview LaTeX in Org Mode with Emacs in MacOS<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=utterances class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://utteranc.es/>utterances</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.148.2">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2017 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>KK</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{utterances:{darkTheme:"github-dark",issueTerm:"pathname",label:"utterances",lightTheme:"github-light",repo:"bebound/bebound.github.io"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>