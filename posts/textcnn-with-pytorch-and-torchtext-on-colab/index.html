<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>TextCNN with PyTorch and Torchtext on Colab - KK's Blog (fromkk)</title><meta name=Description content="fromkk.com is my personal blog, Explore insightful posts on Python, machine learning, and other stuff. keyword: python, machine learning, programming"><meta property="og:url" content="https://fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/"><meta property="og:site_name" content="KK's Blog (fromkk)"><meta property="og:title" content="TextCNN with PyTorch and Torchtext on Colab"><meta property="og:description" content="PyTorch is a really powerful framework to build the machine learning models. Although some features is missing when compared with TensorFlow (For example, the early stop function, History to draw plot), its code style is more intuitive.
Torchtext is a NLP package which is also made by pytorch team. It provide a way to read text, processing and iterate the texts.
Google Colab is a Jupyter notebook environment host by Google, you can use free GPU and TPU to run your modal."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-12-03T15:47:00+08:00"><meta property="article:modified_time" content="2025-08-10T18:44:05+08:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="TextCNN"><meta property="og:image" content="https://fromkk.com/images/avatar.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fromkk.com/images/avatar.png"><meta name=twitter:title content="TextCNN with PyTorch and Torchtext on Colab"><meta name=twitter:description content="PyTorch is a really powerful framework to build the machine learning models. Although some features is missing when compared with TensorFlow (For example, the early stop function, History to draw plot), its code style is more intuitive.
Torchtext is a NLP package which is also made by pytorch team. It provide a way to read text, processing and iterate the texts.
Google Colab is a Jupyter notebook environment host by Google, you can use free GPU and TPU to run your modal."><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/><link rel=prev href=https://fromkk.com/posts/csrf-in-django/><link rel=next href=https://fromkk.com/posts/python-dictionary-implementation/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"TextCNN with PyTorch and Torchtext on Colab","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/fromkk.com\/posts\/textcnn-with-pytorch-and-torchtext-on-colab\/"},"genre":"posts","keywords":"Machine Learning, TextCNN","wordcount":638,"url":"https:\/\/fromkk.com\/posts\/textcnn-with-pytorch-and-torchtext-on-colab\/","datePublished":"2018-12-03T15:47:00+08:00","dateModified":"2025-08-10T18:44:05+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"KK"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="KK's Blog (fromkk)"><span class=header-title-pre><img class='logo lazyautosizes ls-is-cached lazyloaded' src=/images/avatar.png></span>KK's Blog (fromkk)</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/about/>About </a><a class=menu-item href=https://github.com/bebound/bebound.github.io title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="KK's Blog (fromkk)"><span class=header-title-pre><img class='logo lazyautosizes ls-is-cached lazyloaded' src=/images/avatar.png></span>KK's Blog (fromkk)</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/about/ title>About</a><a class=menu-item href=https://github.com/bebound/bebound.github.io title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">TextCNN with PyTorch and Torchtext on Colab</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>KK</a></span>&nbsp;<span class=post-category>included in <a href=/categories/machine-learning/><i class="far fa-folder fa-fw" aria-hidden=true></i>Machine-Learning</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2018-12-03>2018-12-03</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;638 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;3 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#ref>Ref</a></li></ul></nav></div></div><div class=content id=content><p><a href=https://pytorch.org target=_blank rel="noopener noreffer">PyTorch</a> is a really powerful framework to build the machine learning models. Although some features is missing when compared with TensorFlow (For example, the early stop function, History to draw plot), its code style is more intuitive.</p><p><a href=https://github.com/pytorch/text target=_blank rel="noopener noreffer">Torchtext</a> is a NLP package which is also made by <code>pytorch</code> team. It provide a way to read text, processing and iterate the texts.</p><p><a href=https://colab.research.google.com target=_blank rel="noopener noreffer">Google Colab</a> is a Jupyter notebook environment host by Google, you can use free GPU and TPU to run your modal.</p><p>Here is a simple tutorial to build a TextCNN modal and run it on Colab.</p><p>The <a href=https://arxiv.org/abs/1408.5882 target=_blank rel="noopener noreffer">TextCNN paper</a> was published by Kim in 2014. The model&rsquo;s idea is pretty simple, but the performance is impressive. If you trying to solve the text classification problem, this model is a good choice to start with.</p><p>The main architecture is shown below:</p><figure class=image-size-s><img src=/images/textcnn.png></figure><p>It uses different kernels to extract text features, then use the softmax regression to classify text base on the features.</p><p>Now we can build this model step by step.</p><p>First build the model. The model I use is CNN-multichannel, which contains two sets of word embedding. Both of them is the copy of word embedding generate from corpus, but only one set will update embedding during training.</p><p>The code is below:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>textCNNMulti</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self,args):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        dim <span style=color:#f92672>=</span> args[<span style=color:#e6db74>&#39;dim&#39;</span>]
</span></span><span style=display:flex><span>        n_class <span style=color:#f92672>=</span> args[<span style=color:#e6db74>&#39;n_class&#39;</span>]
</span></span><span style=display:flex><span>        embedding_matrix<span style=color:#f92672>=</span>args[<span style=color:#e6db74>&#39;embedding_matrix&#39;</span>]
</span></span><span style=display:flex><span>        kernels<span style=color:#f92672>=</span>[<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>]
</span></span><span style=display:flex><span>        kernel_number<span style=color:#f92672>=</span>[<span style=color:#ae81ff>150</span>,<span style=color:#ae81ff>150</span>,<span style=color:#ae81ff>150</span>]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>static_embed <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding<span style=color:#f92672>.</span>from_pretrained(embedding_matrix)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>non_static_embed <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding<span style=color:#f92672>.</span>from_pretrained(embedding_matrix, freeze<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>convs <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList([nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>2</span>, number, (size, dim),padding<span style=color:#f92672>=</span>(size<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>0</span>)) <span style=color:#66d9ef>for</span> (size,number) <span style=color:#f92672>in</span> zip(kernels,kernel_number)])
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dropout<span style=color:#f92672>=</span>nn<span style=color:#f92672>.</span>Dropout()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>out <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(sum(kernel_number), n_class)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        non_static_input <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>non_static_embed(x)
</span></span><span style=display:flex><span>        static_input <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>static_embed(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>stack([non_static_input, static_input], dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> [F<span style=color:#f92672>.</span>relu(conv(x))<span style=color:#f92672>.</span>squeeze(<span style=color:#ae81ff>3</span>) <span style=color:#66d9ef>for</span> conv <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>convs]
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> [F<span style=color:#f92672>.</span>max_pool1d(i, i<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>2</span>))<span style=color:#f92672>.</span>squeeze(<span style=color:#ae81ff>2</span>) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> x]
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat(x, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dropout(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>out(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><p>Second, convert text into word index, so each sentence become a vector for training.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>TEXT <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>Field(lower<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,batch_first<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>LABEL <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>LabelField()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train, val, test <span style=color:#f92672>=</span> datasets<span style=color:#f92672>.</span>SST<span style=color:#f92672>.</span>splits(TEXT, LABEL, <span style=color:#e6db74>&#39;data/&#39;</span>,fine_grained<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>TEXT<span style=color:#f92672>.</span>build_vocab(train, vectors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;glove.840B.300d&#34;</span>)
</span></span><span style=display:flex><span>LABEL<span style=color:#f92672>.</span>build_vocab(train,val,test)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_iter, val_iter, test_iter <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>BucketIterator<span style=color:#f92672>.</span>splits(
</span></span><span style=display:flex><span>    (train, val, test), batch_sizes<span style=color:#f92672>=</span>(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>256</span>),shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p><code>Field</code> defines how to process text, here is the most common parameters:</p><blockquote><p>sequential – Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.</p><p>use_vocab – Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.</p><p>preprocessing – The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor. Default: None.</p><p>batch_first – Whether to produce tensors with the batch dimension first. Default: False.</p></blockquote><p><code>datasets.SST.splits</code> will load the <code>SST</code> datasets, and split into train, validation, and test Dataset objects.</p><p><code>build_vocab</code> will create the Vocab object for Field, which contains the information to convert word into word index and vice versa. Also, the word embedding will save as <code>Field.Vocab.vectors</code>. <code>vectors</code> contains all of the word embedding. Torchtext can download some pretrained vectors automatically, such as <code>glove.840B.300d</code>, <code>fasttext.en.300d</code>. You can also load your vectors in this way, <code>xxx.vec</code> should be the standard word2vec format.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torchtext.vocab <span style=color:#f92672>import</span> Vectors
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vectors <span style=color:#f92672>=</span> Vectors(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;xxx.vec&#39;</span>, cache<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;./&#39;</span>)
</span></span><span style=display:flex><span>TEXT<span style=color:#f92672>.</span>build_vocab(train, val, test, vectors<span style=color:#f92672>=</span>vectors)
</span></span></code></pre></div><p><code>data.BucketIterator.splits</code> will returns iterators that loads batches of data from datasets, and the text in same batch has similar lengths.</p><p>Now, we can start to train the model. First we wrap some parameters into <code>args</code>, it contains settings like output class, learning rate, log interval and so on.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>args<span style=color:#f92672>=</span>{}
</span></span><span style=display:flex><span>args[<span style=color:#e6db74>&#39;vocb_size&#39;</span>]<span style=color:#f92672>=</span>len(TEXT<span style=color:#f92672>.</span>vocab)
</span></span><span style=display:flex><span>args[<span style=color:#e6db74>&#39;dim&#39;</span>]<span style=color:#f92672>=</span><span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>args[<span style=color:#e6db74>&#39;n_class&#39;</span>]<span style=color:#f92672>=</span>len(LABEL<span style=color:#f92672>.</span>vocab)
</span></span><span style=display:flex><span>args[<span style=color:#e6db74>&#39;embedding_matrix&#39;</span>]<span style=color:#f92672>=</span>TEXT<span style=color:#f92672>.</span>vocab<span style=color:#f92672>.</span>vectors
</span></span><span style=display:flex><span>args[<span style=color:#e6db74>&#39;lr&#39;</span>]<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>
</span></span><span style=display:flex><span>args[<span style=color:#e6db74>&#39;momentum&#39;</span>]<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>
</span></span><span style=display:flex><span>args[<span style=color:#e6db74>&#39;epochs&#39;</span>]<span style=color:#f92672>=</span><span style=color:#ae81ff>180</span>
</span></span><span style=display:flex><span>args[<span style=color:#e6db74>&#39;log_interval&#39;</span>]<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>args[<span style=color:#e6db74>&#39;test_interval&#39;</span>]<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>
</span></span><span style=display:flex><span>args[<span style=color:#e6db74>&#39;save_dir&#39;</span>]<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;./&#39;</span>
</span></span></code></pre></div><p>Finally, we can train the model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#f92672>=</span>textCNNMulti(args)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span>args[<span style=color:#e6db74>&#39;lr&#39;</span>],momentum<span style=color:#f92672>=</span>args[<span style=color:#e6db74>&#39;momentum&#39;</span>])
</span></span><span style=display:flex><span>criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>CrossEntropyLoss()
</span></span><span style=display:flex><span>steps<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, args[<span style=color:#e6db74>&#39;epochs&#39;</span>]<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i,data <span style=color:#f92672>in</span> enumerate(train_iter):
</span></span><span style=display:flex><span>        steps<span style=color:#f92672>+=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        x, target <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>text, data<span style=color:#f92672>.</span>label
</span></span><span style=display:flex><span>        x<span style=color:#f92672>=</span>x<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        target<span style=color:#f92672>.</span>sub_(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        target<span style=color:#f92672>=</span>target<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> model(x)
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> criterion(output, target)
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>        loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>step()
</span></span></code></pre></div><p>You can found <code>textcnn.ipynb</code> on <a href=https://github.com/bebound/textcnn target=_blank rel="noopener noreffer">GitHub</a> or <a href="https://colab.research.google.com/drive/1iZE5O0aBEOEhkWNpARqK5u151qrlwJq-#scrollTo=qR-sHoABrMg3&amp;uniqifier=2" target=_blank rel="noopener noreffer">Colab</a>.</p><h2 id=ref>Ref</h2><ol><li><a href=https://arxiv.org/abs/1408.5882 target=_blank rel="noopener noreffer">Convolutional Neural Networks for Sentence Classiﬁcation</a></li><li><a href=http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/ target=_blank rel="noopener noreffer">Understanding Convolutional Neural Networks for NLP</a></li><li><a href=https://torchtext.readthedocs.io/en/latest/data.html target=_blank rel="noopener noreffer">Torchtext Docs</a></li><li><a href=https://github.com/castorini/Castor target=_blank rel="noopener noreffer">Castor</a></li></ol></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2025-08-10</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/ data-title="TextCNN with PyTorch and Torchtext on Colab" data-hashtags="Machine Learning,TextCNN"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/ data-hashtag="Machine Learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/ data-title="TextCNN with PyTorch and Torchtext on Colab"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/ data-title="TextCNN with PyTorch and Torchtext on Colab"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/ data-title="TextCNN with PyTorch and Torchtext on Colab"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/machine-learning/>Machine Learning</a>,&nbsp;<a href=/tags/textcnn/>TextCNN</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/csrf-in-django/ class=prev rel=prev title="CSRF in Django"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>CSRF in Django</a>
<a href=/posts/python-dictionary-implementation/ class=next rel=next title="Python Dictionary Implementation">Python Dictionary Implementation<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=utterances class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://utteranc.es/>utterances</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.154.3">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2017 - 2026</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>KK</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{utterances:{darkTheme:"github-dark",issueTerm:"pathname",label:"utterances",lightTheme:"github-light",repo:"bebound/bebound.github.io"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>