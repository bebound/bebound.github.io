<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=content-language content="en"><meta name=color-scheme content="light dark"><meta name=author content="KK"><meta name=description content="LSTM The avoid the problem of vanishing gradient and exploding gradient in vanilla RNN, LSTM was published, which can remember information for longer periods of time.
Here is the structure of LSTM:
  The calculate procedure are:
\[\begin{aligned} f_t&=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)\\\
i_t&=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\\
o_t&=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\\
\tilde{C_t}&=tanh(W_C\cdot[h_{t-1},x_t]+b_C)\\\
C_t&=f_t\ast C_{t-1}+i_t\ast \tilde{C_t}\\\
h_t&=o_t \ast tanh(C_t) \end{aligned}\]
\(f_t\),\(i_t\),\(o_t\) are forget gate, input gate and output gate respectively. \(\tilde{C_t}\) is the new memory content. \(C_t\) is cell state. \(h_t\) is the output."><meta name=keywords content="fromkk,blog,kk blog,developer,personal,python,golang,go,linux,machine learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="LSTM and GRU"><meta name=twitter:description content="LSTM The avoid the problem of vanishing gradient and exploding gradient in vanilla RNN, LSTM was published, which can remember information for longer periods of time.
Here is the structure of LSTM:
  The calculate procedure are:
\[\begin{aligned} f_t&=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)\\\
i_t&=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\\
o_t&=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\\
\tilde{C_t}&=tanh(W_C\cdot[h_{t-1},x_t]+b_C)\\\
C_t&=f_t\ast C_{t-1}+i_t\ast \tilde{C_t}\\\
h_t&=o_t \ast tanh(C_t) \end{aligned}\]
\(f_t\),\(i_t\),\(o_t\) are forget gate, input gate and output gate respectively. \(\tilde{C_t}\) is the new memory content. \(C_t\) is cell state. \(h_t\) is the output."><meta property="og:title" content="LSTM and GRU"><meta property="og:description" content="LSTM The avoid the problem of vanishing gradient and exploding gradient in vanilla RNN, LSTM was published, which can remember information for longer periods of time.
Here is the structure of LSTM:
  The calculate procedure are:
\[\begin{aligned} f_t&=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)\\\
i_t&=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\\
o_t&=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\\
\tilde{C_t}&=tanh(W_C\cdot[h_{t-1},x_t]+b_C)\\\
C_t&=f_t\ast C_{t-1}+i_t\ast \tilde{C_t}\\\
h_t&=o_t \ast tanh(C_t) \end{aligned}\]
\(f_t\),\(i_t\),\(o_t\) are forget gate, input gate and output gate respectively. \(\tilde{C_t}\) is the new memory content. \(C_t\) is cell state. \(h_t\) is the output."><meta property="og:type" content="article"><meta property="og:url" content="https://www.fromkk.com/posts/lstm-and-gru/"><meta property="article:published_time" content="2018-04-22T14:39:00+08:00"><meta property="article:modified_time" content="2019-11-29T00:29:04+08:00"><title>LSTM and GRU · KK's Blog (fromkk)</title><link rel=canonical href=https://www.fromkk.com/posts/lstm-and-gru/><link rel=preload href="https://www.fromkk.com/fonts/forkawesome-webfont.woff2?v=1.1.7" as=font type=font/woff2 integrity="sha256-hEIt6X6xzye8ubyk8/uxjz68cRZHsJxoKS9fQ8idUGQ=" crossorigin><link rel=stylesheet href=https://www.fromkk.com/css/coder.min.ec198d25949ddd79a670b1ead43ca88e0bc2c1343266d0df0a9eeb7f3f207777.css integrity="sha256-7BmNJZSd3XmmcLHq1DyojgvCwTQyZtDfCp7rfz8gd3c=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://www.fromkk.com/css/coder-dark.min.89c82b6022b96f77aeb521b240daec4f87ea029d84d1c78b8acd0735b91b3c92.css integrity="sha256-icgrYCK5b3eutSGyQNrsT4fqAp2E0ceLis0HNbkbPJI=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://www.fromkk.com/icons/icon-192x192.png><link rel=apple-touch-icon sizes=180x180 href=https://www.fromkk.com/icons/icon-192x192.png><meta name=generator content="Hugo 0.68.3"></head><body class=colorscheme-auto><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://www.fromkk.com/>KK's Blog (fromkk)</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/tags/>Tags</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/posts/index.xml>RSS</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://www.fromkk.com/posts/lstm-and-gru/>LSTM and GRU</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i><time datetime=2018-04-22T14:39:00+08:00>04/22/2018</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>One-minute read</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i><a href=https://www.fromkk.com/tags/machine-learning/>Machine Learning</a>
<span class=separator>•</span>
<a href=https://www.fromkk.com/tags/lstm/>LSTM</a>
<span class=separator>•</span>
<a href=https://www.fromkk.com/tags/gru/>GRU</a></div></div></header><div><h2 id=lstm>LSTM</h2><p>The avoid the problem of vanishing gradient and exploding gradient in vanilla RNN, LSTM was published, which can remember information for longer periods of time.</p><p>Here is the structure of LSTM:</p><figure><img src=https://www.fromkk.com/images/LSTM_LSTM.png width=400></figure><p>The calculate procedure are:</p><p>\[\begin{aligned}
f_t&=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)\\\<br>i_t&=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\\<br>o_t&=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\\<br>\tilde{C_t}&=tanh(W_C\cdot[h_{t-1},x_t]+b_C)\\\<br>C_t&=f_t\ast C_{t-1}+i_t\ast \tilde{C_t}\\\<br>h_t&=o_t \ast tanh(C_t)
\end{aligned}\]</p><p>\(f_t\),\(i_t\),\(o_t\) are forget gate, input gate and output gate respectively. \(\tilde{C_t}\) is the new memory content. \(C_t\) is cell state. \(h_t\) is the output.</p><p>Use \(f_t\) and \(i_t\) to update \(C_t\), use \(o_t\) to decide which part of hidden state should be outputted.</p><h2 id=gru>GRU</h2><figure><img src=https://www.fromkk.com/images/LSTM_GRU.png width=400></figure><p>\[\begin{aligned}
z_t&=\sigma(W_z\cdot[h_{t-1},x_t])\\\<br>r_t&=\sigma(W_r\cdot[h_{t-1},x_t])\\\<br>\tilde{h_t}&=tanh(W\cdot[r_t \ast h_{t-1},x_t])\\\<br>h_t&=(1-z_t)\ast h_{t-1}+z_t \ast \tilde{h_t}
\end{aligned}\]</p><p>\(z_t\) is update gate, \(r_t\) is reset gate, \(\tilde{h_t}\) is candidate activation, \(h_t\) is activation.</p><p>Compare with LSTM, GRU merge cell state and hidden state to one hidden state, and use \(z_t\) to decide how to update the state rather than \(f_t\) and \(i_t\).</p><h2 id=ref>Ref:</h2><ol><li><a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>Understanding LSTM Networks</a></li></ol></div><footer><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"kkblog-1"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></footer></article><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}};</script></section></div><footer class=footer><section class=container>©
2021
KK
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=https://www.fromkk.com/js/dark-mode.min.0213e1773e6d1c5a644f847c67a6f8abac49a3776e2976f6008038af8c5b76a1.js integrity="sha256-AhPhdz5tHFpkT4R8Z6b4q6xJo3duKXb2AIA4r4xbdqE="></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-153788833-1','auto');ga('send','pageview');}</script></body></html>