<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on KK's Blog (fromkk)</title><link>https://www.fromkk.com/posts/</link><description>Recent content in Posts on KK's Blog (fromkk)</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License，please give source if you wish to quote or reproduce.</copyright><lastBuildDate>Sat, 11 Sep 2021 21:12:00 +0800</lastBuildDate><atom:link href="https://www.fromkk.com/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Using JSONField before Django 3.1</title><link>https://www.fromkk.com/posts/using-jsonfield-before-django-3-dot-1/</link><pubDate>Sat, 11 Sep 2021 21:12:00 +0800</pubDate><guid>https://www.fromkk.com/posts/using-jsonfield-before-django-3-dot-1/</guid><description>In Django 3.1, Django support save python data into database as JSON encoded data and it is also possible to make query based on field value in JSONField. The detailed usage can be found here. If you are using older version and want to try this feature. Though there are many packages ported this function, I recommend django-jsonfield-backport.
django-jsonfield-backport This package save data as JSON in database and also support JSON query.</description></item><item><title>Dynamic Allocate Executors when Executing Jobs in Spark</title><link>https://www.fromkk.com/posts/dynamic-allocate-executors-when-executing-jobs-in-spark/</link><pubDate>Sun, 18 Jul 2021 16:52:00 +0800</pubDate><guid>https://www.fromkk.com/posts/dynamic-allocate-executors-when-executing-jobs-in-spark/</guid><description>I wrote a Spark program to process logs. The number of logs always changes as time goes by. To ensure logs can be processed instantly, the number of executors is calculated by the maximum of logs per minutes. As a consequence, the CPU usage is low in executors. In order to decrease resource waste, I tried to find a way to schedule executors during the execution of program.
As shown below, the maximum number of logs per minutes can be a dozen times greater than the minimum number in one day.</description></item><item><title>Improve Kafka throughput</title><link>https://www.fromkk.com/posts/improve-kafka-throughput/</link><pubDate>Fri, 28 May 2021 00:57:00 +0800</pubDate><guid>https://www.fromkk.com/posts/improve-kafka-throughput/</guid><description>Kafka is a high-performance and scalable messaging system. Sometimes when handling big data. The default configuration may limit the maximum performance. In this article, I&amp;rsquo;ll explain how messages are generate and saved in Kafka, and how to improve performance by changing configuration.
Kafka Internals How does Producer Send Messages? In short, messages will assembled into batches (named RecordBatch) and send to broker.
The producer manages some internal queues, and each queue contains RecordBatch that will send to one broker.</description></item><item><title>Fix Error: Cask 'java' is unavailable in Homebrew</title><link>https://www.fromkk.com/posts/fix-error-cask-java-is-unavailable-in-homebrew/</link><pubDate>Sun, 07 Mar 2021 00:10:00 +0800</pubDate><guid>https://www.fromkk.com/posts/fix-error-cask-java-is-unavailable-in-homebrew/</guid><description>After update brew to latest version, when calling cask related command, it always outputs Error: Cask 'java' is unavailable: No Cask with this name exists., such as brew list --cask. However, the brew command works.
After doing some research, I found Java has been moved to homebrew/core. This makes sense now. I installed java by cask, but it&amp;rsquo;s not available now and cask throw this error. If I uninstall java from cask, the error should disappear.</description></item><item><title>Timezone in JVM</title><link>https://www.fromkk.com/posts/timezone-in-jvm/</link><pubDate>Sun, 18 Oct 2020 23:49:00 +0800</pubDate><guid>https://www.fromkk.com/posts/timezone-in-jvm/</guid><description>I wrote a Scala code to get the current time. However, the output is different on the development server and docker.
import java.util.Calendar println(Calendar.getInstance().getTime) On my development server, it outputs Sun Oct 18 18:01:01 CST 2020, but in docker, it print a UTC time.
I guess it related to the timezone setting and do a research, here is the result.
How Did JVM Detect Timezone All of the code can be found in this function: private static synchronized TimeZone setDefaultZone()</description></item><item><title>Using cibuildwheel to Create Python Wheels</title><link>https://www.fromkk.com/posts/using-cibuildwheel-to-create-python-wheels/</link><pubDate>Wed, 29 Jul 2020 22:53:00 +0800</pubDate><guid>https://www.fromkk.com/posts/using-cibuildwheel-to-create-python-wheels/</guid><description>Have you ever tried to install MySQL-python? It contains the C code and need to compile the code while install the package. You have to follow the steps in this articles: Install MySQL and MySQLClient(Python) in MacOS. Things get worse if you are using Windows.
Luckily, as new distribution format Wheel has been published in PEP 427.
The wheel binary package format frees installers from having to know about the build system, saves time by amortizing compile time over many installations, and removes the need to install a build system in the target environment.</description></item><item><title>Retrieve Large Dataset in Elasticsearch</title><link>https://www.fromkk.com/posts/retrieve-large-dataset-in-elasticsearch/</link><pubDate>Sun, 21 Jun 2020 20:33:00 +0800</pubDate><guid>https://www.fromkk.com/posts/retrieve-large-dataset-in-elasticsearch/</guid><description>It&amp;rsquo;s easy to get small dataset from Elasticsearch by using size and from. However, it&amp;rsquo;s impossible to retrieve large dataset in the same way.
Deep Paging Problem As we know it, Elasticsearch data is organised into indexes, which is a logical namespace, and the real data is stored into physical shards. Each shard is an instance of Lucene. There are two kind of shards, primary shards and replica shards. Replica shards is the copy of primary shards in case nodes or shards fail.</description></item><item><title>Program Crash Caused by CPU Instruction</title><link>https://www.fromkk.com/posts/program-crash-caused-by-cpu-instruction/</link><pubDate>Sun, 17 May 2020 17:36:00 +0800</pubDate><guid>https://www.fromkk.com/posts/program-crash-caused-by-cpu-instruction/</guid><description>It&amp;rsquo;s inevitable to dealing with bugs in coding career. The main part of coding are implementing new features, fixing bugs and improving performance. For me, there are two kinds of bugs that is difficult to tackle: those are hard to reproduce, and those occur in code not wrote by you.
Recently, I met a bug which has both features mentioned before. I write a Spark program to analyse the log and cluster them.</description></item><item><title>C-m, RET and Return Key in Emacs</title><link>https://www.fromkk.com/posts/c-m-ret-and-return-key-in-emacs/</link><pubDate>Sat, 11 Apr 2020 21:23:00 +0800</pubDate><guid>https://www.fromkk.com/posts/c-m-ret-and-return-key-in-emacs/</guid><description>I use Emacs to write blog. In the recent update, I found M-RET no longer behave as leader key in org mode, but behave as org-meta-return. And even more strange is that in other mode, it behave as leader key. And M-RET also works in terminal in org mode. In GUI, pressing C-M-m can trigger leader key.
SO I opened this issue, with the help of these friends, the issue has been fixed.</description></item><item><title>Import custom package or module in PySpark</title><link>https://www.fromkk.com/posts/import-custom-package-or-module-in-pyspark/</link><pubDate>Thu, 02 Apr 2020 22:24:00 +0800</pubDate><guid>https://www.fromkk.com/posts/import-custom-package-or-module-in-pyspark/</guid><description>First zip all of the dependencies into zip file like this. Then you can use one of the following methods to import it.
|-- kk.zip | |-- kk.py Using &amp;ndash;py-files in spark-submit When submit spark job, add --py-files=kk.zip parameter. kk.zip will be distributed with the main scrip file, and kk.zip will be inserted at the beginning of PATH environment variable.
Then you can use import kk in your main script file.</description></item><item><title>Time Boundary in InfluxDB Group by Time Statement</title><link>https://www.fromkk.com/posts/time-boundary-in-influxdb-group-by-time-statement/</link><pubDate>Sun, 29 Mar 2020 22:30:00 +0800</pubDate><guid>https://www.fromkk.com/posts/time-boundary-in-influxdb-group-by-time-statement/</guid><description>These days I use InfluxDB to save some time series data. I love these features it provides:
High Performance According to to it&amp;rsquo;s hardware guide, a single node will support more than 750k point write per second, 100 moderate queries per second and 10M series cardinality.
Continuous Queries Simple aggregation can be done by InfluxDB&amp;rsquo;s continuous queries.
Overwrite Duplicated Points If you submit a new point with same measurements, tag set and timestamp, the new data will overwrite the old one.</description></item><item><title>C3 Linearization and Python MRO(Method Resolution Order)</title><link>https://www.fromkk.com/posts/c3-linearization-and-python-mro-method-resolution-order/</link><pubDate>Sat, 14 Mar 2020 17:37:00 +0800</pubDate><guid>https://www.fromkk.com/posts/c3-linearization-and-python-mro-method-resolution-order/</guid><description>Python supports multiple inheritance, its class can be derived from more than one base classes. If the specified attribute or methods was not found in current class, how to decide the search sequence from superclasses? In simple scenario, we know left-to right, bottom to up. But when the inheritance hierarchy become complicated, it&amp;rsquo;s not easy to answer by intuition.
For instance, what&amp;rsquo;s search sequence of class M?
class X:pass class Y: pass class Z:pass class A(X,Y):pass class B(Y,Z):pass class M(B,A,Z):pass The answer is: M, B, A, X, Y, Z, object</description></item><item><title>Difference between Value and Pointer variable in Defer in Go</title><link>https://www.fromkk.com/posts/difference-between-value-and-pointer-variable-in-defer-in-go/</link><pubDate>Thu, 19 Dec 2019 22:33:00 +0800</pubDate><guid>https://www.fromkk.com/posts/difference-between-value-and-pointer-variable-in-defer-in-go/</guid><description>defer is a useful function to do cleanup, as it will execute in LIFO order before the surrounding function returns. If you don&amp;rsquo;t know how it works, sometimes the execution result may confuse you.
How it Works and Why Value or Pointer Receiver Matters I found an interesting code on Stack Overflow:
type X struct { S string } func (x X) Close() { fmt.Println(&amp;#34;Value-Closing&amp;#34;, x.S) } func (x *X) CloseP() { fmt.</description></item><item><title>Near-duplicate with SimHash</title><link>https://www.fromkk.com/posts/near-duplicate-with-simhash/</link><pubDate>Wed, 04 Dec 2019 00:16:00 +0800</pubDate><guid>https://www.fromkk.com/posts/near-duplicate-with-simhash/</guid><description>Before talking about SimHash, let&amp;rsquo;s review some other methods which can also identify duplication.
Longest Common Subsequence(LCS) This is the algorithm used by diff command. It is also edit distance with insertion and deletion as the only two edit operations.
This works good for short strings. However, the algorithm&amp;rsquo;s time complexity is \(O(m*n)\), if two strings&amp;rsquo; lengths are \(m\) and \(n\) respectively. So it&amp;rsquo;s not suitable for large corpus. Also, if two corpus consists of same paragraph but the order is not same.</description></item><item><title>Jaeger Code Structure</title><link>https://www.fromkk.com/posts/jaeger-code-structure/</link><pubDate>Sun, 22 Sep 2019 17:07:00 +0800</pubDate><guid>https://www.fromkk.com/posts/jaeger-code-structure/</guid><description>Here is the main logic for jaeger agent and jaeger collector. (Based on jaeger 1.13.1)
Jaeger Agent Collect UDP packet from 6831 port, convert it to model.Span, send to collector by gRPC
Jaeger Collector Process gRPC or process packet from Zipkin(port 9411).
Jaeger Query Listen gRPC and HTTP request from 16686.</description></item><item><title>The Annotated The Annotated Transformer</title><link>https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/</link><pubDate>Sun, 01 Sep 2019 16:00:00 +0800</pubDate><guid>https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/</guid><description>Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.
First, this is the graph that was referenced by almost all of the post related to Transformer.
Transformer consists of these parts: Input, Encoder*N, Output Input, Decoder*N, Output. I&amp;rsquo;ll explain them step by step.
Input The input word will map to 512 dimension vector.</description></item><item><title>Different Types of Attention</title><link>https://www.fromkk.com/posts/different-types-of-attention/</link><pubDate>Mon, 15 Jul 2019 00:16:00 +0800</pubDate><guid>https://www.fromkk.com/posts/different-types-of-attention/</guid><description>\(s_t\) and \(h_i\) are source hidden states and target hidden state, the shape is (n,1). \(c_t\) is the final context vector, and \(\alpha_{t,s}\) is alignment score.
\[\begin{aligned} c_t&amp;amp;=\sum_{i=1}^n \alpha_{t,s}h_i \\\
\alpha_{t,s}&amp;amp;= \frac{\exp(score(s_t,h_i))}{\sum_{i=1}^n \exp(score(s_t,h_i))} \end{aligned}\]
Global(Soft) VS Local(Hard) Global Attention takes all source hidden states into account, and local attention only use part of the source hidden states.
Content-based VS Location-based Content-based Attention uses both source hidden states and target hidden states, but location-based attention only use source hidden states.</description></item><item><title>Torchtext Snippets</title><link>https://www.fromkk.com/posts/torchtext-snippets/</link><pubDate>Mon, 01 Jul 2019 21:28:00 +0800</pubDate><guid>https://www.fromkk.com/posts/torchtext-snippets/</guid><description>Load separate files data.Field parameters is here.
When calling build_vocab, torchtext will add &amp;lt;unk&amp;gt; in vocabulary list. Set unk_token=None if you want to remove it. If sequential=True (default), it will add &amp;lt;pad&amp;gt; in vocab. &amp;lt;unk&amp;gt; and &amp;lt;pad&amp;gt; will add at the beginning of vocabulary list by default.
LabelField is similar to Field, but it will set sequential=False, unk_token=None and is_target=Ture
INPUT = data.Field(lower=True, batch_first=True) TAG = data.LabelField() train, val, test = data.</description></item><item><title>Build Your Own Tiny Tiny RSS Service</title><link>https://www.fromkk.com/posts/build-your-own-tiny-tiny-rss-service/</link><pubDate>Mon, 10 Jun 2019 00:25:00 +0800</pubDate><guid>https://www.fromkk.com/posts/build-your-own-tiny-tiny-rss-service/</guid><description>After Inoreader change the free plan, which limit the max subscription to 150, I begin to find an alternative. Finally, I found Tiny Tiny RSS. It has a nice website and has the fever API Plugin which was supported by most of the RSS reader APP, so you can read RSS on all of you devices.
This post will tell you how to deploy it on your server.
Prerequisite You need to install Docker and Docker Compose before using docker-compose.</description></item><item><title>Preview LaTeX in Org Mode with Emacs in MacOS</title><link>https://www.fromkk.com/posts/preview-latex-in-org-mode-with-emacs-in-macos/</link><pubDate>Sun, 12 May 2019 20:26:00 +0800</pubDate><guid>https://www.fromkk.com/posts/preview-latex-in-org-mode-with-emacs-in-macos/</guid><description>Using the right Emacs Version I failed to preview LaTeX with emacs-plus. If you have installed d12frosted/emacs-plus, uninstall it and use emacs-mac.
brew tap railwaycat/emacsmacport brew install emacs-mac If you like the fancy spacemacs icon, install it with cask: brew cask install emacs-mac-spacemacs-icon
Install Tex Download and install BasicTeX.pkg here. Add /Library/TeX/texbin to PATH. Install dvisvgm by sudo tlmgr update --self &amp;amp;&amp;amp; sudo tlmgr install dvisvgm collection-fontsrecommended Emacs settings Add TeX related bin to path: (setenv &amp;quot;PATH&amp;quot; (concat (getenv &amp;quot;PATH&amp;quot;) &amp;quot;:/Library/TeX/texbin&amp;quot;)) Tell Org Mode to create svg images: (setq org-latex-create-formula-image-program 'dvisvgm) Now you can see the rendered LaTeX equation by calling org-preview-latex-fragment or using shortcut ,Tx.</description></item><item><title>Using Dueling DQN to Play Flappy Bird</title><link>https://www.fromkk.com/posts/using-ddqn-to-play-flappy-bird/</link><pubDate>Sun, 14 Apr 2019 17:10:00 +0800</pubDate><guid>https://www.fromkk.com/posts/using-ddqn-to-play-flappy-bird/</guid><description>PyTorch provide a simple DQN implementation to solve the cartpole game. However, the code is incorrect, it diverges after training (It has been discussed here).
The official code&amp;rsquo;s training data is below, it&amp;rsquo;s high score is about 50 and finally diverges.
There are many reason that lead to divergence.
First it use the difference of two frame as input in the tutorial, not only it loss the cart&amp;rsquo;s absolute information(This information is useful, as game will terminate if cart moves too far from centre), but also confused the agent when difference is the same but the state is varied.</description></item><item><title>Circular Import in Python</title><link>https://www.fromkk.com/posts/circular-import-in-python/</link><pubDate>Sun, 10 Mar 2019 10:59:00 +0800</pubDate><guid>https://www.fromkk.com/posts/circular-import-in-python/</guid><description>Recently, I found a really good example code for Python circular import, and I&amp;rsquo;d like to record it here.
Here is the code:
1 2 3 4 5 6 7 8 # X.py def X1(): return &amp;#34;x1&amp;#34; from Y import Y2 def X2(): return &amp;#34;x2&amp;#34; 1 2 3 4 5 6 7 8 # Y.py def Y1(): return &amp;#34;y1&amp;#34; from X import X1 def Y2(): return &amp;#34;y2&amp;#34; Guess what will happen if you run python X.</description></item><item><title>Python Dictionary Implementation</title><link>https://www.fromkk.com/posts/python-dictionary-implementation/</link><pubDate>Sun, 17 Feb 2019 21:48:00 +0800</pubDate><guid>https://www.fromkk.com/posts/python-dictionary-implementation/</guid><description>Overview CPython allocation memory to save dictionary, the initial table size is 8, entries are saved as &amp;lt;hash,key,value&amp;gt; in each slot(The slot content changed after Python 3.6). When a new key is added, python use i = hash(key) &amp;amp; mask where mask=table_size-1 to calculate which slot it should be placed. If the slot is occupied, CPython using a probing algorithm to find the empty slot to store new item. When 2/3 of the table is full, the table will be resized.</description></item><item><title>TextCNN with PyTorch and Torchtext on Colab</title><link>https://www.fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/</link><pubDate>Mon, 03 Dec 2018 15:47:00 +0800</pubDate><guid>https://www.fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/</guid><description>PyTorch is a really powerful framework to build the machine learning models. Although some features is missing when compared with TensorFlow (For example, the early stop function, History to draw plot), its code style is more intuitive.
Torchtext is a NLP package which is also made by pytorch team. It provide a way to read text, processing and iterate the texts.
Google Colab is a Jupyter notebook environment host by Google, you can use free GPU and TPU to run your modal.</description></item><item><title>CSRF in Django</title><link>https://www.fromkk.com/posts/csrf-in-django/</link><pubDate>Wed, 07 Nov 2018 13:58:00 +0800</pubDate><guid>https://www.fromkk.com/posts/csrf-in-django/</guid><description>CSRF(Cross-site request forgery) is a way to generate fake user request to target website. For example, on a malicious website A, there is a button, click it will send request to www.B.com/logout. When the user click this button, he will logout from website B unconsciously. Logout is not a big problem, but malicious website can generate more dangerous request like money transfer.
Django CSRF protection Each web framework has different approach to do CSRF protection.</description></item><item><title>Create Node Benchmark in Py2neo</title><link>https://www.fromkk.com/posts/create-node-benchmark-in-py2neo/</link><pubDate>Mon, 05 Nov 2018 15:55:00 +0800</pubDate><guid>https://www.fromkk.com/posts/create-node-benchmark-in-py2neo/</guid><description>Recently, I&amp;rsquo;m working on a neo4j project. I use Py2neo to interact with graph db. Alghough Py2neo is a very pythonic and easy to use, its performance is really poor. Sometimes I have to manually write cypher statement by myself if I can&amp;rsquo;t bear with the slow excution. Here is a small script which I use to compare the performance of 4 diffrent ways to insert nodes.
import time from graph_db import graph from py2neo.</description></item><item><title>Deploy Nikola Org Mode on Travis</title><link>https://www.fromkk.com/posts/deploy-nikola-org-mode-on-travis/</link><pubDate>Sat, 03 Nov 2018 21:20:00 +0800</pubDate><guid>https://www.fromkk.com/posts/deploy-nikola-org-mode-on-travis/</guid><description>Recently, I enjoy using Spacemacs, so I decided to switch to org file from Markdown for writing blog. After several attempts, I managed to let Travis convert org file to HTML. Here are the steps.
Install Org Mode plugin First you need to install Org Mode plugin on your computer following the official guide: Nikola orgmode plugin.
Edit conf.el Org Mode will convert to HTML to display on Nikola. Org Mode plugin will call Emacs to do this job.</description></item><item><title>Using Chinese Characters in Matplotlib</title><link>https://www.fromkk.com/posts/using-chinese-characters-in-matplotlib/</link><pubDate>Thu, 04 Oct 2018 15:53:00 +0800</pubDate><guid>https://www.fromkk.com/posts/using-chinese-characters-in-matplotlib/</guid><description>After searching from Google, here is easiest solution. This should also works on other languages:
import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = &amp;#39;retina&amp;#39; import matplotlib.font_manager as fm f = &amp;#34;/System/Library/Fonts/PingFang.ttc&amp;#34; prop = fm.FontProperties(fname=f) plt.title(&amp;#34;你好&amp;#34;,fontproperties=prop) plt.show() Output:</description></item><item><title>LSTM and GRU</title><link>https://www.fromkk.com/posts/lstm-and-gru/</link><pubDate>Sun, 22 Apr 2018 14:39:00 +0800</pubDate><guid>https://www.fromkk.com/posts/lstm-and-gru/</guid><description>LSTM The avoid the problem of vanishing gradient and exploding gradient in vanilla RNN, LSTM was published, which can remember information for longer periods of time.
Here is the structure of LSTM:
The calculate procedure are:
\[\begin{aligned} f_t&amp;amp;=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)\\\
i_t&amp;amp;=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\\
o_t&amp;amp;=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\\
\tilde{C_t}&amp;amp;=tanh(W_C\cdot[h_{t-1},x_t]+b_C)\\\
C_t&amp;amp;=f_t\ast C_{t-1}+i_t\ast \tilde{C_t}\\\
h_t&amp;amp;=o_t \ast tanh(C_t) \end{aligned}\]
\(f_t\),\(i_t\),\(o_t\) are forget gate, input gate and output gate respectively. \(\tilde{C_t}\) is the new memory content. \(C_t\) is cell state. \(h_t\) is the output.</description></item><item><title>Models and Architectures in Word2vec</title><link>https://www.fromkk.com/posts/models-and-architechtures-in-word2vec/</link><pubDate>Fri, 05 Jan 2018 15:14:00 +0800</pubDate><guid>https://www.fromkk.com/posts/models-and-architechtures-in-word2vec/</guid><description>Generally, word2vec is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.
Models CBOW (Continuous Bag of Words) Use the context to predict the probability of current word. (In the picture, the word is encoded with one-hot encoding, \(W_{V*N}\) is word embedding, and \(W_{V*N}^{'}\), the output weight matrix in hidden layer, is same as \(\hat{\upsilon}\) in following equations)</description></item><item><title>Semi-supervised Text Classification Using doc2vec and Label Spreading</title><link>https://www.fromkk.com/posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/</link><pubDate>Sun, 10 Sep 2017 15:29:00 +0800</pubDate><guid>https://www.fromkk.com/posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/</guid><description>Here is a simple way to classify text without much human effort and get a impressive performance.
It can be divided into two steps:
Get train data by using keyword classification Generate a more accurate classification model by using doc2vec and label spreading Keyword-based Classification Keyword based classification is a simple but effective method. Extracting the target keyword is a monotonous work. I use this method to automatic extract keyword candidate.</description></item><item><title>Parameters in doc2vec</title><link>https://www.fromkk.com/posts/parameters-in-dov2vec/</link><pubDate>Thu, 03 Aug 2017 15:20:00 +0800</pubDate><guid>https://www.fromkk.com/posts/parameters-in-dov2vec/</guid><description>Here are some parameter in gensim's doc2vec class.
window window is the maximum distance between the predicted word and context words used for prediction within a document. It will look behind and ahead.
In skip-gram model, if the window size is 2, the training samples will be this:(the blue word is the input word)
min_count If the word appears less than this value, it will be skipped
sample High frequency word like the is useless for training.</description></item><item><title>Brief Introduction of Label Propagation Algorithm</title><link>https://www.fromkk.com/posts/brief-introduction-of-label-propagation-algorithm/</link><pubDate>Sun, 16 Jul 2017 21:45:00 +0800</pubDate><guid>https://www.fromkk.com/posts/brief-introduction-of-label-propagation-algorithm/</guid><description>As I said before, I&amp;rsquo;m working on a text classification project. I use doc2vec to convert text into vectors, then I use LPA to classify the vectors.
LPA is a simple, effective semi-supervised algorithm. It can use the density of unlabeled data to find a hyperplane to split the data.
Here are the main stop of the algorithm:
Let $ (x_1,y1)&amp;hellip;(x_l,y_l)$ be labeled data, $Y_L = \{y_1&amp;hellip;y_l\} $ are the class labels.</description></item><item><title>Enable C Extension for gensim on Windows</title><link>https://www.fromkk.com/posts/enable-c-extension-for-gensim-on-windows/</link><pubDate>Sat, 10 Jun 2017 14:43:00 +0800</pubDate><guid>https://www.fromkk.com/posts/enable-c-extension-for-gensim-on-windows/</guid><description>These days, I’m working on some text classification works, and I use gensim ’s doc2vec function.
When using gensim, it shows this warning message:
C extension not loaded for Word2Vec, training will be slow.
I search this on Internet and found that gensim has rewrite some part of the code using cython rather than numpy to get better performance. A compiler is required to enable this feature.
I tried to install mingw and add it into the path, but it&amp;rsquo;s not working.</description></item><item><title>Some Useful Shell Tools</title><link>https://www.fromkk.com/posts/some-useful-shell-tools/</link><pubDate>Sun, 07 May 2017 15:34:00 +0800</pubDate><guid>https://www.fromkk.com/posts/some-useful-shell-tools/</guid><description>Here are some shell tools I use, which can boost your productivity.
Prezto A zsh configuration framework. Provides auto completion, prompt theme and lots of modules to work with other useful tools. I extremely love the agnoster theme.
Fasd Help you to navigate between folders and launch application.
Here are the official usage example:
v def conf =&amp;gt; vim /some/awkward/path/to/type/default.conf j abc =&amp;gt; cd /hell/of/a/awkward/path/to/get/to/abcdef m movie =&amp;gt; mplayer /whatever/whatever/whatever/awesome_movie.</description></item><item><title>Start</title><link>https://www.fromkk.com/posts/start/</link><pubDate>Tue, 18 Apr 2017 15:46:00 +0800</pubDate><guid>https://www.fromkk.com/posts/start/</guid><description>Over the years, I have read so many programmers’ blogs, which has helped me a lot. Now I think it’s the time to start my own blog.
I hope this can enforce myself to review what I have learned, and it would even be better if someone can benefit from it.</description></item></channel></rss>