<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Models and Architectures in Word2vec - KK's Blog (fromkk)</title><meta name=Description content="fromkk.com is my personal blog, Explore insightful posts on Python, machine learning, and other stuff. keyword: python, machine learning, programming"><meta property="og:url" content="https://fromkk.com/posts/models-and-architechtures-in-word2vec/"><meta property="og:site_name" content="KK's Blog (fromkk)"><meta property="og:title" content="Models and Architectures in Word2vec"><meta property="og:description" content="Generally, word2vec is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.
Models CBOW (Continuous Bag of Words) Use the context to predict the probability of current word. (In the picture, the word is encoded with one-hot encoding, \(W_{V*N}\) is word embedding, and \(W_{V*N}^{’}\), the output weight matrix in hidden layer, is same as \(\hat{\upsilon}\) in following equations)"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-01-05T15:14:00+08:00"><meta property="article:modified_time" content="2025-08-10T18:44:05+08:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Word2vec"><meta property="og:image" content="https://fromkk.com/images/avatar.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fromkk.com/images/avatar.png"><meta name=twitter:title content="Models and Architectures in Word2vec"><meta name=twitter:description content="Generally, word2vec is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.
Models CBOW (Continuous Bag of Words) Use the context to predict the probability of current word. (In the picture, the word is encoded with one-hot encoding, \(W_{V*N}\) is word embedding, and \(W_{V*N}^{’}\), the output weight matrix in hidden layer, is same as \(\hat{\upsilon}\) in following equations)"><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://fromkk.com/posts/models-and-architechtures-in-word2vec/><link rel=prev href=https://fromkk.com/posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/><link rel=next href=https://fromkk.com/posts/lstm-and-gru/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Models and Architectures in Word2vec","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/fromkk.com\/posts\/models-and-architechtures-in-word2vec\/"},"genre":"posts","keywords":"Machine Learning, word2vec","wordcount":563,"url":"https:\/\/fromkk.com\/posts\/models-and-architechtures-in-word2vec\/","datePublished":"2018-01-05T15:14:00+08:00","dateModified":"2025-08-10T18:44:05+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"KK"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="KK's Blog (fromkk)"><span class=header-title-pre><img class='logo lazyautosizes ls-is-cached lazyloaded' src=/images/avatar.png></span>KK's Blog (fromkk)</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/about/>About </a><a class=menu-item href=https://github.com/bebound/bebound.github.io title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="KK's Blog (fromkk)"><span class=header-title-pre><img class='logo lazyautosizes ls-is-cached lazyloaded' src=/images/avatar.png></span>KK's Blog (fromkk)</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/about/ title>About</a><a class=menu-item href=https://github.com/bebound/bebound.github.io title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Models and Architectures in Word2vec</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>KK</a></span>&nbsp;<span class=post-category>included in <a href=/categories/machine-learning/><i class="far fa-folder fa-fw" aria-hidden=true></i>Machine-Learning</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2018-01-05>2018-01-05</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;563 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;2 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#models>Models</a><ul><li><a href=#cbow--continuous-bag-of-words>CBOW (Continuous Bag of Words)</a></li><li><a href=#skip-gram>Skip-Gram</a></li></ul></li><li><a href=#architectures>Architectures</a><ul><li><a href=#hierarchical-softmax>Hierarchical Softmax</a></li><li><a href=#negative-sampling>Negative Sampling</a></li></ul></li><li><a href=#ref>Ref</a></li></ul></nav></div></div><div class=content id=content><p>Generally, <code>word2vec</code> is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.</p><h2 id=models>Models</h2><h3 id=cbow--continuous-bag-of-words>CBOW (Continuous Bag of Words)</h3><p>Use the context to predict the probability of current word. (In the picture, the word is encoded with one-hot encoding, \(W_{V*N}\) is word embedding, and \(W_{V*N}^{&rsquo;}\), the output weight matrix in hidden layer, is same as \(\hat{\upsilon}\) in following equations)</p><figure class=image-size-s><img src=/images/doc2vec_cbow.png></figure><ol><li>Context words&rsquo; vectors are \(\upsilon_{c-n} &mldr; \upsilon_{c+m}\) (\(m\) is the window size)</li><li>Context vector \(\hat{\upsilon}=\frac{\upsilon_{c-m}+\upsilon_{c-m+1}+&mldr;+\upsilon_{c+m}}{2m}\)</li><li>Score vector \(z_i = u_i\hat{\upsilon}\), where \(u_i\) is the output vector representation of word \(\omega_i\)</li><li>Turn scores into probabilities \(\hat{y}=softmax(z)\)</li><li>We desire probabilities \(\hat{y}\) match the true probabilities \(y\).</li></ol><p>We use cross entropy \(H(\hat{y},y)\) to measure the distance between these two distributions.
\[H(\hat{y},y)=-\sum_{j=1}^{\lvert V \rvert}{y_j\log(\hat{y}_j)}\]</p><p>\(y\) and \(\hat{y}\) is accurate, so the loss simplifies to:
\[H(\hat{y},y)=-y_j\log(\hat{y})\]</p><p>For perfect prediction, \(H(\hat{y},y)=-1\log(1)=0\)</p><p>According to this, we can create this loss function:</p><p>\[\begin{aligned}
minimize\ J &=-\log P(\omega_c\lvert \omega_{c-m},&mldr;,\omega_{c-1},&mldr;,\omega_{c+m}) \\
&= -\log P(u_c \lvert \hat{\upsilon}) \\
&= -\log \frac{\exp(u_c^T\hat{\upsilon})}{\sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})} \\
&= -u_c^T\hat{\upsilon}+\log \sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})
\end{aligned}\]</p><h3 id=skip-gram>Skip-Gram</h3><p>Use current word to predict its context.</p><figure class=image-size-s><img src=/images/doc2vec_skip-gram.png></figure><ol><li>We get the input word&rsquo;s vector \(\upsilon_c\)</li><li>Generate \(2m\) score vectors, \(u_{c-m},&mldr;,u_{c-1},&mldr;,u_{c+m}\).</li><li>Turn scores into probabilities \(\hat{y}=softmax(u)\)</li><li>We desire probabilities \(\hat{y}\) match the true probabilities \(y\).</li></ol><p>\[\begin{aligned}
minimize J &=-\log P(\omega_{c-m},&mldr;,\omega_{c-1},\omega_{c+1},&mldr;\omega_{c+m}\lvert \omega_c)\\
&=-\log \prod_{j=0,j\ne m}^{2m}P(\omega_{c-m+j}\lvert \omega_c)\\
&=-\log \prod_{j=0,j\ne m}^{2m}P(u_{c-m+j}\lvert \upsilon_c)\\
&=-\log \prod_{j=0,j\ne m}^{2m}\frac{\exp (u^T_{c-m+j}\upsilon_c)}{\sum_{k=1}^{\lvert V \rvert}{\exp (u^T_k \upsilon_c)}}\\
&=-\sum_{j=0,j\ne m}^{2m}{u^T_{c-m+j}\upsilon_c+2m\log \sum_{k=1}^{\lvert V \rvert} \exp(u^T_k \upsilon_c)}
\end{aligned}\]</p><h2 id=architectures>Architectures</h2><p>Minimize \(J\) is expensive, you need to calculate the probability of each word in vocabulary list. There are two ways to reduce the computation. Hierarchical Softmax and Negative Sampling.</p><h3 id=hierarchical-softmax>Hierarchical Softmax</h3><p>Encode words into a huffman tree, then each word has a Huffman code. The probability of it&rsquo;s probability \(P(w\lvert Context(\omega))\) can change to choose the path from root to the leaf node, each node is a binary classification. Suppose code \(0\) is a positive label, \(1\) is negative label. If the probability of a positive classification is
\[\sigma(X^T_\omega \theta)=\frac{1}{1+e^{-X^T_\omega}}\]</p><p>Then the probability of negative classification is
\[1-\sigma(X^T_\omega \theta)\]</p><figure class=image-size-s><img src=/images/doc2vec_hierarchical_softmax.png></figure><p><code>足球</code>&rsquo;s Huffman code is \(1001\), then it&rsquo;s probability in each node are</p><p>\[\begin{aligned}
p(d_2^\omega\lvert X_\omega,\theta^\omega_1&=1-\sigma(X^T_\omega \theta^\omega_1))\\
p(d^\omega_3\lvert X_\omega,\theta^\omega_2&=\sigma(X^T_\omega \theta^\omega_2))\\
p(d^\omega_4\lvert X_\omega,\theta^\omega_3&=\sigma(X^T_\omega \theta^\omega_3))\\
p(d^\omega_5\lvert X_\omega,\theta^\omega_4&=1-\sigma(X^T_\omega \theta^\omega_4))\\
\end{aligned}\]</p><p>where \(\theta\) is parameter in the node.</p><p>The probability of the <code>足球</code> is the production of these equation.</p><p>Generally,</p><p>\[p(\omega\lvert Context(\omega))=\prod_{j=2}^{l\omega}p(d^\omega_j\lvert X_\omega,\theta^\omega_{j-1})\]</p><p>This reduce the calculation complexity to \(log(n)\) instead of \(n\)</p><h3 id=negative-sampling>Negative Sampling</h3><p>This method will choose some negative sample, then add the probability of the negative word into loss function. The optimisation target becomes maximise the positive words&rsquo; probability and minimise the negative words&rsquo; probability.</p><p>Let \(P(D=0 \lvert \omega,c)\) be the probability that \((\omega,c)\) did not come from the corpus data. Then the objective function will be</p><p>\[\theta = \text{argmax} \prod_{(\omega,c)\in D} P(D=1\lvert \omega,c,\theta) \prod_{(\omega,c)\in \tilde{D}} P(D=0\lvert \omega,c,\theta)\]</p><p>where \(\theta\) is the parameters of the model(\(\upsilon\) and \(u\)).</p><hr><ul><li>update 04-04-20</li></ul><p>I found this two articles pretty useful: <a href=https://rohanvarma.me/Word2Vec/ target=_blank rel="noopener noreffer">Language Models, Word2Vec, and Efficient Softmax Approximations</a> and <a href=https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72 target=_blank rel="noopener noreffer">Word2vec from Scratch with NumPy</a>.</p><h2 id=ref>Ref</h2><ol><li><a href=http://www.hankcs.com/nlp/word2vec.html target=_blank rel="noopener noreffer">word2vec 原理推导与代码分析</a></li><li><a href=http://cs224d.stanford.edu/lecture_notes/notes1.pdf target=_blank rel="noopener noreffer">CS 224D: Deep Learning for NLP Lecture Notes: Part I</a></li><li><a href=http://blog.csdn.net/itplus/article/details/37969519 target=_blank rel="noopener noreffer">word2vec 中的数学原理详解（一）目录和前言</a></li></ol></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2025-08-10</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://fromkk.com/posts/models-and-architechtures-in-word2vec/ data-title="Models and Architectures in Word2vec" data-hashtags="Machine Learning,word2vec"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://fromkk.com/posts/models-and-architechtures-in-word2vec/ data-hashtag="Machine Learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://fromkk.com/posts/models-and-architechtures-in-word2vec/ data-title="Models and Architectures in Word2vec"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://fromkk.com/posts/models-and-architechtures-in-word2vec/ data-title="Models and Architectures in Word2vec"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://fromkk.com/posts/models-and-architechtures-in-word2vec/ data-title="Models and Architectures in Word2vec"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/machine-learning/>Machine Learning</a>,&nbsp;<a href=/tags/word2vec/>Word2vec</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/ class=prev rel=prev title="Semi-supervised text classification using doc2vec and label spreading"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Semi-supervised text classification using doc2vec and label spreading</a>
<a href=/posts/lstm-and-gru/ class=next rel=next title="LSTM and GRU">LSTM and GRU<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=utterances class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://utteranc.es/>utterances</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.151.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2017 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>KK</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{utterances:{darkTheme:"github-dark",issueTerm:"pathname",label:"utterances",lightTheme:"github-light",repo:"bebound/bebound.github.io"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>