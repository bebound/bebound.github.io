<!doctype html><html lang=en><head><title>Models and Architectures in Word2vec · KK's Blog (fromkk)</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="KK"><meta name=description content="Generally, word2vec is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.
Models Link to heading CBOW (Continuous Bag of Words) Link to heading Use the context to predict the probability of current word. (In the picture, the word is encoded with one-hot encoding, \(W_{V*N}\) is word embedding, and \(W_{V*N}^{&rsquo;}\), the output weight matrix in hidden layer, is same as \(\hat{\upsilon}\) in following equations)"><meta name=keywords content="fromkk,blog,kk blog,developer,personal,python,golang,go,linux,machine learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Models and Architectures in Word2vec"><meta name=twitter:description content="Generally, word2vec is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.
Models Link to heading CBOW (Continuous Bag of Words) Link to heading Use the context to predict the probability of current word. (In the picture, the word is encoded with one-hot encoding, \(W_{V*N}\) is word embedding, and \(W_{V*N}^{&rsquo;}\), the output weight matrix in hidden layer, is same as \(\hat{\upsilon}\) in following equations)"><meta property="og:title" content="Models and Architectures in Word2vec"><meta property="og:description" content="Generally, word2vec is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.
Models Link to heading CBOW (Continuous Bag of Words) Link to heading Use the context to predict the probability of current word. (In the picture, the word is encoded with one-hot encoding, \(W_{V*N}\) is word embedding, and \(W_{V*N}^{&rsquo;}\), the output weight matrix in hidden layer, is same as \(\hat{\upsilon}\) in following equations)"><meta property="og:type" content="article"><meta property="og:url" content="https://www.fromkk.com/posts/models-and-architechtures-in-word2vec/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-01-05T15:14:00+08:00"><meta property="article:modified_time" content="2020-04-04T23:14:59+08:00"><link rel=canonical href=https://www.fromkk.com/posts/models-and-architechtures-in-word2vec/><link rel=preload href="https://www.fromkk.com/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://www.fromkk.com/css/coder.min.ea4c355c5f9913809f506132a80bf3fab84f2679dee370f334f7385a36d24c38.css integrity="sha256-6kw1XF+ZE4CfUGEyqAvz+rhPJnne43DzNPc4WjbSTDg=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://www.fromkk.com/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=https://www.fromkk.com/images/favicon.svg sizes=any><link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://www.fromkk.com/icons/icon-192x192.png><link rel=apple-touch-icon sizes=180x180 href=https://www.fromkk.com/icons/icon-192x192.png><link rel=manifest href=https://www.fromkk.com/site.webmanifest><link rel=mask-icon href=https://www.fromkk.com/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://www.fromkk.com/>KK's Blog (fromkk)</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/tags/>Tags</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/posts/index.xml>RSS</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://www.fromkk.com/posts/models-and-architechtures-in-word2vec/>Models and Architectures in Word2vec</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2018-01-05T15:14:00+08:00>01/05/2018</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
3-minute read</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=https://www.fromkk.com/tags/machine-learning/>Machine Learning</a></span>
<span class=separator>•</span>
<span class=tag><a href=https://www.fromkk.com/tags/word2vec/>word2vec</a></span></div></div></header><div class=post-content><p>Generally, <code>word2vec</code> is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.</p><h2 id=models>Models
<a class=heading-link href=#models><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h3 id=cbow--continuous-bag-of-words>CBOW (Continuous Bag of Words)
<a class=heading-link href=#cbow--continuous-bag-of-words><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Use the context to predict the probability of current word. (In the picture, the word is encoded with one-hot encoding, \(W_{V*N}\) is word embedding, and \(W_{V*N}^{&rsquo;}\), the output weight matrix in hidden layer, is same as \(\hat{\upsilon}\) in following equations)</p><figure><img src=https://www.fromkk.com/images/doc2vec_cbow.png width=400></figure><ol><li>Context words&rsquo; vectors are \(\upsilon_{c-n} &mldr; \upsilon_{c+m}\) (\(m\) is the window size)</li><li>Context vector \(\hat{\upsilon}=\frac{\upsilon_{c-m}+\upsilon_{c-m+1}+&mldr;+\upsilon_{c+m}}{2m}\)</li><li>Score vector \(z_i = u_i\hat{\upsilon}\), where \(u_i\) is the output vector representation of word \(\omega_i\)</li><li>Turn scores into probabilities \(\hat{y}=softmax(z)\)</li><li>We desire probabilities \(\hat{y}\) match the true probabilities \(y\).</li></ol><p>We use cross entropy \(H(\hat{y},y)\) to measure the distance between these two distributions.
\[H(\hat{y},y)=-\sum_{j=1}^{\lvert V \rvert}{y_j\log(\hat{y}_j)}\]</p><p>\(y\) and \(\hat{y}\) is accurate, so the loss simplifies to:
\[H(\hat{y},y)=-y_j\log(\hat{y})\]</p><p>For perfect prediction, \(H(\hat{y},y)=-1\log(1)=0\)</p><p>According to this, we can create this loss function:</p><p>\[\begin{aligned}
minimize\ J &=-\log P(\omega_c\lvert \omega_{c-m},&mldr;,\omega_{c-1},&mldr;,\omega_{c+m}) \\\
&= -\log P(u_c \lvert \hat{\upsilon}) \\\
&= -\log \frac{\exp(u_c^T\hat{\upsilon})}{\sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})} \\\
&= -u_c^T\hat{\upsilon}+\log \sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})
\end{aligned}\]</p><h3 id=skip-gram>Skip-Gram
<a class=heading-link href=#skip-gram><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Use current word to predict its context.</p><figure><img src=https://www.fromkk.com/images/doc2vec_skip-gram.png width=400></figure><ol><li>We get the input word&rsquo;s vector \(\upsilon_c\)</li><li>Generate \(2m\) score vectors, \(u_{c-m},&mldr;,u_{c-1},&mldr;,u_{c+m}\).</li><li>Turn scores into probabilities \(\hat{y}=softmax(u)\)</li><li>We desire probabilities \(\hat{y}\) match the true probabilities \(y\).</li></ol><p>\[\begin{aligned}
minimize J &=-\log P(\omega_{c-m},&mldr;,\omega_{c-1},\omega_{c+1},&mldr;\omega_{c+m}\lvert \omega_c)\\\
&=-\log \prod_{j=0,j\ne m}^{2m}P(\omega_{c-m+j}\lvert \omega_c)\\\
&=-\log \prod_{j=0,j\ne m}^{2m}P(u_{c-m+j}\lvert \upsilon_c)\\\
&=-\log \prod_{j=0,j\ne m}^{2m}\frac{\exp (u^T_{c-m+j}\upsilon_c)}{\sum_{k=1}^{\lvert V \rvert}{\exp (u^T_k \upsilon_c)}}\\\
&=-\sum_{j=0,j\ne m}^{2m}{u^T_{c-m+j}\upsilon_c+2m\log \sum_{k=1}^{\lvert V \rvert} \exp(u^T_k \upsilon_c)}
\end{aligned}\]</p><h2 id=architectures>Architectures
<a class=heading-link href=#architectures><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Minimize \(J\) is expensive, you need to calculate the probability of each word in vocabulary list. There are two ways to reduce the computation. Hierarchical Softmax and Negative Sampling.</p><h3 id=hierarchical-softmax>Hierarchical Softmax
<a class=heading-link href=#hierarchical-softmax><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Encode words into a huffman tree, then each word has a Huffman code. The probability of it&rsquo;s probability \(P(w\lvert Context(\omega))\) can change to choose the path from root to the leaf node, each node is a binary classification. Suppose code \(0\) is a positive label, \(1\) is negative label. If the probability of a positive classification is
\[\sigma(X^T_\omega \theta)=\frac{1}{1+e^{-X^T_\omega}}\]</p><p>Then the probability of negative classification is
\[1-\sigma(X^T_\omega \theta)\]</p><p>\[\begin{aligned}
p(d_2^\omega\lvert X_\omega,\theta^\omega_1&=1-\sigma(X^T_\omega \theta^\omega_1))\\\
p(d^\omega_3\lvert X_\omega,\theta^\omega_2&=\sigma(X^T_\omega \theta^\omega_2))\\\
p(d^\omega_4\lvert X_\omega,\theta^\omega_3&=\sigma(X^T_\omega \theta^\omega_3))\\\
p(d^\omega_5\lvert X_\omega,\theta^\omega_4&=1-\sigma(X^T_\omega \theta^\omega_4))\\\
\end{aligned}\]</p><p>where \(\theta\) is parameter in the node.</p><p>The probability of the <code>足球</code> is the production of these equation.</p><p>Generally,</p><p>\[p(\omega\lvert Context(\omega))=\prod_{j=2}^{l\omega}p(d^\omega_j\lvert X_\omega,\theta^\omega_{j-1})\]</p><p>This reduce the calculation complexity to \(log(n)\) instead of \(n\)</p><h3 id=negative-sampling>Negative Sampling
<a class=heading-link href=#negative-sampling><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>This method will choose some negative sample, then add the probability of the negative word into loss function. The optimisation target becomes maximise the positive words&rsquo; probability and minimise the negative words&rsquo; probability.</p><p>Let \(P(D=0 \lvert \omega,c)\) be the probability that \((\omega,c)\) did not come from the corpus data. Then the objective function will be</p><p>\[\theta = \text{argmax} \prod_{(\omega,c)\in D} P(D=1\lvert \omega,c,\theta) \prod_{(\omega,c)\in \tilde{D}} P(D=0\lvert \omega,c,\theta)\]</p><p>where \(\theta\) is the parameters of the model(\(\upsilon\) and \(u\)).</p><hr><ul><li>update 04-04-20</li></ul><p>I found this two articles pretty useful: <a href=https://rohanvarma.me/Word2Vec/ class=external-link target=_blank rel=noopener>Language Models, Word2Vec, and Efficient Softmax Approximations</a> and <a href=https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72 class=external-link target=_blank rel=noopener>Word2vec from Scratch with NumPy</a>.</p><h2 id=ref>Ref:
<a class=heading-link href=#ref><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ol><li><a href=http://www.hankcs.com/nlp/word2vec.html class=external-link target=_blank rel=noopener>word2vec原理推导与代码分析</a></li><li><a href=http://cs224d.stanford.edu/lecture%5Fnotes/notes1.pdf class=external-link target=_blank rel=noopener>CS 224D: Deep Learning for NLP Lecture Notes: Part I</a></li><li><a href=http://blog.csdn.net/itplus/article/details/37969519 class=external-link target=_blank rel=noopener>word2vec 中的数学原理详解（一）目录和前言</a></li></ol></div><footer><div class=comments><script>let getTheme=window.localStorage&&window.localStorage.getItem("colorscheme"),themeInParams="preferred-color-scheme";getTheme==null&&(themeInParams!==""&&themeInParams!=="auto"?getTheme=themeInParams:getTheme=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");let theme=getTheme==="dark"?"github-dark":"github-light",s=document.createElement("script");s.src="https://utteranc.es/client.js",s.setAttribute("repo","bebound/bebound.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("theme",theme),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(s)</script></div></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2023
KK
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=https://www.fromkk.com/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-153788833-1","auto"),ga("send","pageview"))</script></body></html>