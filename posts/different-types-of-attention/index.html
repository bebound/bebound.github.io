<!doctype html><html lang=en><head><title>Different Types of Attention · KK's Blog (fromkk)
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="KK"><meta name=description content="\(s_t\) and \(h_i\) are source hidden states and target hidden state, the shape is (n,1). \(c_t\) is the final context vector, and \(\alpha_{t,s}\) is alignment score.
\[\begin{aligned} c_t&=\sum_{i=1}^n \alpha_{t,s}h_i \\\ \alpha_{t,s}&= \frac{\exp(score(s_t,h_i))}{\sum_{i=1}^n \exp(score(s_t,h_i))} \end{aligned}\]
Global(Soft) VS Local(Hard) Link to heading Global Attention takes all source hidden states into account, and local attention only use part of the source hidden states.
Content-based VS Location-based Link to heading Content-based Attention uses both source hidden states and target hidden states, but location-based attention only use source hidden states."><meta name=keywords content="fromkk,blog,kk blog,developer,personal,python,golang,go,linux,machine learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Different Types of Attention"><meta name=twitter:description content="\(s_t\) and \(h_i\) are source hidden states and target hidden state, the shape is (n,1). \(c_t\) is the final context vector, and \(\alpha_{t,s}\) is alignment score.
\[\begin{aligned} c_t&=\sum_{i=1}^n \alpha_{t,s}h_i \\\ \alpha_{t,s}&= \frac{\exp(score(s_t,h_i))}{\sum_{i=1}^n \exp(score(s_t,h_i))} \end{aligned}\]
Global(Soft) VS Local(Hard) Link to heading Global Attention takes all source hidden states into account, and local attention only use part of the source hidden states.
Content-based VS Location-based Link to heading Content-based Attention uses both source hidden states and target hidden states, but location-based attention only use source hidden states."><meta property="og:title" content="Different Types of Attention"><meta property="og:description" content="\(s_t\) and \(h_i\) are source hidden states and target hidden state, the shape is (n,1). \(c_t\) is the final context vector, and \(\alpha_{t,s}\) is alignment score.
\[\begin{aligned} c_t&=\sum_{i=1}^n \alpha_{t,s}h_i \\\ \alpha_{t,s}&= \frac{\exp(score(s_t,h_i))}{\sum_{i=1}^n \exp(score(s_t,h_i))} \end{aligned}\]
Global(Soft) VS Local(Hard) Link to heading Global Attention takes all source hidden states into account, and local attention only use part of the source hidden states.
Content-based VS Location-based Link to heading Content-based Attention uses both source hidden states and target hidden states, but location-based attention only use source hidden states."><meta property="og:type" content="article"><meta property="og:url" content="https://www.fromkk.com/posts/different-types-of-attention/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-07-15T00:16:00+08:00"><meta property="article:modified_time" content="2020-04-02T21:08:18+08:00"><link rel=canonical href=https://www.fromkk.com/posts/different-types-of-attention/><link rel=preload href="https://www.fromkk.com/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://www.fromkk.com/css/coder.min.ea4c355c5f9913809f506132a80bf3fab84f2679dee370f334f7385a36d24c38.css integrity="sha256-6kw1XF+ZE4CfUGEyqAvz+rhPJnne43DzNPc4WjbSTDg=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://www.fromkk.com/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=https://www.fromkk.com/images/favicon.svg sizes=any><link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://www.fromkk.com/icons/icon-192x192.png><link rel=apple-touch-icon sizes=180x180 href=https://www.fromkk.com/icons/icon-192x192.png><link rel=manifest href=https://www.fromkk.com/site.webmanifest><link rel=mask-icon href=https://www.fromkk.com/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://www.fromkk.com/>KK's Blog (fromkk)
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/tags/>Tags</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/posts/index.xml>RSS</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://www.fromkk.com/posts/different-types-of-attention/>Different Types of Attention</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2019-07-15T00:16:00+08:00>07/15/2019
</time></span><span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
One-minute read</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=https://www.fromkk.com/tags/machine-learning/>Machine Learning</a></span></div></div></header><div class=post-content><p>\(s_t\) and \(h_i\) are source hidden states and target hidden state, the shape is <code>(n,1)</code>. \(c_t\) is the final context vector, and \(\alpha_{t,s}\) is alignment score.</p><p>\[\begin{aligned}
c_t&=\sum_{i=1}^n \alpha_{t,s}h_i \\\
\alpha_{t,s}&= \frac{\exp(score(s_t,h_i))}{\sum_{i=1}^n \exp(score(s_t,h_i))}
\end{aligned}\]</p><h2 id=global--soft--vs-local--hard>Global(Soft) VS Local(Hard)
<a class=heading-link href=#global--soft--vs-local--hard><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Global Attention takes all source hidden states into account, and local attention only use part of the source hidden states.</p><h2 id=content-based-vs-location-based>Content-based VS Location-based
<a class=heading-link href=#content-based-vs-location-based><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Content-based Attention uses both source hidden states and target hidden states, but location-based attention only use source hidden states.</p><p>Here are several popular attention mechanisms:</p><h4 id=dot-product>Dot-Product
<a class=heading-link href=#dot-product><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>\[score(s_t,h_i)=s_t^Th_i\]</p><h4 id=scaled-dot-product>Scaled Dot-Product
<a class=heading-link href=#scaled-dot-product><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>\[score(s_t,h_i)=\frac{s_t^Th_i}{\sqrt{n}}\]
where n is the vectors dimension. Google&rsquo;s Transformer model has similar scaling factor when calculate self-attention: \(score=\frac{KQ^T}{\sqrt{n}}\)</p><h4 id=location-base>Location-Base
<a class=heading-link href=#location-base><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>\[socre(s_t,h_i)=softmax(W_as_t)\]</p><h4 id=general>General
<a class=heading-link href=#general><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>\[score(s_t,h_i)=s_t^TW_ah_i\]</p><p>\(Wa\)&rsquo;s shape is <code>(n,n)</code></p><h4 id=concat>Concat
<a class=heading-link href=#concat><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>\[score(s_t,h_i)=v_a^Ttanh(W_a[s_t,h_i])\]</p><p>\(v_a\)&rsquo;s shape is <code>(x,1)</code>, and \(Wa\) &rsquo;s shape is <code>(x,x)</code>. This is similar to a neural network with one hidden layer.</p><p>When I doing a slot filling project, I compare these mechanisms. <strong>Concat</strong> attention produce the best result.</p><h2 id=ref>Ref:
<a class=heading-link href=#ref><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ol><li><a href=http://cnyah.com/2017/08/01/attention-variants/ class=external-link target=_blank rel=noopener>Attention Variants</a></li><li><a href=https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html class=external-link target=_blank rel=noopener>Attention? Attention!</a></li><li><a href=https://towardsdatascience.com/attention-seq2seq-with-pytorch-learning-to-invert-a-sequence-34faf4133e53 class=external-link target=_blank rel=noopener>Attention Seq2Seq with PyTorch: learning to invert a sequence</a></li></ol></div><footer><div class=comments><script>let getTheme=window.localStorage&&window.localStorage.getItem("colorscheme"),themeInParams="preferred-color-scheme";getTheme==null&&(themeInParams!==""&&themeInParams!=="auto"?getTheme=themeInParams:getTheme=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");let theme=getTheme==="dark"?"github-dark":"github-light",s=document.createElement("script");s.src="https://utteranc.es/client.js",s.setAttribute("repo","bebound/bebound.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("theme",theme),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(s)</script></div></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2023
KK
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=https://www.fromkk.com/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-153788833-1","auto"),ga("send","pageview"))</script></body></html>