<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=content-language content="en">
<meta name=color-scheme content="light dark">
<meta name=author content="KK">
<meta name=description content="\(s_t\) and \(h_i\) are source hidden states and target hidden state, the shape is (n,1). \(c_t\) is the final context vector, and \(\alpha_{t,s}\) is alignment score.
\[\begin{aligned} c_t&=\sum_{i=1}^n \alpha_{t,s}h_i \\\ \alpha_{t,s}&= \frac{\exp(score(s_t,h_i))}{\sum_{i=1}^n \exp(score(s_t,h_i))} \end{aligned}\]
Global(Soft) VS Local(Hard)    Global Attention takes all source hidden states into account, and local attention only use part of the source hidden states.
Content-based VS Location-based    Content-based Attention uses both source hidden states and target hidden states, but location-based attention only use source hidden states.">
<meta name=keywords content="fromkk,blog,kk blog,developer,personal,python,golang,go,linux,machine learning">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Different Types of Attention">
<meta name=twitter:description content="\(s_t\) and \(h_i\) are source hidden states and target hidden state, the shape is (n,1). \(c_t\) is the final context vector, and \(\alpha_{t,s}\) is alignment score.
\[\begin{aligned} c_t&=\sum_{i=1}^n \alpha_{t,s}h_i \\\ \alpha_{t,s}&= \frac{\exp(score(s_t,h_i))}{\sum_{i=1}^n \exp(score(s_t,h_i))} \end{aligned}\]
Global(Soft) VS Local(Hard)    Global Attention takes all source hidden states into account, and local attention only use part of the source hidden states.
Content-based VS Location-based    Content-based Attention uses both source hidden states and target hidden states, but location-based attention only use source hidden states.">
<meta property="og:title" content="Different Types of Attention">
<meta property="og:description" content="\(s_t\) and \(h_i\) are source hidden states and target hidden state, the shape is (n,1). \(c_t\) is the final context vector, and \(\alpha_{t,s}\) is alignment score.
\[\begin{aligned} c_t&=\sum_{i=1}^n \alpha_{t,s}h_i \\\ \alpha_{t,s}&= \frac{\exp(score(s_t,h_i))}{\sum_{i=1}^n \exp(score(s_t,h_i))} \end{aligned}\]
Global(Soft) VS Local(Hard)    Global Attention takes all source hidden states into account, and local attention only use part of the source hidden states.
Content-based VS Location-based    Content-based Attention uses both source hidden states and target hidden states, but location-based attention only use source hidden states.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://www.fromkk.com/posts/different-types-of-attention/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2019-07-15T00:16:00+08:00">
<meta property="article:modified_time" content="2020-04-02T21:08:18+08:00">
<title>
Different Types of Attention · KK's Blog (fromkk)
</title>
<link rel=canonical href=https://www.fromkk.com/posts/different-types-of-attention/>
<link rel=preload href="https://www.fromkk.com/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin>
<link rel=stylesheet href=https://www.fromkk.com/css/coder.min.d9fddbffe6f27e69985dc5fe0471cdb0e57fbf4775714bc3d847accb08f4a1f6.css integrity="sha256-2f3b/+byfmmYXcX+BHHNsOV/v0d1cUvD2Eesywj0ofY=" crossorigin=anonymous media=screen>
<link rel=stylesheet href=https://www.fromkk.com/css/coder-dark.min.002ee2378e14c7a68f1f0a53d9694ed252090987c4e768023fac694a4fc5f793.css integrity="sha256-AC7iN44Ux6aPHwpT2WlO0lIJCYfE52gCP6xpSk/F95M=" crossorigin=anonymous media=screen>
<link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-32x32.png sizes=32x32>
<link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-16x16.png sizes=16x16>
<link rel=apple-touch-icon href=https://www.fromkk.com/icons/icon-192x192.png>
<link rel=apple-touch-icon sizes=180x180 href=https://www.fromkk.com/icons/icon-192x192.png>
<meta name=generator content="Hugo 0.92.2">
</head>
<body class="preload-transitions colorscheme-auto">
<div class=float-container>
<a id=dark-mode-toggle class=colorscheme-toggle>
<i class="fa fa-adjust fa-fw" aria-hidden=true></i>
</a>
</div>
<main class=wrapper>
<nav class=navigation>
<section class=container>
<a class=navigation-title href=https://www.fromkk.com/>
KK's Blog (fromkk)
</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle>
<i class="fa fa-bars fa-fw" aria-hidden=true></i>
</label>
<ul class=navigation-list>
<li class=navigation-item>
<a class=navigation-link href=https://www.fromkk.com/posts/>Blog</a>
</li>
<li class=navigation-item>
<a class=navigation-link href=https://www.fromkk.com/tags/>Tags</a>
</li>
<li class=navigation-item>
<a class=navigation-link href=https://www.fromkk.com/about/>About</a>
</li>
<li class=navigation-item>
<a class=navigation-link href=https://www.fromkk.com/posts/index.xml>RSS</a>
</li>
</ul>
</section>
</nav>
<div class=content>
<section class="container post">
<article>
<header>
<div class=post-title>
<h1 class=title>
<a class=title-link href=https://www.fromkk.com/posts/different-types-of-attention/>
Different Types of Attention
</a>
</h1>
</div>
<div class=post-meta>
<div class=date>
<span class=posted-on>
<i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2019-07-15T00:16:00+08:00>
07/15/2019
</time>
</span>
<span class=reading-time>
<i class="fa fa-clock-o" aria-hidden=true></i>
One-minute read
</span>
</div>
<div class=tags>
<i class="fa fa-tag" aria-hidden=true></i>
<span class=tag>
<a href=https://www.fromkk.com/tags/machine-learning/>Machine Learning</a>
</span></div>
</div>
</header>
<div>
<p>\(s_t\) and \(h_i\) are source hidden states and target hidden state, the shape is <code>(n,1)</code>. \(c_t\) is the final context vector, and \(\alpha_{t,s}\) is alignment score.</p>
<p>\[\begin{aligned}
c_t&=\sum_{i=1}^n \alpha_{t,s}h_i \\\
\alpha_{t,s}&= \frac{\exp(score(s_t,h_i))}{\sum_{i=1}^n \exp(score(s_t,h_i))}
\end{aligned}\]</p>
<h2 id=global--soft--vs-local--hard>
Global(Soft) VS Local(Hard)
<a class=heading-link href=#global--soft--vs-local--hard>
<i class="fa fa-link" aria-hidden=true></i>
</a>
</h2>
<p>Global Attention takes all source hidden states into account, and local attention only use part of the source hidden states.</p>
<h2 id=content-based-vs-location-based>
Content-based VS Location-based
<a class=heading-link href=#content-based-vs-location-based>
<i class="fa fa-link" aria-hidden=true></i>
</a>
</h2>
<p>Content-based Attention uses both source hidden states and target hidden states, but location-based attention only use source hidden states.</p>
<p>Here are several popular attention mechanisms:</p>
<h4 id=dot-product>
Dot-Product
<a class=heading-link href=#dot-product>
<i class="fa fa-link" aria-hidden=true></i>
</a>
</h4>
<p>\[score(s_t,h_i)=s_t^Th_i\]</p>
<h4 id=scaled-dot-product>
Scaled Dot-Product
<a class=heading-link href=#scaled-dot-product>
<i class="fa fa-link" aria-hidden=true></i>
</a>
</h4>
<p>\[score(s_t,h_i)=\frac{s_t^Th_i}{\sqrt{n}}\]
where n is the vectors dimension. Google&rsquo;s Transformer model has similar scaling factor when calculate self-attention: \(score=\frac{KQ^T}{\sqrt{n}}\)</p>
<h4 id=location-base>
Location-Base
<a class=heading-link href=#location-base>
<i class="fa fa-link" aria-hidden=true></i>
</a>
</h4>
<p>\[socre(s_t,h_i)=softmax(W_as_t)\]</p>
<h4 id=general>
General
<a class=heading-link href=#general>
<i class="fa fa-link" aria-hidden=true></i>
</a>
</h4>
<p>\[score(s_t,h_i)=s_t^TW_ah_i\]</p>
<p>\(Wa\)&rsquo;s shape is <code>(n,n)</code></p>
<h4 id=concat>
Concat
<a class=heading-link href=#concat>
<i class="fa fa-link" aria-hidden=true></i>
</a>
</h4>
<p>\[score(s_t,h_i)=v_a^Ttanh(W_a[s_t,h_i])\]</p>
<p>\(v_a\)&rsquo;s shape is <code>(x,1)</code>, and \(Wa\) &rsquo;s shape is <code>(x,x)</code>. This is similar to a neural network with one hidden layer.</p>
<p>When I doing a slot filling project, I compare these mechanisms. <strong>Concat</strong> attention produce the best result.</p>
<h2 id=ref>
Ref:
<a class=heading-link href=#ref>
<i class="fa fa-link" aria-hidden=true></i>
</a>
</h2>
<ol>
<li><a href=http://cnyah.com/2017/08/01/attention-variants/>Attention Variants</a></li>
<li><a href=https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html>Attention? Attention!</a></li>
<li><a href=https://towardsdatascience.com/attention-seq2seq-with-pytorch-learning-to-invert-a-sequence-34faf4133e53>Attention Seq2Seq with PyTorch: learning to invert a sequence</a></li>
</ol>
</div>
<footer>
<script src=https://utteranc.es/client.js repo=bebound/bebound.github.io issue-term=pathname label=comment theme=preferred-color-scheme crossorigin=anonymous async></script>
</footer>
</article>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:!0},{left:'$',right:'$',display:!1},{left:'\\(',right:'\\)',display:!1},{left:'\\[',right:'\\]',display:!0}]})"></script>
</section>
</div>
<footer class=footer>
<section class=container>
©
2022
KK
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.
</section>
</footer>
</main>
<script src=https://www.fromkk.com/js/coder.min.39a51230dce2ac866c049b52573e38bf60666af4bc63c1bdf203b9b2d95b1cd6.js integrity="sha256-OaUSMNzirIZsBJtSVz44v2BmavS8Y8G98gO5stlbHNY="></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-153788833-1','auto'),ga('send','pageview'))</script>
</body>
</html>