<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>The Annotated The Annotated Transformer - KK's Blog (fromkk)</title><meta name=Description content="fromkk.com is my personal blog, Explore insightful posts on Python, machine learning, and other stuff. keyword: python, machine learning, programming"><meta property="og:url" content="https://fromkk.com/posts/the-annotated-the-annotated-transformer/"><meta property="og:site_name" content="KK's Blog (fromkk)"><meta property="og:title" content="The Annotated The Annotated Transformer"><meta property="og:description" content="Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.
First, this is the graph that was referenced by almost all of the post related to Transformer.
Transformer consists of these parts: Input, Encoder*N, Output Input, Decoder*N, Output. I’ll explain them step by step.
Input The input word will map to 512 dimension vector. Then generate Positional Encoding(PE) and add it to the original embeddings."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-09-01T16:00:00+08:00"><meta property="article:modified_time" content="2025-08-10T18:44:05+08:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Transformer"><meta property="og:image" content="https://fromkk.com/images/avatar.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://fromkk.com/images/avatar.png"><meta name=twitter:title content="The Annotated The Annotated Transformer"><meta name=twitter:description content="Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.
First, this is the graph that was referenced by almost all of the post related to Transformer.
Transformer consists of these parts: Input, Encoder*N, Output Input, Decoder*N, Output. I’ll explain them step by step.
Input The input word will map to 512 dimension vector. Then generate Positional Encoding(PE) and add it to the original embeddings."><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://fromkk.com/posts/the-annotated-the-annotated-transformer/><link rel=prev href=https://fromkk.com/posts/different-types-of-attention/><link rel=next href=https://fromkk.com/posts/jaeger-code-structure/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"The Annotated The Annotated Transformer","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/fromkk.com\/posts\/the-annotated-the-annotated-transformer\/"},"genre":"posts","keywords":"Machine Learning, Transformer","wordcount":792,"url":"https:\/\/fromkk.com\/posts\/the-annotated-the-annotated-transformer\/","datePublished":"2019-09-01T16:00:00+08:00","dateModified":"2025-08-10T18:44:05+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"KK"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="KK's Blog (fromkk)"><span class=header-title-pre><img class='logo lazyautosizes ls-is-cached lazyloaded' src=/images/avatar.png></span>KK's Blog (fromkk)</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/about/>About </a><a class=menu-item href=https://github.com/bebound/bebound.github.io title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="KK's Blog (fromkk)"><span class=header-title-pre><img class='logo lazyautosizes ls-is-cached lazyloaded' src=/images/avatar.png></span>KK's Blog (fromkk)</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/about/ title>About</a><a class=menu-item href=https://github.com/bebound/bebound.github.io title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">The Annotated The Annotated Transformer</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>KK</a></span>&nbsp;<span class=post-category>included in <a href=/categories/machine-learning/><i class="far fa-folder fa-fw" aria-hidden=true></i>Machine-Learning</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2019-09-01>2019-09-01</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;792 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;2 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#input>Input</a><ul><li><a href=#positional-encoding>Positional Encoding</a></li></ul></li><li><a href=#encoder>Encoder</a><ul><li><a href=#multi-head-attention>Multi-Head Attention</a></li><li><a href=#add-and-norm>Add & Norm</a></li><li><a href=#position-wise-feed-forward-network>Position-wise Feed Forward Network</a></li></ul></li><li><a href=#output-input>Output Input</a></li><li><a href=#decoder>Decoder</a><ul><li><a href=#masked-multi-head-attention>Masked Multi-Head Attention</a></li><li><a href=#key-and-value-in-decoder-multi-head-attention-layer>Key and Value in Decoder Multi-Head Attention Layer</a></li></ul></li><li><a href=#output>Output</a></li><li><a href=#ref>Ref</a></li></ul></nav></div></div><div class=content id=content><p>Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.</p><p>First, this is the graph that was referenced by almost all of the post related to Transformer.</p><figure class=image-size-s><img src=/images/transformer_main.png></figure><p>Transformer consists of these parts: Input, Encoder*N, Output Input, Decoder*N, Output. I&rsquo;ll explain them step by step.</p><h2 id=input>Input</h2><p>The input word will map to 512 dimension vector. Then generate Positional Encoding(PE) and add it to the original embeddings.</p><h3 id=positional-encoding>Positional Encoding</h3><p>The transformer model does not contains recurrence and convolution. In order to let the model capture the sequence of input word, it add PE into embeddings.</p><figure><img src=/images/transformer_add_pe.png width=500></figure><p>PE will generate a 512 dimension vector for each position:</p><p>\[\begin{align*}
PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{model}}) \\
PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{model}})
\end{align*}\]
The even and odd dimension use <code>sin</code> and <code>cos</code> function respectively.</p><p>For example, the second word&rsquo;s PE should be: \(sin(2 / 10000^{0 / 512}), cos(2 / 10000^{0 / 512}), sin(2 / 10000^{2 / 512}), cos(2 / 10000^{2 / 512})\text{&mldr;}\)</p><p>The value range of PE is <code>(-1,1)</code>, and each position&rsquo;s PE is slight different, as <code>cos</code> and <code>sin</code> has different frequency. Also, for any fixed offset k, \(PE_{pos+k}\) can be represented as a linear function of \(PE_{pos}\).</p><p>For even dimension, let \(10000^{2i/d_{model}}\) be \(\alpha\), for even dimension:</p><p>\[\begin{aligned}
PE_{pos+k}&=sin((pos+k)/\alpha) \\
&=sin(pos/\alpha)cos(k/\alpha)+cos(pos/\alpha)sin(k/\alpha)\\
&=PE_{pos\_even}K_1+PE_{pos\_odd}K_2
\end{aligned}\]</p><figure><img src=/images/transformer_pe1.png width=500></figure><p>The PE implementation in <a href=https://github.com/tensorflow/tensor2tensor/blob/5bfe69a7d68b7d61d51fac36c6088f94b9d6fdc6/tensor2tensor/layers/common_attention.py#L457 target=_blank rel="noopener noreffer">tensor2tensor</a> use <code>sin</code> in first half of dimension and <code>cos</code> in the rest part of dimension.</p><figure><img src=/images/transformer_pe2.png width=500></figure><h2 id=encoder>Encoder</h2><p>There are 6 Encoder layer in Transformer, each layer consists of two sub-layer: Multi-Head Attention and Feed Forward Neural Network.</p><h3 id=multi-head-attention>Multi-Head Attention</h3><p>Let&rsquo;s begin with single head attention. In short, it maps word embeddings to <code>q</code> <code>k</code> <code>v</code> and use <code>q</code> <code>k</code> <code>v</code> vector to calculate the attention.</p><p>The input words map to <code>q</code> <code>k</code> <code>v</code> by multiply the Query, Keys Values matrix. Then for the given Query, the attention for each word in sentence will be calculated by this formula: \(\mathrm{attention}=\mathrm{softmax}(\frac{qk^T}{\sqrt{d_k}})v\), where <code>q</code> <code>k</code> <code>v</code> is a 64 dimension vector.</p><figure><img src=/images/transformer_self_attention.png width=500></figure><p>Matrix view:</p><p>\(Attention(Q, K, V) = \mathrm{softmax}(\frac{(XW^Q)(XW^K)^T}{\sqrt{d_k}})(XW^V)\) where \(X\) is the input embedding.</p><p>The single head attention only output a 64 dimension vector, but the input dimension is 512. How to transform back to 512? That&rsquo;s why transformer has multi-head attention.</p><p>Each head has its own \(W^Q\) \(W^K\) \(W^V\) matrix, and produces \(Z_0,Z_1&mldr;Z_7\),(\(Z_0\)&rsquo;s shape is <code>(512, 64)</code>) the concat the outputted vectors as \(O\). \(O\) will multiply a weight matrix \(W^O\) (\(W^O\)&rsquo;s shape is <code>(512, 512)</code>) and the result is \(Z\), which will be sent to Feed Forward Network.</p><figure><img src=/images/transformer_multihead.png width=500></figure><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.</p><p>The whole procedure looks like this:</p><figure><img src=/images/transformer_multihead_all.png width=500></figure><h3 id=add-and-norm>Add & Norm</h3><p>This layer works like this line of code: <code>norm(x+dropout(sublayer(x)))</code> or <code>x+dropout(sublayer(norm(x)))</code>. The sublayer is Multi-Head Attention or FF Network.</p><h4 id=layer-normalization>Layer Normalization</h4><p>Layer Norm is similar to Batch Normalization, but it tries to normalize the whole layer&rsquo;s features rather than each feature.(<strong>Scale</strong> and <strong>Shift</strong> also apply for each feature) More details can be found in this <a href=https://arxiv.org/abs/1607.06450 target=_blank rel="noopener noreffer">paper</a>.</p><figure><img src=/images/transformer_layer_norm.png width=500></figure><h3 id=position-wise-feed-forward-network>Position-wise Feed Forward Network</h3><p>This layer is a Neural Network whose size is <code>(512, 2048, 512)</code>. The exact same feed-forward network is independently applied to each position.</p><figure><img src=/images/transformer_encoder.png width=500></figure><h2 id=output-input>Output Input</h2><p>Same as Input.</p><h2 id=decoder>Decoder</h2><p>The decoder is pretty similar to Encoder. It also has 6 layers, but has 3 sublayers in each Decoder. It add a masked multi-head-attention at the beginning of Decoder.</p><h3 id=masked-multi-head-attention>Masked Multi-Head Attention</h3><p>This layer is used to block future words during training. For example, if the output is <code>&lt;bos> hello world &lt;eos></code>. First, we should use <code>&lt;bos></code> as input to predict <code>hello</code>, <code>hello world &lt;eos></code> will be masked to 0.</p><h3 id=key-and-value-in-decoder-multi-head-attention-layer>Key and Value in Decoder Multi-Head Attention Layer</h3><p>In Encoder, the <code>q</code> <code>k</code> <code>v</code> vector is generated by \(XW^Q\), \(XW^K\) and \(XW^V\). In the second sub-layer of Decoder, <code>q</code> <code>k</code> <code>v</code> was generated by \(XW^Q\), \(YW^K\) and \(YW^V\), where \(Y\) is the Encoder&rsquo;s output, \(X\) is the <code>&lt;init of sentence></code> or previous output.</p><p>The animation below illustrates how to apply the Transformer to machine translation.</p><figure><img src=/images/transformer_translate.gif width=500></figure><h2 id=output>Output</h2><p>Using a linear layer to predict the output.</p><h2 id=ref>Ref</h2><ol><li><a href=http://nlp.seas.harvard.edu/2018/04/03/attention.html target=_blank rel="noopener noreffer">The Annotated Transformer</a></li><li><a href=http://jalammar.github.io/illustrated-transformer/ target=_blank rel="noopener noreffer">The Illustrated Transformer</a></li><li><a href=https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XMb3ZC97FPs target=_blank rel="noopener noreffer">The Transformer – Attention is all you need</a></li><li><a href=https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d target=_blank rel="noopener noreffer">Seq2seq pay Attention to Self Attention: Part 2</a></li><li><a href=https://juejin.im/post/5b9f1af0e51d450e425eb32d target=_blank rel="noopener noreffer">Transformer模型的PyTorch实现</a></li><li><a href=https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec target=_blank rel="noopener noreffer">How to code The Transformer in Pytorch</a></li><li><a href=https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1 target=_blank rel="noopener noreffer">Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention</a></li><li><a href=https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html target=_blank rel="noopener noreffer">Transformer: A Novel Neural Network Architecture for Language Understanding</a></li><li><a href=https://d2l.ai/chapter_attention-mechanisms/transformer.html target=_blank rel="noopener noreffer">Dive into Deep Learning - 10.3 Transformer</a></li><li><a href=https://zhuanlan.zhihu.com/p/80986272 target=_blank rel="noopener noreffer">10分钟带你深入理解Transformer原理及实现</a></li></ol></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2025-08-10</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://fromkk.com/posts/the-annotated-the-annotated-transformer/ data-title="The Annotated The Annotated Transformer" data-hashtags="Machine Learning,Transformer"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://fromkk.com/posts/the-annotated-the-annotated-transformer/ data-hashtag="Machine Learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://fromkk.com/posts/the-annotated-the-annotated-transformer/ data-title="The Annotated The Annotated Transformer"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://fromkk.com/posts/the-annotated-the-annotated-transformer/ data-title="The Annotated The Annotated Transformer"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://fromkk.com/posts/the-annotated-the-annotated-transformer/ data-title="The Annotated The Annotated Transformer"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/machine-learning/>Machine Learning</a>,&nbsp;<a href=/tags/transformer/>Transformer</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/different-types-of-attention/ class=prev rel=prev title="Different types of Attention"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Different types of Attention</a>
<a href=/posts/jaeger-code-structure/ class=next rel=next title="Jaeger Code Structure">Jaeger Code Structure<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=utterances class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://utteranc.es/>utterances</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.155.1">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2017 - 2026</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>KK</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{utterances:{darkTheme:"github-dark",issueTerm:"pathname",label:"utterances",lightTheme:"github-light",repo:"bebound/bebound.github.io"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>