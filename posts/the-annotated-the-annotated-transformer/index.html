<!doctype html><html lang=en><head><title>The Annotated The Annotated Transformer · KK's Blog (fromkk)</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="KK"><meta name=description content="Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.
First, this is the graph that was referenced by almost all of the post related to Transformer.
Transformer consists of these parts: Input, Encoder*N, Output Input, Decoder*N, Output. I&rsquo;ll explain them step by step.
Input Link to heading The input word will map to 512 dimension vector."><meta name=keywords content="fromkk,blog,kk blog,developer,personal,python,golang,go,linux,machine learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Annotated The Annotated Transformer"><meta name=twitter:description content="Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.
First, this is the graph that was referenced by almost all of the post related to Transformer.
Transformer consists of these parts: Input, Encoder*N, Output Input, Decoder*N, Output. I&rsquo;ll explain them step by step.
Input Link to heading The input word will map to 512 dimension vector."><meta property="og:title" content="The Annotated The Annotated Transformer"><meta property="og:description" content="Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.
First, this is the graph that was referenced by almost all of the post related to Transformer.
Transformer consists of these parts: Input, Encoder*N, Output Input, Decoder*N, Output. I&rsquo;ll explain them step by step.
Input Link to heading The input word will map to 512 dimension vector."><meta property="og:type" content="article"><meta property="og:url" content="https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-09-01T16:00:00+08:00"><meta property="article:modified_time" content="2021-05-10T21:19:26+08:00"><link rel=canonical href=https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/><link rel=preload href="https://www.fromkk.com/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=https://www.fromkk.com/css/coder.min.ea4c355c5f9913809f506132a80bf3fab84f2679dee370f334f7385a36d24c38.css integrity="sha256-6kw1XF+ZE4CfUGEyqAvz+rhPJnne43DzNPc4WjbSTDg=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://www.fromkk.com/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=https://www.fromkk.com/images/favicon.svg sizes=any><link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://www.fromkk.com/icons/icon-16x16.png sizes=16x16><link rel=apple-touch-icon href=https://www.fromkk.com/icons/icon-192x192.png><link rel=apple-touch-icon sizes=180x180 href=https://www.fromkk.com/icons/icon-192x192.png><link rel=manifest href=https://www.fromkk.com/site.webmanifest><link rel=mask-icon href=https://www.fromkk.com/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://www.fromkk.com/>KK's Blog (fromkk)</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/tags/>Tags</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/about/>About</a></li><li class=navigation-item><a class=navigation-link href=https://www.fromkk.com/posts/index.xml>RSS</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/>The Annotated The Annotated Transformer</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2019-09-01T16:00:00+08:00>09/01/2019</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
4-minute read</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=https://www.fromkk.com/tags/machine-learning/>Machine Learning</a></span>
<span class=separator>•</span>
<span class=tag><a href=https://www.fromkk.com/tags/transformer/>Transformer</a></span></div></div></header><div class=post-content><p>Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.</p><p>First, this is the graph that was referenced by almost all of the post related to Transformer.</p><figure><img src=https://www.fromkk.com/images/transformer_main.png width=400></figure><p>Transformer consists of these parts: Input, Encoder*N, Output Input, Decoder*N, Output. I&rsquo;ll explain them step by step.</p><h2 id=input>Input
<a class=heading-link href=#input><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The input word will map to 512 dimension vector. Then generate Positional Encoding(PE) and add it to the original embeddings.</p><h3 id=positional-encoding>Positional Encoding
<a class=heading-link href=#positional-encoding><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The transformer model does not contains recurrence and convolution. In order to let the model capture the sequence of input word, it add PE into embeddings.</p><figure><img src=https://www.fromkk.com/images/transformer_add_pe.png width=500></figure><p>PE will generate a 512 dimension vector for each position:</p><p>\[\begin{align*}
PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{model}}) \\\
PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{model}})
\end{align*}\]
The even and odd dimension use <code>sin</code> and <code>cos</code> function respectively.</p><p>For example, the second word&rsquo;s PE should be: \(sin(2 / 10000^{0 / 512}), cos(2 / 10000^{0 / 512}), sin(2 / 10000^{2 / 512}), cos(2 / 10000^{2 / 512})\text{&mldr;}\)</p><p>The value range of PE is <code>(-1,1)</code>, and each position&rsquo;s PE is slight different, as <code>cos</code> and <code>sin</code> has different frequency. Also, for any fixed offset k, \(PE_{pos+k}\) can be represented as a linear function of \(PE_{pos}\).</p><p>For even dimension, let \(10000^{2i/d_{model}}\) be \(\alpha\), for even dimension:</p><p>\[\begin{aligned}
PE_{pos+k}&=sin((pos+k)/\alpha) \\\
&=sin(pos/\alpha)cos(k/\alpha)+cos(pos/\alpha)sin(k/\alpha)\\\
&=PE_{pos\_even}K_1+PE_{pos\_odd}K_2
\end{aligned}\]</p><figure><img src=https://www.fromkk.com/images/transformer_pe1.png width=500></figure><p>The PE implementation in <a href=https://github.com/tensorflow/tensor2tensor/blob/5bfe69a7d68b7d61d51fac36c6088f94b9d6fdc6/tensor2tensor/layers/common%5Fattention.py#L457 class=external-link target=_blank rel=noopener>tensor2tensor</a> use <code>sin</code> in first half of dimension and <code>cos</code> in the rest part of dimension.</p><figure><img src=https://www.fromkk.com/images/transformer_pe2.png width=500></figure><h2 id=encoder>Encoder
<a class=heading-link href=#encoder><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>There are 6 Encoder layer in Transformer, each layer consists of two sub-layer: Multi-Head Attention and Feed Forward Neural Network.</p><h3 id=multi-head-attention>Multi-Head Attention
<a class=heading-link href=#multi-head-attention><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Let&rsquo;s begin with single head attention. In short, it maps word embeddings to <code>q</code> <code>k</code> <code>v</code> and use <code>q</code> <code>k</code> <code>v</code> vector to calculate the attention.</p><p>The input words map to <code>q</code> <code>k</code> <code>v</code> by multiply the Query, Keys Values matrix. Then for the given Query, the attention for each word in sentence will be calculated by this formula: \(\mathrm{attention}=\mathrm{softmax}(\frac{qk^T}{\sqrt{d_k}})v\), where <code>q</code> <code>k</code> <code>v</code> is a 64 dimension vector.</p><figure><img src=https://www.fromkk.com/images/transformer_self_attention.png width=500></figure><p>Matrix view:</p><p>\(Attention(Q, K, V) = \mathrm{softmax}(\frac{(XW^Q)(XW^K)^T}{\sqrt{d_k}})(XW^V)\) where \(X\) is the input embedding.</p><p>The single head attention only output a 64 dimension vector, but the input dimension is 512. How to transform back to 512? That&rsquo;s why transformer has multi-head attention.</p><p>Each head has its own \(W^Q\) \(W^K\) \(W^V\) matrix, and produces \(Z_0,Z_1&mldr;Z_7\),(\(Z_0\)&rsquo;s shape is <code>(512, 64)</code>) the concat the outputted vectors as \(O\). \(O\) will multiply a weight matrix \(W^O\) (\(W^O\)&rsquo;s shape is <code>(512, 512)</code>) and the result is \(Z\), which will be sent to Feed Forward Network.</p><figure><img src=https://www.fromkk.com/images/transformer_multihead.png width=500></figure><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.</p><p>The whole procedure looks like this:</p><figure><img src=https://www.fromkk.com/images/transformer_multihead_all.png width=500></figure><h3 id=add-and-norm>Add & Norm
<a class=heading-link href=#add-and-norm><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>This layer works like this line of code: <code>norm(x+dropout(sublayer(x)))</code> or <code>x+dropout(sublayer(norm(x)))</code>. The sublayer is Multi-Head Attention or FF Network.</p><h4 id=layer-normalization>Layer Normalization
<a class=heading-link href=#layer-normalization><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>Layer Norm is similar to Batch Normalization, but it tries to normalize the whole layer&rsquo;s features rather than each feature.(<strong>Scale</strong> and <strong>Shift</strong> also apply for each feature) More details can be found in this <a href=https://arxiv.org/abs/1607.06450 class=external-link target=_blank rel=noopener>paper</a>.</p><figure><img src=https://www.fromkk.com/images/transformer_layer_norm.png width=500></figure><h3 id=position-wise-feed-forward-network>Position-wise Feed Forward Network
<a class=heading-link href=#position-wise-feed-forward-network><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>This layer is a Neural Network whose size is <code>(512, 2048, 512)</code>. The exact same feed-forward network is independently applied to each position.</p><figure><img src=https://www.fromkk.com/images/transformer_encoder.png width=500></figure><h2 id=output-input>Output Input
<a class=heading-link href=#output-input><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Same as Input.</p><h2 id=decoder>Decoder
<a class=heading-link href=#decoder><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The decoder is pretty similar to Encoder. It also has 6 layers, but has 3 sublayers in each Decoder. It add a masked multi-head-attention at the beginning of Decoder.</p><h3 id=masked-multi-head-attention>Masked Multi-Head Attention
<a class=heading-link href=#masked-multi-head-attention><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>This layer is used to block future words during training. For example, if the output is <code>&lt;bos> hello world &lt;eos></code>. First, we should use <code>&lt;bos></code> as input to predict <code>hello</code>, <code>hello world &lt;eos></code> will be masked to 0.</p><h3 id=key-and-value-in-decoder-multi-head-attention-layer>Key and Value in Decoder Multi-Head Attention Layer
<a class=heading-link href=#key-and-value-in-decoder-multi-head-attention-layer><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>In Encoder, the <code>q</code> <code>k</code> <code>v</code> vector is generated by \(XW^Q\), \(XW^K\) and \(XW^V\). In the second sub-layer of Decoder, <code>q</code> <code>k</code> <code>v</code> was generated by \(XW^Q\), \(YW^K\) and \(YW^V\), where \(Y\) is the Encoder&rsquo;s output, \(X\) is the <code>&lt;init of sentence></code> or previous output.</p><p>The animation below illustrates how to apply the Transformer to machine translation.</p><figure><img src=https://www.fromkk.com/images/transformer_translate.gif width=500></figure><h2 id=output>Output
<a class=heading-link href=#output><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Using a linear layer to predict the output.</p><h2 id=ref>Ref:
<a class=heading-link href=#ref><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ol><li><a href=http://nlp.seas.harvard.edu/2018/04/03/attention.html class=external-link target=_blank rel=noopener>The Annotated Transformer</a></li><li><a href=http://jalammar.github.io/illustrated-transformer/ class=external-link target=_blank rel=noopener>The Illustrated Transformer</a></li><li><a href=https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XMb3ZC97FPs class=external-link target=_blank rel=noopener>The Transformer – Attention is all you need</a></li><li><a href=https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d class=external-link target=_blank rel=noopener>Seq2seq pay Attention to Self Attention: Part 2</a></li><li><a href=https://juejin.im/post/5b9f1af0e51d450e425eb32d class=external-link target=_blank rel=noopener>Transformer模型的PyTorch实现</a></li><li><a href=https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec class=external-link target=_blank rel=noopener>How to code The Transformer in Pytorch</a></li><li><a href=https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1 class=external-link target=_blank rel=noopener>Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention</a></li><li><a href=https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html class=external-link target=_blank rel=noopener>Transformer: A Novel Neural Network Architecture for Language Understanding</a></li><li><a href=https://d2l.ai/chapter%5Fattention-mechanisms/transformer.html class=external-link target=_blank rel=noopener>Dive into Deep Learning - 10.3 Transformer</a></li><li><a href=https://zhuanlan.zhihu.com/p/80986272 class=external-link target=_blank rel=noopener>10分钟带你深入理解Transformer原理及实现</a></li></ol></div><footer><div class=comments><script>let getTheme=window.localStorage&&window.localStorage.getItem("colorscheme"),themeInParams="preferred-color-scheme";getTheme==null&&(themeInParams!==""&&themeInParams!=="auto"?getTheme=themeInParams:getTheme=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");let theme=getTheme==="dark"?"github-dark":"github-light",s=document.createElement("script");s.src="https://utteranc.es/client.js",s.setAttribute("repo","bebound/bebound.github.io"),s.setAttribute("issue-term","pathname"),s.setAttribute("theme",theme),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(s)</script></div></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2023
KK
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=https://www.fromkk.com/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-153788833-1","auto"),ga("send","pageview"))</script></body></html>