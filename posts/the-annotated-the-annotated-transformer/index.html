<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="rgb(255,255,255)"><title>The Annotated The Annotated Transformer &#183; KK's Blog (fromkk)</title>
<meta name=title content="The Annotated The Annotated Transformer &#183; KK's Blog (fromkk)"><script type=text/javascript src=/js/appearance.min.022d0ebc3b46a335eb1c7ef79b7f2de143d7cd5156d433638592ef1ce5f8554e.js integrity="sha256-Ai0OvDtGozXrHH73m38t4UPXzVFW1DNjhZLvHOX4VU4="></script><link type=text/css rel=stylesheet href=/css/main.bundle.min.18ac715ee0ef31ccb3489b9f5dacc8b2c95d2ce05804c075e005a59adcd670ef.css integrity="sha256-GKxxXuDvMcyzSJufXazIssldLOBYBMB14AWlmtzWcO8="><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.584bbad802c9a2b9301e29557fa688dc72062b2386b655a7a19c02c3a1d36248.js integrity="sha256-WEu62ALJorkwHilVf6aI3HIGKyOGtlWnoZwCw6HTYkg=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      
        Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.
First, this is the graph that was referenced by almost all of the post related to Transformer.
      
    "><link rel=canonical href=https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="The Annotated The Annotated Transformer"><meta property="og:description" content="Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.
First, this is the graph that was referenced by almost all of the post related to Transformer."><meta property="og:type" content="article"><meta property="og:url" content="https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-09-01T16:00:00+08:00"><meta property="article:modified_time" content="2023-12-18T21:38:37+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Annotated The Annotated Transformer"><meta name=twitter:description content="Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.
First, this is the graph that was referenced by almost all of the post related to Transformer."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"The Annotated The Annotated Transformer","headline":"The Annotated The Annotated Transformer","abstract":"Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.\nFirst, this is the graph that was referenced by almost all of the post related to Transformer.","inLanguage":"en","url":"https:\/\/www.fromkk.com\/posts\/the-annotated-the-annotated-transformer\/","author":{"@type":"Person","name":"KK"},"copyrightYear":"2019","dateCreated":"2019-09-01T16:00:00\u002b08:00","datePublished":"2019-09-01T16:00:00\u002b08:00","dateModified":"2023-12-18T21:38:37\u002b08:00","keywords":["Machine Learning","Transformer"],"mainEntityOfPage":"true","wordCount":"746"}</script><meta name=author content="KK"><link href=mailto:bebound@gmail.com rel=me><link href=https://github.com/bebound rel=me><link href=https://stackoverflow.com/users/2251785/hunger rel=me></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold pe-2 text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral print:hidden sm:py-10"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>KK&rsquo;s Blog (fromkk)</a></div><ul class="flex flex-col list-none text-end sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Blog</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/tags/ title=Tags><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Tags</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/about/ title=About><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">About</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=https://fromkk.com/posts/index.xml title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">RSS</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=appearance-switcher-1 type=button aria-label="appearance switcher">
<span class="inline transition-colors group-dark:hover:text-primary-400 group-hover:text-primary-600 dark:hidden" title="Switch to dark appearance"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span>
</span><span class="hidden transition-colors group-dark:hover:text-primary-400 group-hover:text-primary-600 dark:inline" title="Switch to light appearance"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></span></button></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=search-button-1 title="Search (/)">
<span class="transition-colors group-dark:hover:text-primary-400 group-hover:text-primary-600"><span class="relative inline-block align-text-bottom px-1 icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">The Annotated The Annotated Transformer</h1><div class="mt-8 mb-12 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2019-09-01 16:00:00 +0800 +0800">1 September 2019</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">4 mins</span></div><div class="flex flex-wrap my-1 text-xs leading-relaxed text-neutral-500 dark:text-neutral-400"><a href=/tags/machine-learning/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Machine Learning</a>
<a href=/tags/transformer/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Transformer</a></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 print:hidden lg:sticky lg:top-10"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="-ms-5 block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="-ms-5 border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#input>Input</a><ul><li><a href=#positional-encoding>Positional Encoding</a></li></ul></li><li><a href=#encoder>Encoder</a><ul><li><a href=#multi-head-attention>Multi-Head Attention</a></li><li><a href=#add-and-norm>Add & Norm</a><ul><li><a href=#layer-normalization>Layer Normalization</a></li></ul></li><li><a href=#position-wise-feed-forward-network>Position-wise Feed Forward Network</a></li></ul></li><li><a href=#output-input>Output Input</a></li><li><a href=#decoder>Decoder</a><ul><li><a href=#masked-multi-head-attention>Masked Multi-Head Attention</a></li><li><a href=#key-and-value-in-decoder-multi-head-attention-layer>Key and Value in Decoder Multi-Head Attention Layer</a></li></ul></li><li><a href=#output>Output</a></li><li><a href=#ref>Ref</a></li></ul></nav></div></details></div></div><div class="min-w-0 min-h-0 max-w-prose grow"><p>Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.</p><p>First, this is the graph that was referenced by almost all of the post related to Transformer.</p><figure><img class="mx-auto my-0 rounded-md" alt loading=lazy src=/images/transformer_main.png></figure><p>Transformer consists of these parts: Input, Encoder*N, Output Input, Decoder*N, Output. I&rsquo;ll explain them step by step.</p><h2 id=input class="relative group">Input <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#input aria-label=Anchor>#</a></span></h2><p>The input word will map to 512 dimension vector. Then generate Positional Encoding(PE) and add it to the original embeddings.</p><h3 id=positional-encoding class="relative group">Positional Encoding <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#positional-encoding aria-label=Anchor>#</a></span></h3><p>The transformer model does not contains recurrence and convolution. In order to let the model capture the sequence of input word, it add PE into embeddings.</p><figure><img class="mx-auto my-0 rounded-md" alt loading=lazy src=/images/transformer_add_pe.png></figure><p>PE will generate a 512 dimension vector for each position:</p><p>\[\begin{align*}
PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{model}}) \\
PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{model}})
\end{align*}\]
The even and odd dimension use <code>sin</code> and <code>cos</code> function respectively.</p><p>For example, the second word&rsquo;s PE should be: \(sin(2 / 10000^{0 / 512}), cos(2 / 10000^{0 / 512}), sin(2 / 10000^{2 / 512}), cos(2 / 10000^{2 / 512})\text{&mldr;}\)</p><p>The value range of PE is <code>(-1,1)</code>, and each position&rsquo;s PE is slight different, as <code>cos</code> and <code>sin</code> has different frequency. Also, for any fixed offset k, \(PE_{pos+k}\) can be represented as a linear function of \(PE_{pos}\).</p><p>For even dimension, let \(10000^{2i/d_{model}}\) be \(\alpha\), for even dimension:</p><p>\[\begin{aligned}
PE_{pos+k}&=sin((pos+k)/\alpha) \\
&=sin(pos/\alpha)cos(k/\alpha)+cos(pos/\alpha)sin(k/\alpha)\\
&=PE_{pos\_even}K_1+PE_{pos\_odd}K_2
\end{aligned}\]</p><figure><img class="mx-auto my-0 rounded-md" alt loading=lazy src=/images/transformer_pe1.png></figure><p>The PE implementation in <a href=https://github.com/tensorflow/tensor2tensor/blob/5bfe69a7d68b7d61d51fac36c6088f94b9d6fdc6/tensor2tensor/layers/common_attention.py#L457 target=_blank rel=noreferrer>tensor2tensor</a> use <code>sin</code> in first half of dimension and <code>cos</code> in the rest part of dimension.</p><figure><img class="mx-auto my-0 rounded-md" alt loading=lazy src=/images/transformer_pe2.png></figure><h2 id=encoder class="relative group">Encoder <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#encoder aria-label=Anchor>#</a></span></h2><p>There are 6 Encoder layer in Transformer, each layer consists of two sub-layer: Multi-Head Attention and Feed Forward Neural Network.</p><h3 id=multi-head-attention class="relative group">Multi-Head Attention <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multi-head-attention aria-label=Anchor>#</a></span></h3><p>Let&rsquo;s begin with single head attention. In short, it maps word embeddings to <code>q</code> <code>k</code> <code>v</code> and use <code>q</code> <code>k</code> <code>v</code> vector to calculate the attention.</p><p>The input words map to <code>q</code> <code>k</code> <code>v</code> by multiply the Query, Keys Values matrix. Then for the given Query, the attention for each word in sentence will be calculated by this formula: \(\mathrm{attention}=\mathrm{softmax}(\frac{qk^T}{\sqrt{d_k}})v\), where <code>q</code> <code>k</code> <code>v</code> is a 64 dimension vector.</p><figure><img class="mx-auto my-0 rounded-md" alt loading=lazy src=/images/transformer_self_attention.png></figure><p>Matrix view:</p><p>\(Attention(Q, K, V) = \mathrm{softmax}(\frac{(XW^Q)(XW^K)^T}{\sqrt{d_k}})(XW^V)\) where \(X\) is the input embedding.</p><p>The single head attention only output a 64 dimension vector, but the input dimension is 512. How to transform back to 512? That&rsquo;s why transformer has multi-head attention.</p><p>Each head has its own \(W^Q\) \(W^K\) \(W^V\) matrix, and produces \(Z_0,Z_1&mldr;Z_7\),(\(Z_0\)&rsquo;s shape is <code>(512, 64)</code>) the concat the outputted vectors as \(O\). \(O\) will multiply a weight matrix \(W^O\) (\(W^O\)&rsquo;s shape is <code>(512, 512)</code>) and the result is \(Z\), which will be sent to Feed Forward Network.</p><figure><img class="mx-auto my-0 rounded-md" alt loading=lazy src=/images/transformer_multihead.png></figure><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.</p><p>The whole procedure looks like this:</p><figure><img class="mx-auto my-0 rounded-md" alt loading=lazy src=/images/transformer_multihead_all.png></figure><h3 id=add-and-norm class="relative group">Add & Norm <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#add-and-norm aria-label=Anchor>#</a></span></h3><p>This layer works like this line of code: <code>norm(x+dropout(sublayer(x)))</code> or <code>x+dropout(sublayer(norm(x)))</code>. The sublayer is Multi-Head Attention or FF Network.</p><h4 id=layer-normalization class="relative group">Layer Normalization <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#layer-normalization aria-label=Anchor>#</a></span></h4><p>Layer Norm is similar to Batch Normalization, but it tries to normalize the whole layer&rsquo;s features rather than each feature.(<strong>Scale</strong> and <strong>Shift</strong> also apply for each feature) More details can be found in this <a href=https://arxiv.org/abs/1607.06450 target=_blank rel=noreferrer>paper</a>.</p><figure><img class="mx-auto my-0 rounded-md" alt loading=lazy src=/images/transformer_layer_norm.png></figure><h3 id=position-wise-feed-forward-network class="relative group">Position-wise Feed Forward Network <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#position-wise-feed-forward-network aria-label=Anchor>#</a></span></h3><p>This layer is a Neural Network whose size is <code>(512, 2048, 512)</code>. The exact same feed-forward network is independently applied to each position.</p><figure><img class="mx-auto my-0 rounded-md" alt loading=lazy src=/images/transformer_encoder.png></figure><h2 id=output-input class="relative group">Output Input <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#output-input aria-label=Anchor>#</a></span></h2><p>Same as Input.</p><h2 id=decoder class="relative group">Decoder <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#decoder aria-label=Anchor>#</a></span></h2><p>The decoder is pretty similar to Encoder. It also has 6 layers, but has 3 sublayers in each Decoder. It add a masked multi-head-attention at the beginning of Decoder.</p><h3 id=masked-multi-head-attention class="relative group">Masked Multi-Head Attention <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#masked-multi-head-attention aria-label=Anchor>#</a></span></h3><p>This layer is used to block future words during training. For example, if the output is <code>&lt;bos> hello world &lt;eos></code>. First, we should use <code>&lt;bos></code> as input to predict <code>hello</code>, <code>hello world &lt;eos></code> will be masked to 0.</p><h3 id=key-and-value-in-decoder-multi-head-attention-layer class="relative group">Key and Value in Decoder Multi-Head Attention Layer <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-and-value-in-decoder-multi-head-attention-layer aria-label=Anchor>#</a></span></h3><p>In Encoder, the <code>q</code> <code>k</code> <code>v</code> vector is generated by \(XW^Q\), \(XW^K\) and \(XW^V\). In the second sub-layer of Decoder, <code>q</code> <code>k</code> <code>v</code> was generated by \(XW^Q\), \(YW^K\) and \(YW^V\), where \(Y\) is the Encoder&rsquo;s output, \(X\) is the <code>&lt;init of sentence></code> or previous output.</p><p>The animation below illustrates how to apply the Transformer to machine translation.</p><figure><img class="mx-auto my-0 rounded-md" alt loading=lazy src=/images/transformer_translate.gif></figure><h2 id=output class="relative group">Output <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#output aria-label=Anchor>#</a></span></h2><p>Using a linear layer to predict the output.</p><h2 id=ref class="relative group">Ref <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ref aria-label=Anchor>#</a></span></h2><ol><li><a href=http://nlp.seas.harvard.edu/2018/04/03/attention.html target=_blank rel=noreferrer>The Annotated Transformer</a></li><li><a href=http://jalammar.github.io/illustrated-transformer/ target=_blank rel=noreferrer>The Illustrated Transformer</a></li><li><a href=https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XMb3ZC97FPs target=_blank rel=noreferrer>The Transformer – Attention is all you need</a></li><li><a href=https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d target=_blank rel=noreferrer>Seq2seq pay Attention to Self Attention: Part 2</a></li><li><a href=https://juejin.im/post/5b9f1af0e51d450e425eb32d target=_blank rel=noreferrer>Transformer模型的PyTorch实现</a></li><li><a href=https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec target=_blank rel=noreferrer>How to code The Transformer in Pytorch</a></li><li><a href=https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1 target=_blank rel=noreferrer>Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention</a></li><li><a href=https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html target=_blank rel=noreferrer>Transformer: A Novel Neural Network Architecture for Language Understanding</a></li><li><a href=https://d2l.ai/chapter_attention-mechanisms/transformer.html target=_blank rel=noreferrer>Dive into Deep Learning - 10.3 Transformer</a></li><li><a href=https://zhuanlan.zhihu.com/p/80986272 target=_blank rel=noreferrer>10分钟带你深入理解Transformer原理及实现</a></li></ol></div></section><footer class="pt-8 max-w-prose print:hidden"><div class=flex><img class="!mb-0 !mt-0 me-4 h-24 w-24 rounded-full" width=96 height=96 alt=KK src=/images/avatar_hu00593763ccab73774a1bf658c8ae3c2c_120479_192x192_fill_box_center_3.png loading=lazy><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">KK</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=mailto:bebound@gmail.com target=_blank aria-label=Email rel="me noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://github.com/bebound target=_blank aria-label=Github rel="me noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://stackoverflow.com/users/2251785/hunger target=_blank aria-label=Stack-Overflow rel="me noopener noreferrer"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentcolor" d="M290.7 311 95 269.7 86.8 309l195.7 41zm51-87L188.2 95.7l-25.5 30.8 153.5 128.3zm-31.2 39.7L129.2 179l-16.7 36.5L293.7 3e2zM262 32l-32 24 119.3 160.3 32-24zm20.5 328h-2e2v39.7h2e2zm39.7 80H42.7V320h-40v160h359.5V320h-40z"/></svg></span></a></div></div></div></div><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/different-types-of-attention/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Different types of Attention</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2019-07-15 00:16:00 +0800 +0800">15 July 2019</time>
</span></span></a></span><span><a class="group flex text-right" href=/posts/jaeger-code-structure/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Jaeger Code Structure</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2019-09-22 17:07:00 +0800 +0800">22 September 2019</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=bebound/bebound.github.io issue-term=pathname theme=preferred-color-scheme crossorigin=anonymous async></script></div></div></footer></article><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
KK</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://github.com/jpanther/congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="flex flex-row items-center"><div class="me-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"><button id=appearance-switcher-0 type=button aria-label="appearance switcher"><div class="flex items-center justify-center w-12 h-12 dark:hidden" title="Switch to dark appearance"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden w-12 h-12 dark:flex" title="Switch to light appearance"><span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://www.fromkk.com><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative inline-block align-text-bottom px-1 icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative inline-block align-text-bottom px-1 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>