{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textcnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hb6znk-DDmnu",
        "colab_type": "code",
        "outputId": "158a53c7-6233-4f59-a7f8-eb4cc1e01041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext numpy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tscyOkDXESk5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext import data,datasets\n",
        "\n",
        "TEXT = data.Field(lower=True,batch_first=True)\n",
        "LABEL = data.Field(sequential=False)\n",
        "\n",
        "# make splits for data\n",
        "train, val, test = datasets.SST.splits(TEXT, LABEL, 'data/',fine_grained=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dNES61KHEinz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "01ce90c1-415a-4c55-f63c-1693cc01f5e2"
      },
      "cell_type": "code",
      "source": [
        "# TEXT.build_vocab(train, vectors=\"fasttext.en.300d\")\n",
        "TEXT.build_vocab(train, vectors=\"glove.840B.300d\")\n",
        "LABEL.build_vocab(train,val,test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.840B.300d.zip: 2.18GB [10:29, 3.46MB/s]                            \n",
            "100%|█████████▉| 2195594/2196017 [04:51<00:00, 7968.55it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "tsiDQBmDWj1N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8c3f7062-fe92-45cc-c116-9b7465a85e5f"
      },
      "cell_type": "code",
      "source": [
        "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
        "print('len(LABEL.vocab)', len(LABEL.vocab)-1)   # vocab include '<unk>'\n",
        "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(TEXT.vocab) 16581\n",
            "len(LABEL.vocab) 5\n",
            "TEXT.vocab.vectors.size() torch.Size([16581, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B0ZHHoI3HtIJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "_DEBUG=False\n",
        "\n",
        "def ilog(*args,**kwargs):\n",
        "    if _DEBUG:\n",
        "        print(*args,**kwargs)\n",
        "    \n",
        "class textCNN(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super().__init__()\n",
        "        dim = args['dim']\n",
        "        n_class = args['n_class']\n",
        "        embedding_matrix=args['embedding_matrix']\n",
        "        kernels=[3,4,5]\n",
        "        kernel_number=[150,150,150]\n",
        "        self.embeding = nn.Embedding.from_pretrained(embedding_matrix)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
        "        self.dropout=nn.Dropout()\n",
        "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
        " \n",
        "    def forward(self, x):\n",
        "        ilog('ori input',x.size())\n",
        "        x = self.embeding(x)\n",
        "        ilog('after embeding',x.size())\n",
        "        x = x.unsqueeze(1)\n",
        "        ilog('unsqueeze',x.size())\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        ilog(x[0].size())\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "    \n",
        "class textCNNMulti(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super().__init__()\n",
        "        dim = args['dim']\n",
        "        n_class = args['n_class']\n",
        "        embedding_matrix=args['embedding_matrix']\n",
        "        kernels=[3,4,5]\n",
        "        kernel_number=[150,150,150]\n",
        "        self.static_embed = nn.Embedding.from_pretrained(embedding_matrix)\n",
        "        self.non_static_embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(2, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
        "        self.dropout=nn.Dropout()\n",
        "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
        " \n",
        "    def forward(self, x):\n",
        "        ilog('ori input',x.size())\n",
        "        non_static_input = self.non_static_embed(x)\n",
        "        static_input = self.static_embed(x)\n",
        "        x = torch.stack([non_static_input, static_input], dim=1)\n",
        "        ilog('after embeding',x.size())\n",
        "        ilog('unsqueeze',x.size())\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        ilog(x[0].size())\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class textCNNNonStatic(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super().__init__()\n",
        "        dim = args['dim']\n",
        "        n_class = args['n_class']\n",
        "        embedding_matrix=args['embedding_matrix']\n",
        "        kernels=[3,4,5]\n",
        "        kernel_number=[150,150,150]\n",
        "        self.embeding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
        "        self.dropout=nn.Dropout()\n",
        "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
        " \n",
        "    def forward(self, x):\n",
        "        ilog('ori input',x.size())\n",
        "        x = self.embeding(x)\n",
        "        ilog('after embeding',x.size())\n",
        "        x = x.unsqueeze(1)\n",
        "        ilog('unsqueeze',x.size())\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        ilog(x[0].size())\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vtMDFmnqqVFK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train, val, test), batch_sizes=(128, 256, 256),shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwZ494LkqwVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "91a2bed6-128c-4d2e-b1ff-6758bb2f4353"
      },
      "cell_type": "code",
      "source": [
        "args={}\n",
        "args['vocb_size']=len(TEXT.vocab)\n",
        "args['dim']=300\n",
        "args['n_class']=len(LABEL.vocab)-1\n",
        "args['embedding_matrix']=TEXT.vocab.vectors\n",
        "args['lr']=0.001\n",
        "args['momentum']=0.8\n",
        "args['epochs']=180\n",
        "args['log_interval']=100\n",
        "args['test_interval']=500\n",
        "args['save_dir']='./'\n",
        "\n",
        "print(args['vocb_size'])\n",
        "print(args['n_class'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16581\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "19sOr41FrGWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50e43c3b-0256-4c72-e124-40f9ad56e6a4"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbebeded330>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "qR-sHoABrMg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3366
        },
        "outputId": "0f3cac5d-5e0c-45d9-9628-322c19e5a7be"
      },
      "cell_type": "code",
      "source": [
        "def save(model, save_dir, save_prefix, steps):\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    save_prefix = os.path.join(save_dir, save_prefix)\n",
        "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "model=textCNNMulti(args)\n",
        "model.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'],momentum=args['momentum'])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_acc = 0\n",
        "last_step = 0\n",
        "model.train()\n",
        "steps=0\n",
        "\n",
        "\n",
        "def eval(data_iter, model, args):\n",
        "    model.eval()\n",
        "    corrects, avg_loss = 0, 0\n",
        "    for i,data in enumerate(data_iter):\n",
        "        x, target = data.text, data.label\n",
        "        x=x.cuda()\n",
        " \n",
        "        target.sub_(1)\n",
        "        target=target.cuda()\n",
        "\n",
        "        logit = model(x)\n",
        "        loss = F.cross_entropy(logit, target, reduction='sum')\n",
        "\n",
        "        avg_loss += loss.item()\n",
        "        corrects += (torch.max(logit, 1)\n",
        "                     [1].view(target.size()).data == target.data).sum()\n",
        "\n",
        "    size = len(data_iter.dataset)\n",
        "    avg_loss /= size\n",
        "    accuracy = 100.0 * int(corrects)/size\n",
        "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
        "                                                                       accuracy, \n",
        "                                                                       corrects, \n",
        "                                                                       size))\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(1, args['epochs']+1):\n",
        "    for i,data in enumerate(train_iter):\n",
        "        steps+=1\n",
        "\n",
        "        x, target = data.text, data.label\n",
        "        x=x.cuda()\n",
        "\n",
        "\n",
        "        target.sub_(1)\n",
        "        target=target.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if steps % args['log_interval'] == 0:\n",
        "            corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            accuracy = 100.0 * int(corrects)/data.batch_size\n",
        "            print(\n",
        "                'Epoch [{}] Batch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(epoch,\n",
        "                                                                         steps, \n",
        "                                                                         loss.item(), \n",
        "                                                                         accuracy,\n",
        "                                                                         corrects,\n",
        "                                                                         data.batch_size))\n",
        "        if steps % args['test_interval'] == 0:\n",
        "            val_acc = eval(val_iter, model, args)\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                last_step = steps\n",
        "                save(model, args['save_dir'], 'best', steps)\n",
        "\n",
        "        model.train()\n",
        "print('final_result')\n",
        "eval(test_iter, model, args)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [2] Batch[100] - loss: 1.575456  acc: 24.2188%(31/128)\n",
            "Epoch [3] Batch[200] - loss: 1.565488  acc: 28.9062%(37/128)\n",
            "Epoch [5] Batch[300] - loss: 1.486482  acc: 35.9375%(46/128)\n",
            "Epoch [6] Batch[400] - loss: 1.529465  acc: 32.0312%(41/128)\n",
            "Epoch [8] Batch[500] - loss: 1.528383  acc: 31.2500%(40/128)\n",
            "\n",
            "Evaluation - loss: 1.529624  acc: 35.2407%(388/1101) \n",
            "\n",
            "Epoch [9] Batch[600] - loss: 1.485863  acc: 36.7188%(47/128)\n",
            "Epoch [11] Batch[700] - loss: 1.497726  acc: 35.1562%(45/128)\n",
            "Epoch [12] Batch[800] - loss: 1.530256  acc: 28.1250%(36/128)\n",
            "Epoch [14] Batch[900] - loss: 1.498434  acc: 33.5938%(43/128)\n",
            "Epoch [15] Batch[1000] - loss: 1.480918  acc: 37.5000%(48/128)\n",
            "\n",
            "Evaluation - loss: 1.474611  acc: 36.6031%(403/1101) \n",
            "\n",
            "Epoch [17] Batch[1100] - loss: 1.431244  acc: 39.0625%(50/128)\n",
            "Epoch [18] Batch[1200] - loss: 1.457355  acc: 40.6250%(52/128)\n",
            "Epoch [20] Batch[1300] - loss: 1.423446  acc: 38.2812%(49/128)\n",
            "Epoch [21] Batch[1400] - loss: 1.401799  acc: 36.7188%(47/128)\n",
            "Epoch [23] Batch[1500] - loss: 1.403295  acc: 39.0625%(50/128)\n",
            "\n",
            "Evaluation - loss: 1.410609  acc: 38.8738%(428/1101) \n",
            "\n",
            "Epoch [24] Batch[1600] - loss: 1.418186  acc: 31.2500%(40/128)\n",
            "Epoch [26] Batch[1700] - loss: 1.339528  acc: 46.0938%(59/128)\n",
            "Epoch [27] Batch[1800] - loss: 1.394407  acc: 34.3750%(44/128)\n",
            "Epoch [29] Batch[1900] - loss: 1.344718  acc: 39.0625%(50/128)\n",
            "Epoch [30] Batch[2000] - loss: 1.344487  acc: 39.0625%(50/128)\n",
            "\n",
            "Evaluation - loss: 1.361082  acc: 40.1453%(442/1101) \n",
            "\n",
            "Epoch [32] Batch[2100] - loss: 1.366098  acc: 37.5000%(48/128)\n",
            "Epoch [33] Batch[2200] - loss: 1.287819  acc: 41.4062%(53/128)\n",
            "Epoch [35] Batch[2300] - loss: 1.227895  acc: 50.7812%(65/128)\n",
            "Epoch [36] Batch[2400] - loss: 1.246688  acc: 43.7500%(56/128)\n",
            "Epoch [38] Batch[2500] - loss: 1.352906  acc: 41.4062%(53/128)\n",
            "\n",
            "Evaluation - loss: 1.328769  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [39] Batch[2600] - loss: 1.301761  acc: 42.9688%(55/128)\n",
            "Epoch [41] Batch[2700] - loss: 1.256693  acc: 45.3125%(58/128)\n",
            "Epoch [42] Batch[2800] - loss: 1.200461  acc: 48.4375%(62/128)\n",
            "Epoch [44] Batch[2900] - loss: 1.196441  acc: 48.4375%(62/128)\n",
            "Epoch [45] Batch[3000] - loss: 1.153560  acc: 51.5625%(66/128)\n",
            "\n",
            "Evaluation - loss: 1.310412  acc: 40.9628%(451/1101) \n",
            "\n",
            "Epoch [47] Batch[3100] - loss: 1.289289  acc: 46.0938%(59/128)\n",
            "Epoch [48] Batch[3200] - loss: 1.172327  acc: 50.0000%(64/128)\n",
            "Epoch [50] Batch[3300] - loss: 1.192238  acc: 50.0000%(64/128)\n",
            "Epoch [51] Batch[3400] - loss: 1.168073  acc: 54.6875%(70/128)\n",
            "Epoch [53] Batch[3500] - loss: 1.165576  acc: 51.5625%(66/128)\n",
            "\n",
            "Evaluation - loss: 1.299171  acc: 41.2352%(454/1101) \n",
            "\n",
            "Epoch [54] Batch[3600] - loss: 1.196363  acc: 51.5625%(66/128)\n",
            "Epoch [56] Batch[3700] - loss: 1.294268  acc: 45.3125%(58/128)\n",
            "Epoch [57] Batch[3800] - loss: 1.164914  acc: 60.1562%(77/128)\n",
            "Epoch [59] Batch[3900] - loss: 1.173069  acc: 51.5625%(66/128)\n",
            "Epoch [60] Batch[4000] - loss: 1.097863  acc: 60.9375%(78/128)\n",
            "\n",
            "Evaluation - loss: 1.288787  acc: 42.6885%(470/1101) \n",
            "\n",
            "Epoch [62] Batch[4100] - loss: 1.158594  acc: 48.4375%(62/128)\n",
            "Epoch [63] Batch[4200] - loss: 1.148816  acc: 50.0000%(64/128)\n",
            "Epoch [65] Batch[4300] - loss: 1.226114  acc: 48.4375%(62/128)\n",
            "Epoch [66] Batch[4400] - loss: 1.141401  acc: 54.6875%(70/128)\n",
            "Epoch [68] Batch[4500] - loss: 1.137902  acc: 49.2188%(63/128)\n",
            "\n",
            "Evaluation - loss: 1.277118  acc: 42.8701%(472/1101) \n",
            "\n",
            "Epoch [69] Batch[4600] - loss: 1.114951  acc: 55.4688%(71/128)\n",
            "Epoch [71] Batch[4700] - loss: 1.107861  acc: 60.1562%(77/128)\n",
            "Epoch [72] Batch[4800] - loss: 1.117285  acc: 52.3438%(67/128)\n",
            "Epoch [74] Batch[4900] - loss: 1.187159  acc: 51.5625%(66/128)\n",
            "Epoch [75] Batch[5000] - loss: 1.054172  acc: 57.8125%(74/128)\n",
            "\n",
            "Evaluation - loss: 1.276164  acc: 42.9609%(473/1101) \n",
            "\n",
            "Epoch [77] Batch[5100] - loss: 1.049909  acc: 60.1562%(77/128)\n",
            "Epoch [78] Batch[5200] - loss: 1.099555  acc: 57.8125%(74/128)\n",
            "Epoch [80] Batch[5300] - loss: 1.085122  acc: 53.9062%(69/128)\n",
            "Epoch [81] Batch[5400] - loss: 1.027369  acc: 63.2812%(81/128)\n",
            "Epoch [83] Batch[5500] - loss: 1.110674  acc: 53.9062%(69/128)\n",
            "\n",
            "Evaluation - loss: 1.269338  acc: 43.6876%(481/1101) \n",
            "\n",
            "Epoch [84] Batch[5600] - loss: 1.000599  acc: 65.6250%(84/128)\n",
            "Epoch [86] Batch[5700] - loss: 0.988945  acc: 62.5000%(80/128)\n",
            "Epoch [87] Batch[5800] - loss: 1.075526  acc: 62.5000%(80/128)\n",
            "Epoch [89] Batch[5900] - loss: 0.945038  acc: 64.8438%(83/128)\n",
            "Epoch [90] Batch[6000] - loss: 1.026899  acc: 61.7188%(79/128)\n",
            "\n",
            "Evaluation - loss: 1.265900  acc: 44.0509%(485/1101) \n",
            "\n",
            "Epoch [92] Batch[6100] - loss: 1.030410  acc: 64.0625%(82/128)\n",
            "Epoch [93] Batch[6200] - loss: 1.032446  acc: 56.2500%(72/128)\n",
            "Epoch [95] Batch[6300] - loss: 1.078320  acc: 51.5625%(66/128)\n",
            "Epoch [96] Batch[6400] - loss: 1.008852  acc: 65.6250%(84/128)\n",
            "Epoch [98] Batch[6500] - loss: 0.961762  acc: 62.5000%(80/128)\n",
            "\n",
            "Evaluation - loss: 1.261912  acc: 44.0509%(485/1101) \n",
            "\n",
            "Epoch [99] Batch[6600] - loss: 1.069657  acc: 56.2500%(72/128)\n",
            "Epoch [100] Batch[6700] - loss: 1.001598  acc: 59.3750%(76/128)\n",
            "Epoch [102] Batch[6800] - loss: 0.986852  acc: 66.4062%(85/128)\n",
            "Epoch [103] Batch[6900] - loss: 0.934148  acc: 69.5312%(89/128)\n",
            "Epoch [105] Batch[7000] - loss: 0.981333  acc: 62.5000%(80/128)\n",
            "\n",
            "Evaluation - loss: 1.256646  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [106] Batch[7100] - loss: 0.989093  acc: 59.3750%(76/128)\n",
            "Epoch [108] Batch[7200] - loss: 0.925865  acc: 63.2812%(81/128)\n",
            "Epoch [109] Batch[7300] - loss: 0.917391  acc: 62.5000%(80/128)\n",
            "Epoch [111] Batch[7400] - loss: 0.951420  acc: 61.7188%(79/128)\n",
            "Epoch [112] Batch[7500] - loss: 0.871006  acc: 70.3125%(90/128)\n",
            "\n",
            "Evaluation - loss: 1.257575  acc: 43.6876%(481/1101) \n",
            "\n",
            "Epoch [114] Batch[7600] - loss: 0.883045  acc: 69.5312%(89/128)\n",
            "Epoch [115] Batch[7700] - loss: 0.881066  acc: 71.8750%(92/128)\n",
            "Epoch [117] Batch[7800] - loss: 0.999407  acc: 64.8438%(83/128)\n",
            "Epoch [118] Batch[7900] - loss: 0.901454  acc: 67.1875%(86/128)\n",
            "Epoch [120] Batch[8000] - loss: 0.893411  acc: 69.5312%(89/128)\n",
            "\n",
            "Evaluation - loss: 1.262024  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [121] Batch[8100] - loss: 0.927260  acc: 64.8438%(83/128)\n",
            "Epoch [123] Batch[8200] - loss: 0.864754  acc: 69.5312%(89/128)\n",
            "Epoch [124] Batch[8300] - loss: 0.816795  acc: 67.1875%(86/128)\n",
            "Epoch [126] Batch[8400] - loss: 0.907245  acc: 70.3125%(90/128)\n",
            "Epoch [127] Batch[8500] - loss: 0.898747  acc: 67.9688%(87/128)\n",
            "\n",
            "Evaluation - loss: 1.255663  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [129] Batch[8600] - loss: 0.837140  acc: 70.3125%(90/128)\n",
            "Epoch [130] Batch[8700] - loss: 0.874366  acc: 65.6250%(84/128)\n",
            "Epoch [132] Batch[8800] - loss: 0.845209  acc: 69.5312%(89/128)\n",
            "Epoch [133] Batch[8900] - loss: 0.874071  acc: 71.8750%(92/128)\n",
            "Epoch [135] Batch[9000] - loss: 0.763104  acc: 78.1250%(100/128)\n",
            "\n",
            "Evaluation - loss: 1.258844  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [136] Batch[9100] - loss: 0.796341  acc: 71.0938%(91/128)\n",
            "Epoch [138] Batch[9200] - loss: 0.783580  acc: 71.0938%(91/128)\n",
            "Epoch [139] Batch[9300] - loss: 0.793860  acc: 78.1250%(100/128)\n",
            "Epoch [141] Batch[9400] - loss: 0.788521  acc: 74.2188%(95/128)\n",
            "Epoch [142] Batch[9500] - loss: 0.673299  acc: 83.5938%(107/128)\n",
            "\n",
            "Evaluation - loss: 1.257783  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [144] Batch[9600] - loss: 0.789295  acc: 70.3125%(90/128)\n",
            "Epoch [145] Batch[9700] - loss: 0.846536  acc: 73.4375%(94/128)\n",
            "Epoch [147] Batch[9800] - loss: 0.775176  acc: 79.6875%(102/128)\n",
            "Epoch [148] Batch[9900] - loss: 0.760558  acc: 77.3438%(99/128)\n",
            "Epoch [150] Batch[10000] - loss: 0.761281  acc: 79.6875%(102/128)\n",
            "\n",
            "Evaluation - loss: 1.260450  acc: 44.4142%(489/1101) \n",
            "\n",
            "Epoch [151] Batch[10100] - loss: 0.746036  acc: 75.0000%(96/128)\n",
            "Epoch [153] Batch[10200] - loss: 0.784595  acc: 75.7812%(97/128)\n",
            "Epoch [154] Batch[10300] - loss: 0.704199  acc: 82.8125%(106/128)\n",
            "Epoch [156] Batch[10400] - loss: 0.733516  acc: 78.9062%(101/128)\n",
            "Epoch [157] Batch[10500] - loss: 0.793995  acc: 71.8750%(92/128)\n",
            "\n",
            "Evaluation - loss: 1.257931  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [159] Batch[10600] - loss: 0.615319  acc: 84.3750%(108/128)\n",
            "Epoch [160] Batch[10700] - loss: 0.722885  acc: 74.2188%(95/128)\n",
            "Epoch [162] Batch[10800] - loss: 0.691430  acc: 83.5938%(107/128)\n",
            "Epoch [163] Batch[10900] - loss: 0.685125  acc: 85.9375%(110/128)\n",
            "Epoch [165] Batch[11000] - loss: 0.682910  acc: 77.3438%(99/128)\n",
            "\n",
            "Evaluation - loss: 1.261615  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [166] Batch[11100] - loss: 0.711283  acc: 80.4688%(103/128)\n",
            "Epoch [168] Batch[11200] - loss: 0.675482  acc: 84.3750%(108/128)\n",
            "Epoch [169] Batch[11300] - loss: 0.644571  acc: 80.4688%(103/128)\n",
            "Epoch [171] Batch[11400] - loss: 0.697808  acc: 78.1250%(100/128)\n",
            "Epoch [172] Batch[11500] - loss: 0.638265  acc: 81.2500%(104/128)\n",
            "\n",
            "Evaluation - loss: 1.267426  acc: 43.5059%(479/1101) \n",
            "\n",
            "Epoch [174] Batch[11600] - loss: 0.702670  acc: 76.5625%(98/128)\n",
            "Epoch [175] Batch[11700] - loss: 0.688972  acc: 79.6875%(102/128)\n",
            "Epoch [177] Batch[11800] - loss: 0.620397  acc: 84.3750%(81/96)\n",
            "Epoch [178] Batch[11900] - loss: 0.661132  acc: 79.6875%(102/128)\n",
            "Epoch [180] Batch[12000] - loss: 0.560323  acc: 85.9375%(110/128)\n",
            "\n",
            "Evaluation - loss: 1.272562  acc: 43.9600%(484/1101) \n",
            "\n",
            "final_result\n",
            "\n",
            "Evaluation - loss: 1.223668  acc: 45.4751%(1005/2210) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45.47511312217195"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}