<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>word2vec on KK's Blog (fromkk)</title><link>https://www.fromkk.com/tags/word2vec/</link><description>Recent content in word2vec on KK's Blog (fromkk)</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License，please give source if you wish to quote or reproduce.</copyright><lastBuildDate>Fri, 05 Jan 2018 15:14:00 +0800</lastBuildDate><atom:link href="https://www.fromkk.com/tags/word2vec/index.xml" rel="self" type="application/rss+xml"/><item><title>Models and Architectures in Word2vec</title><link>https://www.fromkk.com/posts/models-and-architechtures-in-word2vec/</link><pubDate>Fri, 05 Jan 2018 15:14:00 +0800</pubDate><guid>https://www.fromkk.com/posts/models-and-architechtures-in-word2vec/</guid><description>Generally, word2vec is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.
Models Link to heading CBOW (Continuous Bag of Words) Link to heading Use the context to predict the probability of current word. (In the picture, the word is encoded with one-hot encoding, \(W_{V*N}\) is word embedding, and \(W_{V*N}^{&amp;rsquo;}\), the output weight matrix in hidden layer, is same as \(\hat{\upsilon}\) in following equations)</description></item><item><title>Semi-supervised Text Classification Using doc2vec and Label Spreading</title><link>https://www.fromkk.com/posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/</link><pubDate>Sun, 10 Sep 2017 15:29:00 +0800</pubDate><guid>https://www.fromkk.com/posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/</guid><description>Here is a simple way to classify text without much human effort and get a impressive performance.
It can be divided into two steps:
Get train data by using keyword classification Generate a more accurate classification model by using doc2vec and label spreading Keyword-based Classification Link to heading Keyword based classification is a simple but effective method. Extracting the target keyword is a monotonous work. I use this method to automatic extract keyword candidate.</description></item><item><title>Parameters in doc2vec</title><link>https://www.fromkk.com/posts/parameters-in-dov2vec/</link><pubDate>Thu, 03 Aug 2017 15:20:00 +0800</pubDate><guid>https://www.fromkk.com/posts/parameters-in-dov2vec/</guid><description>Here are some parameter in gensim&amp;rsquo;s doc2vec class.
window Link to heading window is the maximum distance between the predicted word and context words used for prediction within a document. It will look behind and ahead.
In skip-gram model, if the window size is 2, the training samples will be this:(the blue word is the input word)
min_count Link to heading If the word appears less than this value, it will be skipped</description></item></channel></rss>