<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Word2vec on KK's Blog (fromkk)</title><link>https://www.fromkk.com/tags/word2vec/</link><description>Recent content in Word2vec on KK's Blog (fromkk)</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 18 Dec 2023 21:38:36 +0800</lastBuildDate><atom:link href="https://www.fromkk.com/tags/word2vec/index.xml" rel="self" type="application/rss+xml"/><item><title>Models and Architectures in Word2vec</title><link>https://www.fromkk.com/posts/models-and-architechtures-in-word2vec/</link><pubDate>Fri, 05 Jan 2018 15:14:00 +0800</pubDate><guid>https://www.fromkk.com/posts/models-and-architechtures-in-word2vec/</guid><description>Generally, word2vec is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.
Models #CBOW (Continuous Bag of Words) #Use the context to predict the probability of current word.</description></item><item><title>Parameters in doc2vec</title><link>https://www.fromkk.com/posts/parameters-in-dov2vec/</link><pubDate>Thu, 03 Aug 2017 15:20:00 +0800</pubDate><guid>https://www.fromkk.com/posts/parameters-in-dov2vec/</guid><description>Here are some parameter in gensim&amp;rsquo;s doc2vec class.
window #window is the maximum distance between the predicted word and context words used for prediction within a document. It will look behind and ahead.
In skip-gram model, if the window size is 2, the training samples will be this:(the blue word is the input word)</description></item></channel></rss>