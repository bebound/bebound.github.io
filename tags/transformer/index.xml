<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformer on KK's Blog (fromkk)</title><link>https://www.fromkk.com/tags/transformer/</link><description>Recent content in Transformer on KK's Blog (fromkk)</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License，please give source if you wish to quote or reproduce.</copyright><lastBuildDate>Sun, 01 Sep 2019 16:00:00 +0800</lastBuildDate><atom:link href="https://www.fromkk.com/tags/transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>The Annotated The Annotated Transformer</title><link>https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/</link><pubDate>Sun, 01 Sep 2019 16:00:00 +0800</pubDate><guid>https://www.fromkk.com/posts/the-annotated-the-annotated-transformer/</guid><description>Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.
First, this is the graph that was referenced by almost all of the post related to Transformer.
Transformer consists of these parts: Input, Encoder*N, Output Input, Decoder*N, Output. I&amp;rsquo;ll explain them step by step.
Input Link to heading The input word will map to 512 dimension vector.</description></item></channel></rss>