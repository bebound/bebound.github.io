<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LSTM on KK's Blog (fromkk)</title><link>https://www.fromkk.com/tags/lstm/</link><description>Recent content in LSTM on KK's Blog (fromkk)</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 18 Dec 2023 21:38:36 +0800</lastBuildDate><atom:link href="https://www.fromkk.com/tags/lstm/index.xml" rel="self" type="application/rss+xml"/><item><title>LSTM and GRU</title><link>https://www.fromkk.com/posts/lstm-and-gru/</link><pubDate>Sun, 22 Apr 2018 14:39:00 +0800</pubDate><guid>https://www.fromkk.com/posts/lstm-and-gru/</guid><description>LSTM #The avoid the problem of vanishing gradient and exploding gradient in vanilla RNN, LSTM was published, which can remember information for longer periods of time.
Here is the structure of LSTM:
The calculate procedure are:
\[\begin{aligned} f_t&amp;amp;=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)\\ i_t&amp;amp;=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\ o_t&amp;amp;=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\ \tilde{C_t}&amp;amp;=tanh(W_C\cdot[h_{t-1},x_t]+b_C)\\ C_t&amp;amp;=f_t\ast C_{t-1}+i_t\ast \tilde{C_t}\\ h_t&amp;amp;=o_t \ast tanh(C_t) \end{aligned}\]</description></item></channel></rss>