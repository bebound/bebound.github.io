<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Doc2vec on KK's Blog (fromkk)</title><link>https://fromkk.com/tags/doc2vec/</link><description>Recent content in Doc2vec on KK's Blog (fromkk)</description><generator>Hugo</generator><language>en</language><managingEditor>bebound@gmail.com (KK)</managingEditor><webMaster>bebound@gmail.com (KK)</webMaster><lastBuildDate>Fri, 18 Jul 2025 19:07:21 +0800</lastBuildDate><atom:link href="https://fromkk.com/tags/doc2vec/index.xml" rel="self" type="application/rss+xml"/><item><title>Semi-supervised text classification using doc2vec and label spreading</title><link>https://fromkk.com/posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/</link><pubDate>Sun, 10 Sep 2017 15:29:00 +0800</pubDate><author>bebound@gmail.com (KK)</author><guid>https://fromkk.com/posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/</guid><description>&lt;p>Here is a simple way to classify text without much human effort and get a impressive performance.&lt;/p>
&lt;p>It can be divided into two steps:&lt;/p>
&lt;ol>
&lt;li>Get train data by using keyword classification&lt;/li>
&lt;li>Generate a more accurate classification model by using doc2vec and label spreading&lt;/li>
&lt;/ol>
&lt;h2 id="keyword-based-classification">Keyword-based Classification&lt;/h2>
&lt;p>Keyword based classification is a simple but effective method. Extracting the target keyword is a monotonous work. I use this method to automatic extract keyword candidate.&lt;/p></description></item><item><title>Parameters in doc2vec</title><link>https://fromkk.com/posts/parameters-in-dov2vec/</link><pubDate>Thu, 03 Aug 2017 15:20:00 +0800</pubDate><author>bebound@gmail.com (KK)</author><guid>https://fromkk.com/posts/parameters-in-dov2vec/</guid><description>&lt;p>Here are some parameter in &lt;code>gensim&lt;/code>&amp;rsquo;s &lt;code>doc2vec&lt;/code> class.&lt;/p>
&lt;h3 id="window">window&lt;/h3>
&lt;p>window is the maximum distance between the predicted word and context words used for prediction within a document. It will look behind and ahead.&lt;/p>
&lt;p>In &lt;code>skip-gram&lt;/code> model, if the window size is 2, the training samples will be this:(the blue word is the input word)&lt;/p>
&lt;figure class="image-size-s">&lt;img src="https://fromkk.com/images/doc2vec_window.png">
&lt;/figure>

&lt;h3 id="min-count">min_count&lt;/h3>
&lt;p>If the word appears less than this value, it will be skipped&lt;/p>
&lt;h3 id="sample">sample&lt;/h3>
&lt;p>High frequency word like &lt;code>the&lt;/code> is useless for training. &lt;code>sample&lt;/code> is a threshold for deleting these higher-frequency words. The probability of keeping the word \(w_i\) is:&lt;/p></description></item></channel></rss>