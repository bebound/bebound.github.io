[{"content":"","date":null,"permalink":"https://fromkk.com/","section":"KK's Blog (fromkk)","summary":"","title":"KK's Blog (fromkk)"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/nas/","section":"Tags","summary":"","title":"NAS"},{"content":"","date":null,"permalink":"https://fromkk.com/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"Three years ago, I bought a QNAP TS-453Dmini NAS. Although it has a slow WEB UI and slow restart, it still fits my needs as all of the applications I need are running in Docker.\nRecently, I want to move some files from my Mac to NAS to save space. I need a application behave like Dropbox, which can show all the files in the NAS and only download the files I need. I have tried the QSync, but it does not have thumbnails for cloud image and it does not have icons to show the file status. I also tried the Seafile, it\u0026rsquo;s a powerful application, which requires 4G RAM to run, and there is bug in the thumbnail. I used to have a Synology ARM NAS, the Synology Drive has all the features I need, so I want to run it on my QNAP NAS. After some research, I managed to run Synology and QNAP together on my NAS. Here is the guide.\nInstall PVE on QNAP #To run Synology on QNAP, the first step is to install PVE. I put the PVE iso to a Ventoy USB drive and reboot the NAS. When hearing the beep, press the DEL key immediately to enter the BIOS. Change the boot order to boot from the Ventoy and then install the PVE on the NAS. Remember to move the PVE disk to HDD bay 3 or 4, and the QNAP bios won\u0026rsquo;t set the bay 1 or 2 as boot disk.\nInstall Synology on PVE #It\u0026rsquo;s pretty easy to install Synology on PVE, there are many guides in the Internet. Thanks to the RR project, you can install Synology on any x86 hardware.\nInstall QNAP on PVE #I have one SSD and one HDD. The SSD is used for PVE and Synology, but I don\u0026rsquo;t have another disk to move the data to Synology. So I still need to run QNAP on PVE. Although the QNAP is not as popular as Synology, there is still a way to run it on PVE. I just followed this guide to install QNAP on PVE. To pass-through the HDD, just run ls -l /dev/disk/by-id/ to show all device and qm set {qnap vm id} -sata0 /dev/disk/by-id/{hdd_id}` to pass-through the HDD to QNAP VM. When restart the QNAP VM, it will ask you whether to load the OS from HDD, reset OS (but keep the data) or init the OS. However, when I choose load the OS, it reboot and back to the same screen. I guess it might be a compatibility issue. Reset OS may work. For me, I create a new disk for the QNAP VM an install the QNAP OS, then I pass-through the HDD to the QNAP VM. Then in the Storage and Snapshot app, under Storage-Disks/VJOBD, on the right corner, click the More button and choose Recover. I managed to recover the data in the HDD. After that, I can use the QNAP as usual.\nSynology OS vs QNAP OS #After using Synology for a while, I found the Synology OS is more user-friendly than QNAP OS. The UI is more responsive and the applications are more polished. There are more community applications available for Synology, and the overall experience feels more integrated. The Synology OS is more optimized. For example, the Synology backend services uses much less CPU and RAM than QNAP. I will migrate all of my data to Synology in the future once I got a new HDD.\nRef: # 除了 synology drive，还有支持选择性同步的本地部署云盘吗 保姆级安装黑威联通 ","date":"29 June 2025","permalink":"https://fromkk.com/posts/run-synology-in-qnap-nas-with-pve/","section":"Posts","summary":"\u003cp\u003eThree years ago, I bought a QNAP TS-453Dmini NAS. Although it has a slow WEB UI and slow restart, it still fits my needs as all of the applications I need are running in Docker.\u003c/p\u003e\n\u003cp\u003eRecently, I want to move some files from my Mac to NAS to save space. I need a application behave like Dropbox, which can show all the files in the NAS and only download the files I need. I have tried the QSync, but it does not have thumbnails for cloud image and it does not have icons to show the file status. I also tried the \u003ca href=\"https://www.seafile.com/home/\" target=\"_blank\" rel=\"noreferrer\"\u003eSeafile\u003c/a\u003e, it\u0026rsquo;s a powerful application, which requires 4G RAM to run, and there is bug in the thumbnail. I used to have a Synology ARM NAS, the Synology Drive has all the features I need, so I want to run it on my QNAP NAS. After some research, I managed to run Synology and QNAP together on my NAS. Here is the guide.\u003c/p\u003e","title":"Run Synology in QNAP NAS with PVE"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Nowadays, pyproject.toml becomes the standard configuration file for packaging. Compare with the old setup.py, it adds two feature pep517 and pep518.\npep517 defines two hooks: build_wheel and build_sdist, which is required to build the package from source. Each build backend must implement these two hooks. It makes it possible to create other build backend such as flit or poetry.\n[build-system] # Defined by PEP 518: requires = [\u0026#34;flit\u0026#34;] # Defined by this PEP: build-backend = \u0026#34;local_backend\u0026#34; backend-path = [\u0026#34;backend\u0026#34;] setuptools, there are some other build back-end such as hatchling and flit. You can find the example here: Python Packaging Uer Guide - Choosing a build backend\npep518 defines the format of pyproject.toml, where you can specify you build dependencies, ensuring the necessary tools will be installed when building project. For example:\n[build-system] requires = [\u0026#34;setuptools ~= 58.0\u0026#34;, \u0026#34;cython ~= 0.29.0\u0026#34;] Is `setup.py` deprecated #According to python packaging doc, it is still the valid configuration file for setuptools, but use setup.py in command is depracetd:\nDeprecated Replacement python setup.py install pip install . python setup.py develop pip install -e . python setup.py sdist python -m build python setup.py bdist_wheel python -m build Build is a pep 517 compatible build fontend, it calls build backend to generate the source and wheel distribution. It\u0026rsquo;s the recommended way to build the package.\nWhen --use-pep517 is activated? #If the source distribution contains pyproject.toml, pip will use pep517 to build the package.\nIf the current env does not have setuptools` or wheel, pip will use pep517 to build the package: pip source code\nYou can also force pip to use pep517 with --use-pep517, or disable it and use the legacy behavior with --no-use-pep517.\nWhat\u0026rsquo;s the difference between --use-pep517 and legacy behavior? #This is a typical log which uses --use-pep517:\nroot@427314aff523:/# pip install requests --no-binary :all: --no-deps Collecting requests Using cached requests-2.32.3.tar.gz (131 kB) Installing build dependencies ... done Getting requirements to build wheel ... done Preparing metadata (pyproject.toml) ... done Building wheels for collected packages: requests Building wheel for requests (pyproject.toml) ... done Created wheel for requests: filename=requests-2.32.3-py3-none-any.whl size=64922 sha256=9ee1e853d3d86a8b484cf10c2920601befe81bfad4bd0c3319274b67143ac266 This is the one which uses the legacy behavior:\nProcessing d:\\a\\_work\\1\\s\\src\\azure-cli Preparing metadata (setup.py): started Preparing metadata (setup.py): finished with status \u0026#39;done\u0026#39; Building wheels for collected packages: azure-cli Building wheel for azure-cli (setup.py): started Building wheel for azure-cli (setup.py): finished with status \u0026#39;done\u0026#39; The main difference is that --use-pep517 will create a temp build env and build the package in it. The build env is totally fresh, it has to install the build dependencies such as setuptools first, then call the backend to build the wheel pacakge. Finally, it will install the wheel package.\nIn legacy behavior, pip use the current env\u0026rsquo;s pip, setuptools and wheel to build the package with python setup.py bdist_wheel, then install the wheel package.\nA weird issue related to --use-pep517 #When bump Python 3.12 in Azure CLI, the get-pip.py does not install setuptools by default, as well as wheel. So the pip tries to use pep517 to build azure-cli.\nHowever, the runner agent is using Python 3.12.6 and the embedded Python is 3.12.7. They have a compatibility issue. In the build env, the python -m pip fails with code 57005, because it tries to load modules in 3.12.6. I have to use python -Im pip to install the package. However, in the build env, the -I param is not honored, the command still fails. I\u0026rsquo;ve created an issue in pip repo. The workaround is so complicated, so I have to install setuptools and wheel to let pip use the legacy behavior, which use the env\u0026rsquo;s pip to build the package.\nThe details can be found in the PR\nRef: # pip build_env.py source code original issue when bump Python 3.12 pip build process for pyproject.toml PEP 517 and 518 in Plain English Writing your pyproject.toml ","date":"24 November 2024","permalink":"https://fromkk.com/posts/modern-pip-build-process-use-pep517/","section":"Posts","summary":"\u003cp\u003eNowadays, \u003ccode\u003epyproject.toml\u003c/code\u003e becomes the standard configuration file for packaging. Compare with the old \u003ccode\u003esetup.py\u003c/code\u003e, it adds two feature pep517 and pep518.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://peps.python.org/pep-0517/\" target=\"_blank\" rel=\"noreferrer\"\u003epep517\u003c/a\u003e defines two hooks: \u003ccode\u003ebuild_wheel\u003c/code\u003e and \u003ccode\u003ebuild_sdist\u003c/code\u003e, which is required to build the package from source. Each build backend must implement these two hooks. It makes it possible to create other build backend such as \u003ccode\u003eflit\u003c/code\u003e or \u003ccode\u003epoetry\u003c/code\u003e.\u003c/p\u003e","title":"Modern pip build process (–use-pep517)"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/pip/","section":"Tags","summary":"","title":"Pip"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"Here is the process how sys.path is set in Python, with some parts omitted.\nPython Command Line Arguments #By default, as initialized upon program startup, a potentially unsafe path is prepended to sys.path:\npython -m: prepend the current working directory.\npython script.py: prepend the script’s directory. If it’s a symbolic link, resolve symbolic links.\npython -c and python (REPL): prepend an empty string, which means the current working directory.\nYou can remove these path with -P param.\nPYTHONPATH #If this environment variable is set, the folders in it will be added to sys.path. The folders are separated by colons on Unix and semicolons on Windows.\nprefix and exec_prefix #These two variable define the standard Python modules and extension modules. Python has a specific path to search depends on the OS. The start point is Python executable path, which is called home (the symbolic links are followed).\nOnce home is determined, the prefix directory is found by looking for pythongmajorversionminorversion.zip. For example, python312.zip. On Windows, the zip package is in the same directory as the Python executable. On Unix, it is in /lib folder. If it is not found, on Windows, it will looks for Lib\\os.py. On Unix, it will look for lib/python3.12/os.py.\nOn macOS, the home is /opt/homebrew/opt/python@3.12/Frameworks/Python.framework/Versions/3.12/bin/python3.12. The prefix is /opt/homebrew/opt/python@3.12/Frameworks/Python.framework/Versions/3.12, because lib/python3.12/os.py is there.\nOn Windows, the exec_prefix is the same as prefix. But on other OS, exec_prefix is determined by lib/python3.xx/lib-dynload. On my mac, it\u0026rsquo;s still /opt/homebrew/opt/python@3.12/Frameworks/Python.framework/Versions/3.12.\nlib/python312.zip, lib/python3.12 and lib/python3.12/lib-dynload are added into sys.path.\nsite module #This module is automatically called during Python startup, which tries to append the site-packages folder into sys.path. It can be disabled with -S option.\nFinding site-packages folder is easy. It can be guessed by prefix and exec_prefix. These two path is head, and the tail part is lib/site-packages on Windows or lib/pythonX.Y/site-packages on *nix. For each of the head-tail combinations, it add the path into sys.path if it exists.\n.pth files #If a name.pth file exits in the site-packages folder, its content are additional items to be added into sys.path. Each line is a relative path.\nThe site module also tries to add USER_SITE folder into sys.path. Default value is ~/.local/lib/pythonX.Y/site-packages for UNIX and non-framework macOS builds, ~/Library/Python/X.Y/lib/python/site-packages for macOS framework builds, and %APPDATA%\\Python\\PythonXY\\site-packages on Windows.\nExample #We can use python3 -m site to quickly check sys.path and user site. Here is the output on my mac:\nsys.path = [ \u0026#39;{current folder}\u0026#39;, \u0026#39;/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python312.zip\u0026#39;, \u0026#39;/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12\u0026#39;, \u0026#39;/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/lib-dynload\u0026#39;, \u0026#39;/opt/homebrew/lib/python3.12/site-packages\u0026#39;, \u0026#39;/opt/homebrew/opt/python@3.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages\u0026#39;, ] USER_BASE: \u0026#39;/Users/kk/Library/Python/3.12\u0026#39; (doesn\u0026#39;t exist) USER_SITE: \u0026#39;/Users/kk/Library/Python/3.12/lib/python/site-packages\u0026#39; (doesn\u0026#39;t exist) ENABLE_USER_SITE: True The path is slightly different from what the document states. The site-packages folder is not the same as prefix. I guess that because Homebrew creates lots of symbol link. python3 is /opt/homebrew/bin/python3 -\u0026gt; opt/homebrew/Cellar/python@3.12/3.12.4/bin/python3 -\u0026gt; /opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/bin/python3.\n\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; sys.prefix \u0026#39;/opt/homebrew/opt/python@3.12/Frameworks/Python.framework/Versions/3.12\u0026#39; \u0026gt;\u0026gt;\u0026gt; sys.exec_prefix \u0026#39;/opt/homebrew/opt/python@3.12/Frameworks/Python.framework/Versions/3.12\u0026#39; Here is the output on my Ubuntu server:\nsys.path = [ \u0026#39;{current folder}\u0026#39;, \u0026#39;/usr/lib/python38.zip\u0026#39;, \u0026#39;/usr/lib/python3.8\u0026#39;, \u0026#39;/usr/lib/python3.8/lib-dynload\u0026#39;, \u0026#39;/home/kk/.local/lib/python3.8/site-packages\u0026#39;, \u0026#39;/usr/local/lib/python3.8/dist-packages\u0026#39;, \u0026#39;/usr/local/lib/python3.8/dist-packages/cloud_init-20.1-py3.8.egg\u0026#39;, \u0026#39;/usr/lib/python3/dist-packages\u0026#39;, ] USER_BASE: \u0026#39;/home/kk/.local\u0026#39; (exists) USER_SITE: \u0026#39;/home/kk/.local/lib/python3.8/site-packages\u0026#39; (exists) ENABLE_USER_SITE: True The doc also does not explain why `/opt/homebrew/lib/python3.12/site-packages` is in the path. This doc is somewhat out-of-date: https://discuss.python.org/t/the-document-on-pythonhome-might-be-wrong/19614\nRef: # sys — System-specific parameters and functions The initialization of the sys.path module search path _pth files ","date":"11 August 2024","permalink":"https://fromkk.com/posts/sys-dot-path-in-python/","section":"Posts","summary":"\u003cp\u003eHere is the process how \u003ccode\u003esys.path\u003c/code\u003e is set in Python, with some parts omitted.\u003c/p\u003e\n\u003ch2 id=\"python-command-line-arguments\" class=\"relative group\"\u003ePython Command Line Arguments \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#python-command-line-arguments\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eBy default, as initialized upon program startup, a potentially unsafe path is prepended to \u003ccode\u003esys.path\u003c/code\u003e:\u003c/p\u003e","title":"sys.path in Python"},{"content":"It\u0026rsquo;s known that Python\u0026rsquo;s import statement is implemented by __import__ function. In general, if we want to import a module dynamically, we can use import_module function, which is a wrapper around __import__.\nThe most important difference between these two functions is that import_module() returns the specified package or module (e.g. pkg.mod), while import() returns the top-level package or module (e.g. pkg). \u0026ndash; https://docs.python.org/3/library/importlib.html#importlib.import_module\nimport itertools and from requests import exceptions can be translated to:\nimport importlib itertools = importlib.import_module(\u0026#39;itertools\u0026#39;) exceptions = importlib.import_module(\u0026#39;requests.exceptions\u0026#39;) __import__ #This is an advanced function that is not needed in everyday Python programming, unlike importlib.import_module().\nHere is an example of how __import__ is called:\nold_import = __import__ def noisy_importer(name, globals=None, locals=None, fromlist=None, level=0): print(f\u0026#39;name: {name!r}\u0026#39;) print(f\u0026#39;fromlist: {fromlist}\u0026#39;) print(f\u0026#39;level: {level}\u0026#39;) print(\u0026#39;-\u0026#39; * 80) return old_import(name, locals, globals, fromlist, level) import builtins builtins.__import__ = noisy_importer print(\u0026#39;import math\u0026#39;) import math print(\u0026#39;from math import sqrt\u0026#39;) from math import sqrt \u0026gt;\u0026gt;\u0026gt; import math name: \u0026#39;math\u0026#39; fromlist: None level: 0 -------------------------------------------------------------------------------- from math import sqrt name: \u0026#39;math\u0026#39; fromlist: (\u0026#39;sqrt\u0026#39;,) level: 0 -------------------------------------------------------------------------------- As we mentioned earlier, the __import__ returns the top level module.\nFor example, requests=__import('requests.exceptions',globals(),locals(),[],0). If you want to get the submodule exceptions, you need to use getattr: equests_exceptions=getattr(__import__('requests', globals(), locals(), [], 0), 'exceptions').\nThere is another tricky way to import the submodule: use a non-empty fromlist: requests_exceptions = __import__('requests.exceptions', globals(), locals(), [None], 0).\nAdditionally, we can also set fromlist to specify the names of submodules that should be imported. The statement from spam.ham import eggs, sausage as saus can be translated to\n_temp = __import__(\u0026#39;spam.ham\u0026#39;, globals(), locals(), [\u0026#39;eggs\u0026#39;, \u0026#39;sausage\u0026#39;], 0) eggs = _temp.eggs saus = _temp.sausage Skip importing non-existing modules with __import__ #This a use case of the __import__ function. Some packages are missing, but we want to make sure that the code does not crash when importing them.\nimport builtins from unittest.mock import Mock old_import = __import__ def skip_imports(name, globals=None, locals=None, fromlist=None, level=0): skip_list = {\u0026#39;urllib3\u0026#39;, \u0026#39;requests_oauthlib\u0026#39;, \u0026#39;cryptography\u0026#39;} if name in skip_list or any(name.startswith(f\u0026#39;{p}.\u0026#39;) for p in skip_list): return Mock() else: return old_import(name, globals, locals, fromlist, level) builtins.__import__ = skip_imports Ref: # Built-in Functions — Python 3.12.1 documentation Python - How to use the import function to import a name from a submodule? - Stack Overflow ","date":"7 April 2024","permalink":"https://fromkk.com/posts/import-in-python/","section":"Posts","summary":"\u003cp\u003eIt\u0026rsquo;s known that Python\u0026rsquo;s \u003ccode\u003eimport\u003c/code\u003e statement is implemented by \u003ccode\u003e__import__\u003c/code\u003e function. In general, if we want to import a module dynamically, we can use \u003ccode\u003eimport_module\u003c/code\u003e function, which is a wrapper around \u003ccode\u003e__import__\u003c/code\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe most important difference between these two functions is that import_module() returns the specified package or module (e.g. pkg.mod), while \u003cstrong\u003eimport\u003c/strong\u003e() returns the top-level package or module (e.g. pkg). \u0026ndash; \u003ca href=\"https://docs.python.org/3/library/importlib.html#importlib.import_module\" target=\"_blank\" rel=\"noreferrer\"\u003ehttps://docs.python.org/3/library/importlib.html#importlib.import_module\u003c/a\u003e\u003c/p\u003e","title":"__import__ in Python"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/git/","section":"Tags","summary":"","title":"Git"},{"content":"The disk performance in WSL2 is poor, it takes a long time to run git status in a host\u0026rsquo;s repo. Moreover, if you set a fancy shell prompt, it will take a long time to show the prompt. This article will introduce how to speed up Git in WSL2.\nHow to speed up Git Command #The performance of file system in WSL2 is poor, it takes a long time to run git status in a host\u0026rsquo;s repo. The solution is to use git.exe in Windows folder. You can add this into your bashrc:\nfunction git() { if $(pwd -P | grep -q \u0026#34;^\\/mnt\\/c\\/*\u0026#34;); then git.exe \u0026#34;$@\u0026#34; else command git \u0026#34;$@\u0026#34; fi } How to speed up Shell Prompt #If you have configured a fancy shell prompt, powerlevel10k for example, it will automatically get the git status when you enter a git repo. It will take a long time to show the prompt inside a host\u0026rsquo;s repo. You can accelerate it with two methods. The first one is disable git status in prompt. You may edit the .p10k.zsh file and comment the vcs prompt element. Therefor, it will not get git status when enter a git repo. However, you can\u0026rsquo;t see the git status though you are in WSL repo.\nThe second way is to disable untracked file check. You can run this command to disable it:\n# stop checking for unstaged and staged changes git config bash.showdirtystate false # stop checking for untracked files git config bash.showuntrackedfiles false In this way, you can still see other git status such as branch name and staged files with a instant response.\nRef # Git status and checkout very slow on some repository Faster git status under WSL2 ","date":"26 December 2023","permalink":"https://fromkk.com/posts/speed-up-git-speed-in-wsl/","section":"Posts","summary":"\u003cp\u003eThe disk performance in WSL2 is poor, it takes a long time to run \u003ccode\u003egit status\u003c/code\u003e in a host\u0026rsquo;s repo. Moreover, if you set a fancy shell prompt, it will take a long time to show the prompt. This article will introduce how to speed up Git in WSL2.\u003c/p\u003e","title":"Improve Git speed in WSL"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/ipod/","section":"Tags","summary":"","title":"IPod"},{"content":"I bought a iPod Video 5.5th Gen 80G recently. It\u0026rsquo;s only 200 Yuan (about $30) and I\u0026rsquo;m satisfied with it.\nRockbox #The original firmware supports few audio format, it even can\u0026rsquo;t play FLAC. I install rockbox on it, which support FLAC and other format and I can transfer music without using iTunes or Finder. It also support theme and plugin, which makes it more powerful.\nMacPod error #If you restore the iPod on macOS, it raises Warning: This is a MacPod, Rockbox only runs on WinPods. See http://www.rockbox.org/wiki/IpodConversionToFAT32 during installation. The easiest way to fix this is to restore it on Windows.\nPermission denied error #When I tried to install rockbox 1.5.1 on macOS, it raised could not open ipod permission denied when I clicked install. Using sudo /Applications/RockboxUtility.app/Contents/MacOS/RockboxUtility can fix this issue. Some said using 1.4.1 on other OS can fix this issue, but I haven\u0026rsquo;t tried it.\nHangs on Waiting for system to remount player #My iPod hangs on Waiting for system to remount player when I install rockbox. After timeout, I disconnected iPod and restart again. The startup screen shows Can't load rockbox.ipod: file not found. I connect iPod to computer and use rockbox utility to install rockbox again. I unchecked the bootloader, and only install rockbox, fonts and Plugin Data. The error is gone.\nTheme #There are many themes in rockbox. I prefer fresh os light and adwaitapod Simplified. They also provide the dark version.\nCustom font #See https://d00k.net/wiki/rockbox_advanced/font_combining/\nReplace SSD #The original HDD is small, slow and fragile comparing with SSD, you can replace it with a SSD.\nDifferent Adapters # CE to m2 adaptor (chip: JMB20330) and a 2242 m.2 SATA SSD (Recommended) CE to TF card adaptor (adaptor is expensive but longer battery life) CE/ZIF SSD (The product discontinued) SSD Size #Not all iPod OS can support large SSD. It has the maximum track limit and SSD size limit in default OS. The track limitation stems from the RAM size, the large capacity model comes with more RAM and higher track threshold. For IPC 6th and 6.5th, if you release a SSD which is larger than 128G, iTunes only recognizes 128G. This is due to the LBA28 Limitation. Both Limitations can be eliminated by rockbox. If you want to stay in the original OS, I recommend you buying a 5.5th Gen 80GB or 7th Gen.\nModel Description Model No. iTunes Storage Limit (see note below) 5th Gen 30Gb MA002 / MA146 / PA002 / PA146 ~20000 Tracks 5th Gen 60Gb MA003 / MA147 / PA003 / PA147 ~50000 Tracks 5.5th Gen 30Gb MA444 / MA446 / PA444 / PA446 / MA664 ~20000 Tracks 5.5th Gen 80Gb MA448 / MA450 / PA448 / PA450 ~50000 Tracks 6th Gen 80Gb MB029 / MB147 / PB029 128Gb / ~50000 Tracks 6th Gen 160Gb MB145 / MB150 / PB145 / PB150 128Gb / ~50000 Tracks / Requires Ribbon 6.5th Gen 120Gb MB565 / PB565 / MB562 / PB562 128Gb / ~50000 Tracks 7th Gen 160Gb PC297 / MC297 / PC293 / MC293 ~50000 Tracks Source: https://www.iflash.xyz/store/iflash-compatibility/\nGuide #iPod 5th Generation (Video) Hard Drive Replacement\nReplace battery #iPod comes with a 580 mah or 850 mah battery base on the thin or thick back cover of your iPod.\nAfter replace the SSD, there is a bigger space for battery. You can replace a larger battery to get longer battery life. Here is the guide: https://www.ifixit.com/Guide/iPod+Classic+Battery+Replacement/561.\nSome seller even provide a 3000 mah battery, I\u0026rsquo;m not sure whether your iPod has enough space for it, please as the seller before buying it. Some said the 3000 mah battery is fake, it\u0026rsquo;s actually a 1800 mah battery. Source: 3rd party extended Battery guide\nGuide #iPod 5th Generation (Video) Hard Drive Replacement\nRef # Topic: iPod classic 6th Gen LBA28 how to exceed 128gb limit? SELLTOONE 128GB SSD for iPod Classic 6th 7th iPod Video 5Gen 5.5th Replace HS081HA MK8010GAH MK8022GAA MK1634GAL MK1231GAL ZIF CE Solid State Drive (128GB) iPod 6/6.5 gen iFlash and Rockbox limitations? ","date":"26 December 2023","permalink":"https://fromkk.com/posts/ipod-video-review/","section":"Posts","summary":"\u003cp\u003eI bought a iPod Video 5.5th Gen 80G recently. It\u0026rsquo;s only 200 Yuan (about $30) and I\u0026rsquo;m satisfied with it.\u003c/p\u003e\n\u003ch2 id=\"rockbox\" class=\"relative group\"\u003eRockbox \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#rockbox\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eThe original firmware supports few audio format, it even can\u0026rsquo;t play FLAC. I install rockbox on it, which support FLAC and other format and I can transfer music without using iTunes or Finder. It also support theme and plugin, which makes it more powerful.\u003c/p\u003e","title":"iPod Video Review"},{"content":"In [Packaging] Support Python 3.11 by bebound · Pull Request #26923 · Azure/azure-cli (github.com) , I bumped azure-cli to use Python 3.11. We\u0026rsquo;ve bump the dependency in other PRs, I thought it should be a small PR, but in the end, a lot of changes are made.\nargs.getargspec #getargspec is dropped in 3.11. You can easily replaced it with getfullargspec . It returns FullArgSpec(args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations) instead of ArgSpec(args, varargs, keywords, defaults) So args, _, kw, _ = inspect.getargspec(fn) can be replaced by args, _, kw, *_ = inspect.getfullargspec(fn) However, getfullargspec is retained primarily for use in code that needs to maintain compatibility with the Python 2 inspect module API.\nNote that signature() and Signature Object provide the recommended API for callable introspection, and support additional behaviours (like positional-only arguments) that are sometimes encountered in extension module APIs. This function is retained primarily for use in code that needs to maintain compatibility with the Python 2 inspect module API. \u0026ndash;inspect \u0026mdash; Inspect live objects \u0026mdash; Python 3.11.4 documentation\nThe modern signature function provides the similar result but needs more modification:\nimport inspect def testfunc(a, /, b=1, c=2, *args, kk, **kwargs): pass print(inspect.getfullargspec(testfunc)) print(inspect.signature(testfunc).parameters) for i, j in inspect.signature(testfunc).parameters.items(): print(i, type(i), j, type(j), j.kind) args, _, kw, *_ = inspect.getfullargspec(testfunc) print(args, kw) from inspect import Parameter parameters = inspect.signature(testfunc).parameters args = [k for k, v in parameters.items() if v.kind in {Parameter.POSITIONAL_OR_KEYWORD, Parameter.POSITIONAL_ONLY}] kw = next(iter([k for k, v in parameters.items() if v.kind == Parameter.VAR_KEYWORD]), None) print(args, kw) FullArgSpec(args=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;], varargs=\u0026#39;args\u0026#39;, varkw=\u0026#39;kwargs\u0026#39;, defaults=(1, 2), kwonlyargs=[\u0026#39;kk\u0026#39;], kwonlydefaults=None, annotations={}) OrderedDict([(\u0026#39;a\u0026#39;, \u0026lt;Parameter \u0026#34;a\u0026#34;\u0026gt;), (\u0026#39;b\u0026#39;, \u0026lt;Parameter \u0026#34;b=1\u0026#34;\u0026gt;), (\u0026#39;c\u0026#39;, \u0026lt;Parameter \u0026#34;c=2\u0026#34;\u0026gt;), (\u0026#39;args\u0026#39;, \u0026lt;Parameter \u0026#34;*args\u0026#34;\u0026gt;), (\u0026#39;kk\u0026#39;, \u0026lt;Parameter \u0026#34;kk\u0026#34;\u0026gt;), (\u0026#39;kwargs\u0026#39;, \u0026lt;Parameter \u0026#34;**kwargs\u0026#34;\u0026gt;)]) a \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; a \u0026lt;class \u0026#39;inspect.Parameter\u0026#39;\u0026gt; POSITIONAL_ONLY b \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; b=1 \u0026lt;class \u0026#39;inspect.Parameter\u0026#39;\u0026gt; POSITIONAL_OR_KEYWORD c \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; c=2 \u0026lt;class \u0026#39;inspect.Parameter\u0026#39;\u0026gt; POSITIONAL_OR_KEYWORD args \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; *args \u0026lt;class \u0026#39;inspect.Parameter\u0026#39;\u0026gt; VAR_POSITIONAL kk \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; kk \u0026lt;class \u0026#39;inspect.Parameter\u0026#39;\u0026gt; KEYWORD_ONLY kwargs \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; **kwargs \u0026lt;class \u0026#39;inspect.Parameter\u0026#39;\u0026gt; VAR_KEYWORD [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;] kwargs [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;] kwargs Enum __format__ change #There is some custom classes in azure-cli, which makes Foo.BAR=\u0026lsquo;bar\u0026rsquo;. In 3.11, the [[https://docs.python.org/3/whatsnew/3.11.html#enum][Enum]] =__format__() changes, it returns the enum and member name (ex: Color.RED). (The __str__ method is the same as Python 3.10)\nChanged Enum.__format__() (the default for format(), str.format() and f-strings) to always produce the same result as Enum.__str__(): for enums inheriting from ReprEnum it will be the member\u0026rsquo;s value; for all other enums it will be the enum and member name (e.g. Color.RED). \u0026ndash;What\u0026rsquo;s New In Python 3.11\nfrom enum import Enum class Foo(str, Enum): BAR = \u0026#34;bar\u0026#34; # Python 3.10 f\u0026#34;{Foo.BAR}\u0026#34; # \u0026gt; bar str(Foo.BAR) # \u0026gt; Foo.BAR # Python 3.11 f\u0026#34;{Foo.BAR}\u0026#34; # \u0026gt; Foo.BAR str(Foo.BAR) # \u0026gt; Foo.BAR The standard way to replace Foo class is StrEnum\nclass Foo(StrEnum): BAR = \u0026#34;bar\u0026#34; # Python 3.11 f\u0026#34;{Foo.BAR}\u0026#34; # \u0026gt; bar If you also use Bar(int, Enum), you can replace it with ReprEnum: Bar(int, ReprEnum).\nunittest.Mock #The unittest module replace unittest.mock._importer with pkgutil.resolve_name in bpo-44686 replace unittest.mock._importer with pkgutil.resolve_name by graingert · Pull Request #18544 · python/cpython (github.com), which also introduces some changes.\nPreviously, it use __import__ to import the patch target, which does not check the module name. But pkgutil.resolve_name will check name first, thus mock.patch fails if the target is not a valid Python module name. For example, this statement fails in 3.11:\n@mock.patch(\u0026#39;azure.cli.command_modules.vm.aaz.2020_09_01_hybrid.network.vnet.List\u0026#39;, _mock_network_client_with_existing_vnet_location) as 2020_09_01_hybrid is not a valid variable name in Python.\n_NAME_PATTERN = re.compile(f\u0026#39;^(?P\u0026lt;pkg\u0026gt;{dotted_words})\u0026#39; f\u0026#39;(?P\u0026lt;cln\u0026gt;:(?P\u0026lt;obj\u0026gt;{dotted_words})?)?$\u0026#39;, re.UNICODE) m = _NAME_PATTERN.match(name) if not m: \u0026gt; raise ValueError(f\u0026#39;invalid format: {name!r}\u0026#39;) E ValueError: invalid format: \u0026#39;azure.cli.command_modules.vm.aaz.2020_09_01_hybrid.network.vnet\u0026#39; As a workaround, mock.patch.object works.\nvnet = import_module(\u0026#39;azure.cli.command_modules.vm.aaz.2018_03_01_hybrid.network.vnet\u0026#39;) with mock.patch.object(vnet, \u0026#39;List\u0026#39;, _mock_network_client_with_existing_vnet): The ultimate solution is fix module name.\nargparse.ArgumentError #bpo-39716: Raise on conflicting subparser names. by anntzer · Pull Request #18605 · python/cpython (github.com) Raise an ArgumentError when the same subparser name is added twice.\nimport argparse parser = argparse.ArgumentParser() t = parser.add_subparsers() t.add_parser(\u0026#39;a\u0026#39;) t.add_parser(\u0026#39;a\u0026#39;) The above code works on 3.10 but raises this error in 3.11:\nTraceback (most recent call last): File \u0026#34;C:\\Users\\kk\\Developer\\azure-cli\\p.py\u0026#34;, line 6, in \u0026lt;module\u0026gt; t.add_parser(\u0026#39;a\u0026#39;) File \u0026#34;C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\argparse.py\u0026#34;, line 1192, in add_parser raise ArgumentError(self, _(\u0026#39;conflicting subparser: %s\u0026#39;) % name) argparse.ArgumentError: argument {a}: conflicting subparser: a Ref # Enum with str or int Mixin Breaking Change in Python 3.11 (pecar.me) Enum with str or int Mixin Breaking Change in Python 3.11 · Issue #100458 · python/cpython (github.com) What\u0026rsquo;s New In Python 3.11 \u0026mdash; Python 3.11.4 documentation ","date":"10 December 2023","permalink":"https://fromkk.com/posts/python-3-dot-11-changes/","section":"Posts","summary":"\u003cp\u003eIn \u003ca href=\"https://github.com/Azure/azure-cli/pull/26923\" target=\"_blank\" rel=\"noreferrer\"\u003e[Packaging] Support Python 3.11 by bebound · Pull Request #26923 · Azure/azure-cli (github.com)\u003c/a\u003e , I bumped azure-cli to use Python 3.11. We\u0026rsquo;ve bump the dependency in other PRs, I thought it should be a small PR, but in the end, a lot of changes are made.\u003c/p\u003e","title":"Python 3.11 changes"},{"content":"When working on a project with multiple developers, the line ending can be troublesome. This article will explain how to configure line ending in Git.\nBasic configuration #The line ending on Windows is CRLF, on Linux is LF. To prevent the line ending issue, we can set core.autocrlf to true on Windows to let git convert CRLF to LF when commit, and convert LF to CRLF when checkout. It is automatically configured if you install git on Windows.\nConfiguring Git to handle line endings - GitHub Docs\n# Configure Git to ensure line endings in files you checkout are correct for Windows. # For compatibility, line endings are converted to Unix style when you commit files. $ git config --global core.autocrlf true Advanced configuration #You can also use .gitattributes to control the line ending in each repository. The .gitattributes file is a text file that tells Git how to handle files in the repository. You can specify the line ending of each file type in this file.\nAuto convert line ending #With * text=auto, Git handles the files in whatever way it thinks is best. This is a good default option.\nUse *.c text to explicitly declare a file as a text file, so this file is always normalized and converted to native line endings on checkout.\nUse *.png binary to explicitly declare a file as binary, so Git does not convert it. (binray is an alias for -text -diff)\nForce conversion when checkout: #You can use eol to force conversion when checkout. The following config enforces bat files to be converted to CRLF when checkout even on Mac and Linux.\n* text=auto *.bat eol=crlf This is the result of git ls-files --eol on Windows and Linux:\ngit ls-files --eol src/azure-cli/az.bat i/lf w/crlf attr/text=auto eol=crlf src/azure-cli/az.bat i means the index, w means the working tree, attr means the attribute used when checking out or committing.\nYou can set eof to crlf or lf. If it\u0026rsquo;s not specified, the line ending will be determined by core.autocrlf or core.eol. If text is set but neither of those variables are set, then the default value is crlf on Windows and lf on Linux and Mac.\nRefresh setting #If you change the .gitattributes file, you need to run the following command to refresh the working tree.\n# Please commit the .gitattributes changes before run this command. git rm -rf --cached . git reset --hard HEAD Extra # Line endings in tarball also follows the .gitattributes. It\u0026rsquo;s identical to Git checkout on Linux machine. The .gitattributes settings will only affect new commits. If you want to change the line ending of the files that already in the Git index after changing line ending settings, you can use git add --renormalize . to force Git to refresh all tracked files. For example, if the bat file has been add as crlf in Git index and then you set it as text in .gitattributes. Running this command asks Git change it to lf in index. Ref # gitattributes - Defining attributes per path Configuring Git to handle line endings - GitHub Docs {CI} Enforce LF-only line endings in git #27137 · Azure/azure-cli (github.com) Git - git-add Documentation \u0026ndash;renormalize ","date":"21 October 2023","permalink":"https://fromkk.com/posts/line-ending-in-git/","section":"Posts","summary":"\u003cp\u003eWhen working on a project with multiple developers, the line ending can be troublesome. This article will explain how to configure line ending in Git.\u003c/p\u003e\n\u003ch2 id=\"basic-configuration\" class=\"relative group\"\u003eBasic configuration \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#basic-configuration\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eThe line ending on Windows is \u003ccode\u003eCRLF\u003c/code\u003e, on Linux is \u003ccode\u003eLF\u003c/code\u003e. To prevent the line ending issue, we can set \u003ccode\u003ecore.autocrlf\u003c/code\u003e to \u003ccode\u003etrue\u003c/code\u003e on Windows to let git convert \u003ccode\u003eCRLF\u003c/code\u003e to \u003ccode\u003eLF\u003c/code\u003e when commit, and convert \u003ccode\u003eLF\u003c/code\u003e to \u003ccode\u003eCRLF\u003c/code\u003e when checkout. It is automatically configured if you install git on Windows.\u003c/p\u003e","title":"Line Ending in Git"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/dockerfile/","section":"Tags","summary":"","title":"Dockerfile"},{"content":"It\u0026rsquo;s very common to copy a local file into the container when build docker image. In general, we use COPY command. But it creates a new layer and increase the final image size. If this is a temporal file and we don\u0026rsquo;t want users waste their storage space, how can we remove it? Here are some approaches.\nDownload the File Dynamically #If the file can be download from URL or you can create a local HTTP server to share the file, you can download the file, use it and delete it in one RUN command. For example:\nRUN wget xxxx \u0026amp;\u0026amp; unzip xxx \u0026amp;\u0026amp; rm xxx RUN --mount Command #You can also mount file when build image if your file can\u0026rsquo;t be download from Internet or the file is secret. Use it to bind files or directories to the build container.\nA bind mount is read-only by default, add rw parameter to make it writable. The changes during the build are discared after the build is complete.\nThe mounted folder are kept in the image, but the files are gone. Don\u0026rsquo;t forget to delete the empty folder if you want to keep image clean.\nMount folder #RUN --mount=type=bind,target=/target_dir/,source=./source_dir/,rw Mount file #RUN --mount=type=bind,target=/azure-cli.rpm,source=./docker/azure-cli.rpm tdnf install ca-certificates /azure-cli.rpm -y \u0026amp;\u0026amp; tdnf clean all --squash option in docker build #You can also use --squash to reduce image size. Once the build is complete, Docker creates a new image loading the diffs from each layer into a single new layer and references all the parent\u0026rsquo;s layers. So the extra space created by COPY command can be freed by squash.\nRef # Dockerfile best practice RUN \u0026ndash;mount How does the new Docker \u0026ndash;squash work ","date":"24 August 2023","permalink":"https://fromkk.com/posts/how-to-copy-files-temporarily-in-dockerfile/","section":"Posts","summary":"\u003cp\u003eIt\u0026rsquo;s very common to copy a local file into the container when build docker image. In general, we use \u003ccode\u003eCOPY\u003c/code\u003e command. But it creates a new layer and increase the final image size. If this is a temporal file and we don\u0026rsquo;t want users waste their storage space, how can we remove it? Here are some approaches.\u003c/p\u003e","title":"How to copy files temporarily in Dockerfile"},{"content":"There is a historical memory leak problem in our Django app and I fixed it recently. As time goes by, the memory usage of app keeps growing and so does the CPU usage.\nAfter some research, I figure out the cause. Some views does not close multiprocessing.Pool after using it. The problem disappears when I use Pool with with statement.\nBut I\u0026rsquo;m still interested in it and wrote some testing code. The script is run in Python 3.6.8 and produce similar result when using multiprocessing.ThreadPool.\nimport time from multiprocessing import Pool def func(i): return i def ori(): # create many thread as time goes by, when i==300 cpu grow to 300%, run out of 16g ram and stuck I have to kill process p = Pool(4) r.append(p.map(func, range(4))) def with_close(): # 100% cpu, 0.1 ram, create 40 thread, takes 41s p = Pool(4) r.append(p.map(func, range(4))) p.close() def with_terminate(): # 5% cpu, 0.1 ram, create 4 thread, takes 425s p = Pool(4) r.append(p.map(func, range(4))) p.terminate() def with_with(): # same as terminate with Pool(4) as p: r.append(p.map(func, range(4))) r = [] s = time.time() for i in range(4000): ori() # with_close() # with_terminate() # with_with() if i % 100 == 0: print(i) print(f\u0026#39;takes {time.time() - s} seconds\u0026#39;) As you can see, there are four functions. The ori function is Pool with no close and terminate, the RAM keeps growing and the script stuck. with_close, with_terminate and with_with will exit normally but time is different.\nWhy close() is faster than terminate() #Pool.terminate() will call terminate() in each worker. Pool.close() just change the pool states and each worker will terminate itself. You can find the source code on GitHub.\nVerify Memory Leak #import gc import time import weakref from multiprocessing import Pool def func(i): return i p = Pool(4) wr = weakref.ref(p) p.map(func, range(4)) print(wr()) print(gc.get_referents(wr())) # p.close() # p.terminate() time.sleep(1) del p gc.collect() print(wr()) print(gc.get_referents(wr())) If not calling close or terminate, after execution, p is still referred by some objects:\n\u0026lt;multiprocessing.pool.Pool object at 0x7fc0e6db0828\u0026gt; [{\u0026#39;_ctx\u0026#39;: \u0026lt;multiprocessing.context.ForkContext object at 0x7fc0e6d455c0\u0026gt;, \u0026#39;_inqueue\u0026#39;: \u0026lt;multiprocessing.queues.SimpleQueue object at 0x7fc0e6db0860\u0026gt;, \u0026#39;_outqueue\u0026#39;: \u0026lt;multiprocessing.queues.SimpleQueue object at 0x7fc0e5babac8\u0026gt;, \u0026#39;_quick_put\u0026#39;: \u0026lt;bound method _ConnectionBase.send of \u0026lt;multiprocessing.connection.Connection object at 0x7fc0e620e8d0\u0026gt;\u0026gt;, \u0026#39;_quick_get\u0026#39;: \u0026lt;bound method _ConnectionBase.recv of \u0026lt;multiprocessing.connection.Connection object at 0x7fc0e4d241d0\u0026gt;\u0026gt;, \u0026#39;_taskqueue\u0026#39;: \u0026lt;queue.Queue object at 0x7fc0e4d24320\u0026gt;, \u0026#39;_cache\u0026#39;: {}, \u0026#39;_state\u0026#39;: 0, \u0026#39;_maxtasksperchild\u0026#39;: None, \u0026#39;_initializer\u0026#39;: None, \u0026#39;_initargs\u0026#39;: (), \u0026#39;_processes\u0026#39;: 4, \u0026#39;_pool\u0026#39;: [\u0026lt;ForkProcess(ForkPoolWorker-1, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-2, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-3, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-4, started daemon)\u0026gt;], \u0026#39;_worker_handler\u0026#39;: \u0026lt;Thread(Thread-1, started daemon 140466410379008)\u0026gt;, \u0026#39;_task_handler\u0026#39;: \u0026lt;Thread(Thread-2, started daemon 140466401986304)\u0026gt;, \u0026#39;_result_handler\u0026#39;: \u0026lt;Thread(Thread-3, started daemon 140466393593600)\u0026gt;, \u0026#39;_terminate\u0026#39;: \u0026lt;Finalize object, callback=_terminate_pool, args=(\u0026lt;queue.Queue object at 0x7fc0e4d24320\u0026gt;, \u0026lt;multiprocessing.queues.SimpleQueue object at 0x7fc0e6db0860\u0026gt;, \u0026lt;multiprocessing.queues.SimpleQueue object at 0x7fc0e5babac8\u0026gt;, [\u0026lt;ForkProcess(ForkPoolWorker-1, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-2, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-3, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-4, started daemon)\u0026gt;], \u0026lt;Thread(Thread-1, started daemon 140466410379008)\u0026gt;, \u0026lt;Thread(Thread-2, started daemon 140466401986304)\u0026gt;, \u0026lt;Thread(Thread-3, started daemon 140466393593600)\u0026gt;, {}), exitprority=15\u0026gt;}, \u0026lt;class \u0026#39;multiprocessing.pool.Pool\u0026#39;\u0026gt;] \u0026lt;multiprocessing.pool.Pool object at 0x7fc0e6db0828\u0026gt; [{\u0026#39;_ctx\u0026#39;: \u0026lt;multiprocessing.context.ForkContext object at 0x7fc0e6d455c0\u0026gt;, \u0026#39;_inqueue\u0026#39;: \u0026lt;multiprocessing.queues.SimpleQueue object at 0x7fc0e6db0860\u0026gt;, \u0026#39;_outqueue\u0026#39;: \u0026lt;multiprocessing.queues.SimpleQueue object at 0x7fc0e5babac8\u0026gt;, \u0026#39;_quick_put\u0026#39;: \u0026lt;bound method _ConnectionBase.send of \u0026lt;multiprocessing.connection.Connection object at 0x7fc0e620e8d0\u0026gt;\u0026gt;, \u0026#39;_quick_get\u0026#39;: \u0026lt;bound method _ConnectionBase.recv of \u0026lt;multiprocessing.connection.Connection object at 0x7fc0e4d241d0\u0026gt;\u0026gt;, \u0026#39;_taskqueue\u0026#39;: \u0026lt;queue.Queue object at 0x7fc0e4d24320\u0026gt;, \u0026#39;_cache\u0026#39;: {}, \u0026#39;_state\u0026#39;: 0, \u0026#39;_maxtasksperchild\u0026#39;: None, \u0026#39;_initializer\u0026#39;: None, \u0026#39;_initargs\u0026#39;: (), \u0026#39;_processes\u0026#39;: 4, \u0026#39;_pool\u0026#39;: [\u0026lt;ForkProcess(ForkPoolWorker-1, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-2, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-3, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-4, started daemon)\u0026gt;], \u0026#39;_worker_handler\u0026#39;: \u0026lt;Thread(Thread-1, started daemon 140466410379008)\u0026gt;, \u0026#39;_task_handler\u0026#39;: \u0026lt;Thread(Thread-2, started daemon 140466401986304)\u0026gt;, \u0026#39;_result_handler\u0026#39;: \u0026lt;Thread(Thread-3, started daemon 140466393593600)\u0026gt;, \u0026#39;_terminate\u0026#39;: \u0026lt;Finalize object, callback=_terminate_pool, args=(\u0026lt;queue.Queue object at 0x7fc0e4d24320\u0026gt;, \u0026lt;multiprocessing.queues.SimpleQueue object at 0x7fc0e6db0860\u0026gt;, \u0026lt;multiprocessing.queues.SimpleQueue object at 0x7fc0e5babac8\u0026gt;, [\u0026lt;ForkProcess(ForkPoolWorker-1, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-2, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-3, started daemon)\u0026gt;, \u0026lt;ForkProcess(ForkPoolWorker-4, started daemon)\u0026gt;], \u0026lt;Thread(Thread-1, started daemon 140466410379008)\u0026gt;, \u0026lt;Thread(Thread-2, started daemon 140466401986304)\u0026gt;, \u0026lt;Thread(Thread-3, started daemon 140466393593600)\u0026gt;, {}), exitprority=15\u0026gt;}, \u0026lt;class \u0026#39;multiprocessing.pool.Pool\u0026#39;\u0026gt;] After calling close() or terminate(), the last two lines become:\nNone [] Document Update #The Python3.7 document adds this warning:\nmultiprocessing.pool objects have internal resources that need to be properly managed (like any other resource) by using the pool as a context manager or by calling close() and terminate() manually. Failure to do this can lead to the process hanging on finalization. Note that is not correct to rely on the garbage collector to destroy the pool as CPython does not assure that the finalizer of the pool will be called (see object.__del__() for more information).\nThe Bug is Fixed in Python 3.8 #In python 3.8.6, the script exits normally and the total execution time also decreases without calling close(). I found this issue is fixed in Python bug tracker: multiprocessing.Pool and ThreadPool leak resources after being deleted.\n","date":"16 March 2022","permalink":"https://fromkk.com/posts/memory-leak-in-python-multiprocessing-dot-pool/","section":"Posts","summary":"\u003cp\u003eThere is a historical memory leak problem in our Django app and I fixed it recently. As time goes by, the memory usage of app keeps growing and so does the CPU usage.\u003c/p\u003e\n\n  \n  \n  \n  \n  \n\n  \n  \n    \n    \n  \n  \u003cfigure class=\"mx-auto my-0 rounded-md\"\u003e\n    \u003cimg src=\"/images/pool_before.png\" alt=\"\" class=\"mx-auto my-0 rounded-md\"/\u003e\n    \n  \u003c/figure\u003e\n\n\n\u003cp\u003eAfter some research, I figure out the cause. Some views does not close \u003ccode\u003emultiprocessing.Pool\u003c/code\u003e after using it. The problem disappears when I use \u003ccode\u003ePool\u003c/code\u003e with \u003ccode\u003ewith\u003c/code\u003e statement.\u003c/p\u003e","title":"Memory Leak in Python multiprocessing.Pool"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/emacs/","section":"Tags","summary":"","title":"Emacs"},{"content":"Auto Switch Input Method in Evil #This setting makes it possible to switch input method based on the context of cursor when entering insert mode.\nsis #I\u0026rsquo;m using sis package with this configuration. You may need to install macism if you\u0026rsquo;re not using railwaycat/emacsmacport. More settings can be found in emacs-smart-input-source.\n(sis-ism-lazyman-config \u0026#34;com.apple.keylayout.US\u0026#34; \u0026#34;com.apple.inputmethod.SCIM.ITABC\u0026#34;) (sis-global-cursor-color-mode t) (sis-global-respect-mode t) (sis-global-context-mode t) (sis-global-inline-mode t) fcitx #You can also install fcitx-remote for-osx and use cute-jumper/fcitx.el to do so. As homebrew no longer support some build options, you need to follow the install instructions in the GitHub repository to build fcitx.\nMono Chinese Font #I use a 14pt English font and 16pt Chinese font, one Chinese character is the same width as two English characters. It can be set by adding this into Emacs configuration file.\ndotspacemacs-default-font \u0026#39;(\u0026#34;Menlo\u0026#34; :size 14.0 :weight normal :width normal) ;; add into dotspacemacs/user-config() (dolist (charset \u0026#39;(kana han symbol cjk-misc bopomofo)) (set-fontset-font (frame-parameter nil \u0026#39;font) charset (font-spec :family \u0026#34;PingFang SC\u0026#34; :size 16))) If you enable the chinese layer in Spacemacs, it provides a more convenient function:\n(spacemacs//set-monospaced-font \u0026#34;Menlo\u0026#34; \u0026#34;PingFang SC\u0026#34; 14 16) PS: valign provides visual alignment for Org Mode and Markdown without changing fonts.\nRef # Spacemacs - Chinese Layer Emacs 中文环境配置 ","date":"17 February 2022","permalink":"https://fromkk.com/posts/emacs-chinese-related-settings/","section":"Posts","summary":"\u003ch2 id=\"auto-switch-input-method-in-evil\" class=\"relative group\"\u003eAuto Switch Input Method in Evil \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#auto-switch-input-method-in-evil\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eThis setting makes it possible to switch input method based on the context of cursor when entering insert mode.\u003c/p\u003e\n\u003ch3 id=\"sis\" class=\"relative group\"\u003esis \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#sis\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h3\u003e\u003cp\u003eI\u0026rsquo;m using \u003ccode\u003esis\u003c/code\u003e package with this configuration. You may need to install \u003ccode\u003emacism\u003c/code\u003e if you\u0026rsquo;re not using \u003ccode\u003erailwaycat/emacsmacport\u003c/code\u003e. More settings can be found in \u003ca href=\"https://github.com/laishulu/emacs-smart-input-source\" target=\"_blank\" rel=\"noreferrer\"\u003eemacs-smart-input-source\u003c/a\u003e.\u003c/p\u003e","title":"Emacs Chinese-related Settings"},{"content":"My first NAS is Synology DS120j, which is ARM based entry level product. It\u0026rsquo;s okay to use it for downloading and backup, but not power enough for running docker and virtual machine.\nSo I bought this NAS last month, and I\u0026rsquo;m satisfied with it. Here are the advantages and disadvantages.\nAdvantages # High performance.\nIt is equipped with J4125 quad-core 2.0 GHz processor, 8G RAM, two 2.5G Ports and 4 bays. Here is the spec. Although J4125 is not the fastest CPU in 2022(the newer model coming with N5105), it is still able to run several docker containers together, and I can even run Synology and Windows 10 inside build-in Virtualization Station.\nLow Price.\nI bought is at 2050 Yuan (about $320). Synology has similar model DS920+, which is 70% more expensive.\nQtier\nA special feature only available on QNAP. It will automatically move hot and cold data between SSD and HDD to get higher performance. There is one noticeable change: this NAS is so quiet and you can hardly hear HDD running.\nDisadvantages # No native HEIC Support\nThis means you can\u0026rsquo;t preview the photos imported from iPhone. You have to pay for $12 to buy CAYIN MediaSign Player to fix this.\nSlow Start and Shutdown\nI know it\u0026rsquo;s rarely to restart NAS, but taking several minutes to restart is not acceptable. I hope QNAP engineers can improve this in the future.\nDocker Images Recommendation #Here are some docker images I think is useful:\njohngong/qbittorrent A torrent client which can also download new file through RSS feed.\np3terx/aria2-pro Everyone knows what is aria2.\ndreamacro/clash A rule based proxy client.\njeessy/ddns-go A simple DDNS client.\nlinuxserver/plex Plex is app which helps you to manage and browser your media library. It can grab the metadata of TV shows, movies and music from Internet. It\u0026rsquo;s provide app in different platform so you can access your media from anywhere.\nThere is only one thing I think that needs to improve: playback speed is fixed, which is not convenient when watching animation.\nYou can also give Emby or Jellyfin a shot.\nportainer/portainer-ce QNAP has build-in docker-compose command. You can use this Web app if you prefer GUI.\nvaultwarden/server I deploy this on my VPS instead of NAS as port 443 is forbidden on NAS. It\u0026rsquo;s a alternative of 1Password. Although the app UI is not perfect, it has all the function required by a password manager. There is no official way to backup data, so I use crontab to run backup script to save data to Google Drive by Rclone.\nThe backup script is quite simple, you need to link your Google Drive account as google in Rclone before using this.\n#!/bin/sh pwd echo backing up rm bit.zip zip -rq bit.zip ./data -x ./data/icon_cache/* ./data/bitwarden.log rclone copy bit.zip google:/应用/vaultwarden ","date":"19 January 2022","permalink":"https://fromkk.com/posts/qnap-ts-453bmini-review/","section":"Posts","summary":"\u003cp\u003eMy first NAS is Synology DS120j, which is ARM based entry level product. It\u0026rsquo;s okay to use it for downloading and backup, but not power enough for running docker and virtual machine.\u003c/p\u003e\n\u003cp\u003eSo I bought this NAS last month, and I\u0026rsquo;m satisfied with it. Here are the advantages and disadvantages.\u003c/p\u003e","title":"QNAP TS-453Dmini Review"},{"content":"Today I tried to delete an inactive Internet account on system preference. It was deleted successfully but come back again after 20 seconds. This drives me nuts.\nI tried these methods, but none of them works.\nBoot in safe mode, delete account. Delete record in ZACCOUNT table in ~/Library/Accounts/Accounts4.sqlite. Delete related items in Keychain Access app. Later, RedHatDude\u0026rsquo;s answer in this post gives me a clue, it looks like a iCloud sync problem. I tried to delete the account on my 3 MacBooks together. Thank goodness! It does not show up again.\nRef # Removing Accounts from Internet Accounts Every time I delete an internet account it comes back Gmail account keeps coming back after delete ","date":"8 January 2022","permalink":"https://fromkk.com/posts/internet-account-keeps-coming-back-after-deletion-on-macos/","section":"Posts","summary":"\u003cp\u003eToday I tried to delete an inactive Internet account on system preference. It was deleted successfully but come back again after 20 seconds. This drives me nuts.\u003c/p\u003e\n\u003cp\u003eI tried these methods, but none of them works.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBoot in \u003ca href=\"https://support.apple.com/en-us/HT201262\" target=\"_blank\" rel=\"noreferrer\"\u003esafe mode\u003c/a\u003e, delete account.\u003c/li\u003e\n\u003cli\u003eDelete record in \u003ccode\u003eZACCOUNT\u003c/code\u003e table in \u003ccode\u003e~/Library/Accounts/Accounts4.sqlite\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eDelete related items in Keychain Access app.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLater, \u003ccode\u003eRedHatDude\u003c/code\u003e\u0026rsquo;s answer in \u003ca href=\"https://discussions.apple.com/thread/252924363?login=true\" target=\"_blank\" rel=\"noreferrer\"\u003ethis post\u003c/a\u003e gives me a clue, it looks like a iCloud sync problem. I tried to delete the account on my 3 MacBooks together. Thank goodness! It does not show up again.\u003c/p\u003e","title":"Internet Account Keeps Coming Back after deletion on MacOS"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/macos/","section":"Tags","summary":"","title":"MacOS"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/django/","section":"Tags","summary":"","title":"Django"},{"content":"In Django, when edit field in admin page or post data to forms, the leading and tailing whitespace in CharField and TextField are removed.\nThe reason is strip=True parameter in forms.CharField, which is added in Djagno 1.9. You can see the discussion in django tiket #4960 and here is source code. models.CharField and models.TextField use formfield() to create form to interact with user, then both of them eventually create a forms.CharField\nIt only affect the value return from forms, you can still update model manually and calling save() to save it with spaces.\nNormally, this feature help us to keep text field clean. But sometimes you may want to get the original value, and here are three different solutions:\nSuppose we have this Test model.\n# models.py class Test(models.Model): char = models.CharField(max_length=20) text = models.TextField() Change ModelAdmin ## admin.py TestAdmin(admin.ModelAdmin): def formfield_for_dbfield(self, db_field, request, **kwargs): if db_field.name in [\u0026#39;char\u0026#39;, \u0026#39;text\u0026#39;]: kwargs[\u0026#39;strip\u0026#39;] = False return super().formfield_for_dbfield(db_field, request, **kwargs) This method tackles the problem by overriding fields\u0026rsquo; default fromfiled method.\nDefine Custom Form ## forms.py class CustomForm(forms.ModelForm): char = forms.CharField(strip=False) text = forms.CharField(strip=False, widget=forms.Textarea) class Meta: model = Test exclude = [] # admin.py TestAdmin(admin.ModelAdmin): form = CustomForm Now when edit data in admin panel, the whitespace is not removed anymore.\nUse Custom Field #You can also use your custom field in models.py. For example:\n# models.py from django.db.models import TextField class NonStrippingTextField(TextField): def formfield(self, **kwargs): kwargs[\u0026#39;strip\u0026#39;] = False return super(NonStrippingTextField, self).formfield(**kwargs) class Test(models.Model): text = NonStrippingTextField() ==\nREST Framework #If you use Django REST framework to edit data, you only need to change the serializer.\nclass TestSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = Test fields = \u0026#39;__all__\u0026#39; extra_kwargs = {\u0026#34;char\u0026#34;: {\u0026#34;trim_whitespace\u0026#34;: False}, \u0026#34;text\u0026#34;: {\u0026#34;trim_whitespace\u0026#34;: False}} Ref # StackOverflow - Django TextField and CharField is stripping spaces and blank lines Djanog - TextField constructor needs a strip=False option StackOverflow - In Django REST control serializer does not automatically remove spaces Allow Whitespace to be a Valid CharField Value in Django Admin ","date":"19 December 2021","permalink":"https://fromkk.com/posts/how-to-disable-auto-strip-in-charfield-in-django/","section":"Posts","summary":"\u003cp\u003eIn Django, when edit field in admin page or post data to forms, the leading and tailing whitespace in \u003ccode\u003eCharField\u003c/code\u003e and \u003ccode\u003eTextField\u003c/code\u003e are removed.\u003c/p\u003e\n\u003cp\u003eThe reason is \u003ccode\u003estrip=True\u003c/code\u003e parameter in \u003ccode\u003eforms.CharField\u003c/code\u003e, which is added in Djagno 1.9. You can see the discussion in \u003ca href=\"https://code.djangoproject.com/ticket/4960\" target=\"_blank\" rel=\"noreferrer\"\u003edjango tiket #4960\u003c/a\u003e and here is \u003ca href=\"https://github.com/django/django/blob/4ce59f602ed28320caf3035212cb4d1c5430da2b/django/forms/fields.py#L211\" target=\"_blank\" rel=\"noreferrer\"\u003esource code\u003c/a\u003e. \u003ccode\u003emodels.CharField\u003c/code\u003e and \u003ccode\u003emodels.TextField\u003c/code\u003e use \u003ccode\u003eformfield()\u003c/code\u003e to create form to interact with user, then both of them eventually create a \u003ccode\u003eforms.CharField\u003c/code\u003e\u003c/p\u003e","title":"How to disable auto strip in Charfield in Django"},{"content":"In Django 3.1, Django support save python data into database as JSON encoded data and it is also possible to make query based on field value in JSONField. The detailed usage can be found here. If you are using older version and want to try this feature. Though there are many packages ported this function, I recommend django-jsonfield-backport.\ndjango-jsonfield-backport #This package save data as JSON in database and also support JSON query. If your database meet the requirements (MySQL \u0026gt; 5.7, PG \u0026gt; 9.5, MariaDB \u0026gt; 10.2 or SQLite \u0026gt; 3.9 with JSON1 extension), you can use JSONField like Django\u0026rsquo;s native implementation.\nfrom django.db import models from django_jsonfield_backport.models import JSONField class ContactInfo(models.Model): data = JSONField() ContactInfo.objects.create(data={ \u0026#39;name\u0026#39;: \u0026#39;John\u0026#39;, \u0026#39;cities\u0026#39;: [\u0026#39;London\u0026#39;, \u0026#39;Cambridge\u0026#39;], \u0026#39;pets\u0026#39;: {\u0026#39;dogs\u0026#39;: [\u0026#39;Rufus\u0026#39;, \u0026#39;Meg\u0026#39;]}, }) ContactInfo.objects.filter( data__name=\u0026#39;John\u0026#39;, data__pets__has_key=\u0026#39;dogs\u0026#39;, data__cities__contains=\u0026#39;London\u0026#39;, ).delete() jsonfield #jsonfield is another popular package to use JSONField. It will save data as Text in database, but you can manipulate field value as python data. In addition, it does not provide JSON querying capability as django-jsonfield-backport.\nDjango REST framework #As data is stored as JSON string in database, the output is string rather than object when Django DRF serialize jsonfield.JSONField. If you prefer to get and update the data like object, you need to manually specify it as `serializer.JSONField` like this:\nfrom rest_framework import serializers from .models import Product class ProductSerializer(serializers.ModelSerializer): images = serializers.JSONField() class Meta: model = Product fields = \u0026#39;__all__\u0026#39; (You do not need to do this when using django-jsonfield-backport, everything just works.)\nRef # GitHub - django-jsonfield-backport Use JSONField with Django Rest Framework JSONField in serializers – Django REST Framework Django REST framework - jsonfield ","date":"11 September 2021","permalink":"https://fromkk.com/posts/using-jsonfield-before-django-3-dot-1/","section":"Posts","summary":"\u003cp\u003eIn Django 3.1, Django support save python data into database as JSON encoded data and it is also possible to make query based on field value in JSONField. The detailed usage can be found \u003ca href=\"https://docs.djangoproject.com/en/3.2/topics/db/queries/#querying-jsonfield\" target=\"_blank\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e. If you are using older version and want to try this feature. Though there are many packages ported this function, I recommend \u003ca href=\"https://github.com/laymonage/django-jsonfield-backport\" target=\"_blank\" rel=\"noreferrer\"\u003edjango-jsonfield-backport\u003c/a\u003e.\u003c/p\u003e","title":"Using JSONField before Django 3.1"},{"content":"I wrote a Spark program to process logs. The number of logs always changes as time goes by. To ensure logs can be processed instantly, the number of executors is calculated by the maximum of logs per minutes. As a consequence, the CPU usage is low in executors. In order to decrease resource waste, I tried to find a way to schedule executors during the execution of program.\nAs shown below, the maximum number of logs per minutes can be a dozen times greater than the minimum number in one day.\nIf I can modify the executor number by size of data to proceed, the resource usage should increase.\nDynamic Allocation #Spark provide a similar configuration to control the number of executors. By enable spark.dynamicAllocation.enabled, spark will change number of running executors by task number automatically.\nHow does Dynamic Allocation Work? #Request Executors #As is known to all, the action operators(such as count, collect) create Spark job. Each job is divided into stages by shuffle operation, and each data partition in the stage will become independent jobs. When dynamic allocation is enabled, if there have been pending tasks for spark.dynamicAllocation.schedulerBacklogTimeout seconds, driver will request for more executors. If the pending task still exists, the executor request will be triggered every spark.dynamicAllocation.sustainedSchedulerBacklogTimeou seconds. Furthermore, the number of executors requested in each round increases exponentially from the previous round. For instance, an application will add 1 executor in the first round, and then 2, 4, 8 and so on executors in the subsequent rounds. The number of total running executor should not exceed spark.dynamicAllocation.maxExecutors.\nWhen receiving the first executor request, driver ask cluster manager to create executor. After the new executor is created, driver checks if there are more request waiting to created and handle all of the pending request.\nThe reason to use this strategy to create executor is to avoid creating too many executor when payload just peak for a short time and make sure there are enough executor to be created in a period of time if the payload keeps high.\nRelease Executors #After the executor is idle for spark.dynamicAllocation.executorIdleTimeout seconds, it will be released. The one which contains cache data will not be removed. To prevent the executor which keeps the shuffle data from being removed, a additional spark service is needed before spark 3.0. From 3.0, the external shuffle service is not required if spark.dynamicAllocation.shuffleTracking.enabled is used.\nDynamic allocation is easy to used, but there are two disadvantage:\nSlow scheduling. Creating executors is serial. If two or more executor is requested, driver will ask cluster manager to create executors for at least two times. This is an issue if pods creation takes time. In general, that is fine as the K8s 1.6 SLO is that 99% of pods should be created in 5s in a 5000 node cluster. Hard to release executor if each task is short. The release is based on the idle time. If there are so many short task, the executor is not like to idle as tasks are assigned uniformly. In our spark program, the task is short and data must be processed in 1 minutes. So dynamic allocation not suitable.\nManual Allocation #Luckily, spark also provide a way to control the number of executors manually. We can use sc.requestExecutors and sc.killExecutors to create and delete executors.\nIn order to use these two function, we have to know the number of running executors and their IDs.\nNumber of Running Executors\nThe Spark program\u0026rsquo;s RAM usage can be obtained from sc.getExecutorMemoryStatus. It returns a dict list like this: [Map(10.73.3.67:59136 -\u0026gt; (2101975449,2101612350))]. The key is IP with port and value is a tuple contains the max RAM and available RAM. Please note that driver is also included in the return data.\nIDs of Running Executors\nIDs is required when calling sc.killExecutors. This can be found in Spark REST API. The executors information such as ID, cores and tasks is record in /applications/[app-id]/executors.\nWith the help of sc.requestExecutors, we can create as many executors as we want in one request. But the pod create time is still too long. To eliminate the pod create request, I used these strategies:\nThe running executors is expected to finish job in 50s, fot the purpose of reversing some time for delayed tasks. When the expected executor is close to current running executors, no executor is requests or released. If there is backlog data, request more executors. Result #After using manually allocation, the CPU usage grows a lot and reaches 40%. The cores used by Spark programs drop from 1700 to 800. Furthermore, the Spark program can scale automatically.\n","date":"18 July 2021","permalink":"https://fromkk.com/posts/dynamic-allocate-executors-when-executing-jobs-in-spark/","section":"Posts","summary":"\u003cp\u003eI wrote a Spark program to process logs. The number of logs always changes as time goes by. To ensure logs can be processed instantly, the number of executors is calculated by the maximum of logs per minutes. As a consequence, the CPU usage is low in executors. In order to decrease resource waste, I tried to find a way to schedule executors during the execution of program.\u003c/p\u003e","title":"Dynamic Allocate Executors when Executing Jobs in Spark"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/spark/","section":"Tags","summary":"","title":"Spark"},{"content":"Kafka is a high-performance and scalable messaging system. Sometimes when handling big data. The default configuration may limit the maximum performance. In this article, I\u0026rsquo;ll explain how messages are generate and saved in Kafka, and how to improve performance by changing configuration.\nKafka Internals #How does Producer Send Messages? #In short, messages will assembled into batches (named RecordBatch) and send to broker.\nThe producer manages some internal queues, and each queue contains RecordBatch that will send to one broker. When calling send method, the producer will look into the internal queue and try to append this message to RecordBatch which is smaller than batch.size (default value is 16KB) or create new RecordBatch.\nThere is also a sender thread in producer which is responsible for turning RecordBatch into requests (\u0026lt;broker node，List(ProducerBatch)\u0026gt;) and send to broker.\nhow are Records Saved? #The details can be found from these two articles: Apache Kafka - Message Format and A Guide To The Kafka Protocol - Apache Kafka - Apache Software Foundation.\nHere are some important properties in RecordBatch are: batch_lenth, compresstion_type, CRC, timestamp and, of course, the List(Record).\nEach Record consists of length, timestamp_delta, key(byte), value(byte) etc.\nWhen look into the kafka topic data directory, you may find files like this:\n00000000000000000000.log 00000000000000000000.index 00000000000000000000.timeindex 00000000000000000035.log 00000000000000000035.index 00000000000000000035.timeindex Kafka saves each partition as segments. When new record comes, it append to the active segment. If the segment\u0026rsquo;s size limit is reached, a new segment is created as becomes the active segment. Segments are named by the offset of its first record, so the segments\u0026rsquo; names are incremental.\nFurthermore, the segment divided into three kinds of file: log file, index file and timeindex file.\nThe log file contains the actual data The index file contains the record\u0026rsquo;s relative offset and its physical position in the log file. This makes the look up complexity for specific offset record to O(1). The timeindex file contains the record\u0026rsquo;s relative offset and its timestamp. How does Consumer pull messages? #Consumer keeps reading data from broker, and decompress data if necessary. It will put data into a internal queue and return the target number of records to client.\nmax.poll.records (default values is 500) means the maximum number of records returned in a single call to poll().\nfetch.min.bytes (default value is 1) means the minimum amount of data the broker should return from a fetch request. If insufficient data is available, the server will wait up to fetch.max.wait.ms ms and accumulate the data before answering the request.\nHow to Improve Performance #Increase Socket Buffer #The default socket buffer value in Java client is too small for high-throughput environment. socket.receive.buffer.bytes (default value is 64KB) and send.buffer.bytes (default value is 128KB) is the SO_RCVBUFF and SO_SNDBUFF for socket connections respectively. I recommend to set it to a bigger value or -1 to use the OS default value.\nbatch.size, linger.ms and buffer.memory #As mentioned before, producer always send message as RecordBatch. Each batch should be smaller than batch.size (default value is 16KB). Increasing batch.size will not only reduce the TCP request to broker, but also lead to better compression ratio when compression is enabled.\nlinger.ms is used to specific the wait time before sending RecordBatch, and it will effect the real size of RecordBatch indirectly. The producer groups together any records that arrive in between request transmissions into a single batched request. If the system load is low and the RecordBatch is not full, the producer sender will still send this batch once it has been waited for linger.ms. linger.ms\u0026rsquo;s default value is 0, which means producer will send message as quick as possible(but the messaged arrived between two send requests will also be batched to RecordBatch). Increasing this value not only makes real batch size be close to batch.size and reducing the number of requests to be sent, but also increases the delay of messages.\nThe buffer.memory (default value is 32MB) controls the total amount of memory available to the producer for buffering. If records are sent faster than they can be transmitted to the server then this buffer space will be exhausted. When the buffer space is exhausted additional send calls will block.\nCompression.type #As the throughput keep growing, bandwidth may become bottleneck. It\u0026rsquo;s easy to tackle this by add compresstion.type param in producer. Once it is configured, the producer will compressed the RecordBatch before sending it to broker. If the records are texts, the compression ratio should be high and bandwidth usage will be significantly decreased.\nThere are two kind of compresstion.type, topic level and producer level.\nIf you set compresstion.type in producer, the producer will compress the records and send it to broker.\nThere is also a topic level compresstion.type configuration. When it is set, producer\u0026rsquo;s compression type is not constrained. The broker will convert data sent from producer to target compresstion.type. compresstion.type can be set as gzip, snappy, lz4, zstd, uncompressed, and producer. The default value is producer, which means the broker will keep the original data send from the producer.\nHow to choose compression type? According to cloudflare\u0026rsquo;s test result in Squeezing the firehose: getting the most from Kafka compression:\ntype CPU ration Compression ratio None 1x 1x Gzip 10.14x 3.58x Snappy 1.61x 2.35x LZ4 2.51x 1.81x Gzip has best compression ratio but take lots of CPU time. Snappy keeps a balance between the CPU time and space. The new compression type zstd added in Kafka 2.1 produce larger compression ratio than Snappy with the cost of a little more CPU time.\nThese are common configurations, you can find more from the official document contains such as max.in.flight.requests.per.connection.\nRef # Kafka message format Kafka高性能探秘 Exploit Apache Kafka’s Message Format to Save Storage and Bandwidth Consuming big messages from Kafka How does max.poll.records affect the consumer poll A Practical Introduction to Kafka Storage Internals Kafka message codec - compress and decompress 20 Best Practices for Working With Apache Kafka at Scale Kakfa Document kafka-python KafkaProducer Deep Dive Into Apache Kafka | Storage Internals ","date":"28 May 2021","permalink":"https://fromkk.com/posts/improve-kafka-throughput/","section":"Posts","summary":"\u003cp\u003eKafka is a high-performance and scalable messaging system. Sometimes when handling big data. The default configuration may limit the maximum performance. In this article, I\u0026rsquo;ll explain how messages are generate and saved in Kafka, and how to improve performance by changing configuration.\u003c/p\u003e","title":"Improve Kafka throughput"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/kafka/","section":"Tags","summary":"","title":"Kafka"},{"content":"After update brew to latest version, when calling cask related command, it always outputs Error: Cask 'java' is unavailable: No Cask with this name exists., such as brew list --cask. However, the brew command works.\nAfter doing some research, I found Java has been moved to homebrew/core. This makes sense now. I installed java by cask, but it\u0026rsquo;s not available now and cask throw this error. If I uninstall java from cask, the error should disappear.\nThis is not easy as cask is broken. Finally, I found this issue: brew cask upgrade fails with \u0026ldquo;No Cask with this name exists\u0026rdquo;. After running rm -rf \u0026quot;$(brew --prefix)/Caskroom/java, cask is back.\n","date":"7 March 2021","permalink":"https://fromkk.com/posts/fix-error-cask-java-is-unavailable-in-homebrew/","section":"Posts","summary":"\u003cp\u003eAfter update brew to latest version, when calling \u003ccode\u003ecask\u003c/code\u003e related command, it always outputs \u003ccode\u003eError: Cask 'java' is unavailable: No Cask with this name exists.\u003c/code\u003e, such as \u003ccode\u003ebrew list --cask\u003c/code\u003e. However, the \u003ccode\u003ebrew\u003c/code\u003e command works.\u003c/p\u003e\n\u003cp\u003eAfter doing some research, I found \u003ca href=\"https://github.com/Homebrew/homebrew-cask/pull/72284\" target=\"_blank\" rel=\"noreferrer\"\u003eJava has been moved to homebrew/core\u003c/a\u003e. This makes sense now. I installed java by cask, but it\u0026rsquo;s not available now and cask throw this error. If I uninstall java from cask, the error should disappear.\u003c/p\u003e","title":"Fix Error: Cask 'java' is unavailable in Homebrew"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/homebrew/","section":"Tags","summary":"","title":"Homebrew"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/java/","section":"Tags","summary":"","title":"Java"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/scala/","section":"Tags","summary":"","title":"Scala"},{"content":"I wrote a Scala code to get the current time. However, the output is different on the development server and docker.\nimport java.util.Calendar println(Calendar.getInstance().getTime) On my development server, it outputs Sun Oct 18 18:01:01 CST 2020, but in docker, it print a UTC time.\nI guess it related to the timezone setting and do a research, here is the result.\nHow Did JVM Detect Timezone #All of the code can be found in this function: private static synchronized TimeZone setDefaultZone()\nString zoneID = AccessController.doPrivileged(new GetPropertyAction(\u0026#34;user.timezone\u0026#34;)); // if the time zone ID is not set (yet), perform the // platform to Java time zone ID mapping. if (zoneID == null || zoneID.isEmpty()) { String javaHome = AccessController.doPrivileged( new GetPropertyAction(\u0026#34;java.home\u0026#34;)); try { zoneID = getSystemTimeZoneID(javaHome); if (zoneID == null) { zoneID = GMT_ID; } } catch (NullPointerException e) { zoneID = GMT_ID; } } First, it will check whether JVM has user.timezone property. If not, it will call this native method getSystemTimeZoneID, it was implemented in java.base/share/native/libjava/TimeZone.c, and the main logic is in java.base/unix/native/libjava/TimeZone_md.c.\nIn Timezone_md.c, it will find timezone by following steps, it will return the timezone immediately once found.\nFind TZ environment. Read /etc/timezone. Read /etc/localtime. If it is a soft link(ex: /usr/share/zoneinfo/Asia/Shanghai), return timezone by path. Otherwise, compare the content with all files in /usr/share/zoneinfo, if found, return timezone. Return GMT as timezone. How to Change Timezone #The available timezone in Linux can be listed by this command: timedatectl list-timezones\nAdd JVM param #You can add -Duser.timezone=Asia/Shanghai as JVM parameters.\nSet TZ environment variable #Add export TZ=Asia/Shanghai in .bashrc.\nChange /etc/timezone #Set its content to Asia/Shanghai\nChange /etc/localtime #Link it to /usr/share/zoneinfo/Asia/Shanghai\nChange timezone manually in Java Program #All of these methods should work\nAdd this line before get time: TimeZone.setDefault(TimeZone.getTimeZone(\u0026quot;Asia/Shanghai\u0026quot;)) Set JVM property by code System.setProperty(\u0026quot;user.timezone\u0026quot;, \u0026quot;Asia/Shanghai\u0026quot;) Set timezone manually in Calendar Calendar.getInstance(TimeZone.getTimeZone(\u0026quot;Asia/Shanghai\u0026quot;)) Ref # How to Set the JVM Time Zone jvm linux 时区设置 Java default timezone detection, revisited Java读取系统默认时区 How to set a JVM TimeZone Properly ","date":"18 October 2020","permalink":"https://fromkk.com/posts/timezone-in-jvm/","section":"Posts","summary":"\u003cp\u003eI wrote a Scala code to get the current time. However, the output is different on the development server and docker.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-scala\" data-lang=\"scala\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ejava.util.Calendar\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eprintln\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eCalendar\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egetInstance\u003c/span\u003e\u003cspan class=\"o\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003egetTime\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eOn my development server, it outputs \u003ccode\u003eSun Oct 18 18:01:01 CST 2020\u003c/code\u003e, but in docker, it print a UTC time.\u003c/p\u003e","title":"Timezone in JVM"},{"content":"Have you ever tried to install MySQL-python? It contains the C code and need to compile the code while install the package. You have to follow the steps in this articles: Install MySQL and MySQLClient(Python) in MacOS. Things get worse if you are using Windows.\nLuckily, as new distribution format Wheel has been published in PEP 427.\nThe wheel binary package format frees installers from having to know about the build system, saves time by amortizing compile time over many installations, and removes the need to install a build system in the target environment.\nInstallation of wheels does not require a compiler on system and is much faster.\nCibuildwheel is a very useful tool for building wheels. It can run on many CI server (GitHub Actions, Travis , Azure Pipelines etc) and build wheels across many platforms.\nUsage #You need to create a configuration file for the CI server, you can read the examples and documents.\nFor example, GitHub Actions can use this configuration file:\nname: Build on: [push, pull_request] jobs: build_wheels: name: Build wheels on ${{ matrix.os }} runs-on: ${{ matrix.os }} strategy: matrix: os: [ubuntu-18.04, windows-latest, macos-latest] steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 name: Install Python with: python-version: \u0026#39;3.7\u0026#39; - name: Install cibuildwheel run: | python -m pip install cibuildwheel==1.5.5 - name: Install Visual C++ for Python 2.7 if: runner.os == \u0026#39;Windows\u0026#39; run: | choco install vcpython27 -f -y - name: Build wheels run: | python -m cibuildwheel --output-dir wheelhouse - uses: actions/upload-artifact@v2 with: path: ./wheelhouse/*.whl Useful Options #These options can be applied by setting environment variables.\nCIBW_BUILD / CIBW_SKIP #Use this options to filter the Python versions to build.\nExample:\n# Only build on Python 3.6 CIBW_BUILD: cp36-* # Skip building on Python 2.7 on the Mac CIBW_SKIP: cp27-macosx_x86_64 # Skip building on Python 3.8 on the Mac CIBW_SKIP: cp38-macosx_x86_64 CIBW_BEFORE_BUILD #Execute the shell command before wheel building.\nUpload to PyPI #Now you can download wheelhouse.zip from Actions panel on GitHub, and unzip it to dist folder. Then manually publish it by rm -rf dist \u0026amp;\u0026amp; python setup.py sdist \u0026amp;\u0026amp; twine upload dist/*. You can get more detailed guide from this article: Packaging Python Projects.\nThis process can also be done automatically by using CI configuration file. You can find the example configuration files from official repo.\nRef # Building Python Platform Wheels for Packages with Binary Extensions How to include external library with python wheel package cibuildwheel ","date":"29 July 2020","permalink":"https://fromkk.com/posts/using-cibuildwheel-to-create-python-wheels/","section":"Posts","summary":"\u003cp\u003eHave you ever tried to install \u003ccode\u003eMySQL-python\u003c/code\u003e? It contains the C code and need to compile the code while install the package. You have to follow the steps in this articles: \u003ca href=\"https://ruddra.com/install-mysqlclient-macos/\" target=\"_blank\" rel=\"noreferrer\"\u003eInstall MySQL and MySQLClient(Python) in MacOS\u003c/a\u003e. Things get worse if you are using Windows.\u003c/p\u003e","title":"Using cibuildwheel to Create Python Wheels"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/elasticsearch/","section":"Tags","summary":"","title":"Elasticsearch"},{"content":"It\u0026rsquo;s easy to get small dataset from Elasticsearch by using size and from. However, it\u0026rsquo;s impossible to retrieve large dataset in the same way.\nDeep Paging Problem #As we know it, Elasticsearch data is organised into indexes, which is a logical namespace, and the real data is stored into physical shards. Each shard is an instance of Lucene. There are two kind of shards, primary shards and replica shards. Replica shards is the copy of primary shards in case nodes or shards fail. By distributing documents in an index across multiple shards, and distributing those shards across multiple nodes, Elasticsearch can ensure redundancy and scalability. By default, Elasticsearch create 5 primary shards and one replica shard for each primary shards.\nHow to decide which shard should the document be distributed? By default, shard = hashCode(doc._id) % primary_shards_number. To make this stable, the number of primary shards cannot be change the index has been created.\nUsually, the shards size should be 20GB to 40GB. The number of shards a node can hold is depending on the heap space. In general, 1GB heap space can hold 20 shards.\nAs data is store in different shards. If there are 5 shards, when doing this query:\nGET /_search?size=10 Each shards will generate 10 search result, and send results to coordinate node. The coordinate node will sort 50 items, and result the first 10 result to user. However when query become this:\nGET /_search?size=10\u0026amp;from=10000 Although we only need 10 items, each shards has to return the first 10010 result to coordinate node, and coordinate node has to sort 50050 items, this search cost lots of resource.\nAs deep paging is costly, Elasticsearch has restrict from+size less than index.max-result-window, the default value is 10000.\nScroll #The search method has to retrieve and sort the result over and over again, because it does not know how to continue the search from previous position.\nscroll is more efficient when retrieve large set of data.\nFor example:\nPOST /twitter/_search?scroll=1m { \u0026#34;size\u0026#34;: 100, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;title\u0026#34; : \u0026#34;elasticsearch\u0026#34; } } } and the returned result will contains a _scroll_id, which should be passed to the scroll API in order to retrieve the rest of data.\nPOST /_search/scroll { \u0026#34;scroll\u0026#34; : \u0026#34;1m\u0026#34;, \u0026#34;scroll_id\u0026#34; : \u0026#34;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==\u0026#34; } Scroll return the matched result at the time of the initial search request, like a snapshot, and ignore the subsequent changes to the documents(index, update or delete). The scroll=1m is used to tell how long should Elasticsearch keep the context. If there no following requests using the returned scroll_id, the scroll context will expire.\nPS: In fact, when dealing the initial search request, scoll will cache all the matched documents\u0026rsquo; id, then get the size document content in batches for each following requests.\nSlice #It\u0026rsquo;s also possible to split the scroll in multiple slices and consume them independently.\nGET /twitter/_search?scroll=1m { \u0026#34;slice\u0026#34;: { \u0026#34;id\u0026#34;: 0, \u0026#34;max\u0026#34;: 2 }, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;title\u0026#34; : \u0026#34;elasticsearch\u0026#34; } } } GET /twitter/_search?scroll=1m { \u0026#34;slice\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;max\u0026#34;: 2 }, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;title\u0026#34; : \u0026#34;elasticsearch\u0026#34; } } } The above request contains split the slice into 2 parts by using max:2 parameter. These union of two requests\u0026rsquo; data is equivalent to the result of a scroll query without slicing.\nThe slice of the document can be calculated by this formula: slice(doc) = hash(doc._id) % max_slice. This is quiet similar to the calculation of shards mentioned before. For example if slice is 4, and shards is 2. Then slices 0,2 are assigned to first shard and slices 1,3 are assigned to second shard.\nWhen slices number is n, each matched documents use a n bitset to remember which slice it belongs to. So you should limit the number of sliced query you perform in parallel to avoid the memory explosion.\nGetting hash(doc._id) is expensive. You can also use another numeric doc_value field to do the slicing without hash function. For instance:\nGET /twitter/_search?scroll=1m { \u0026#34;slice\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;id\u0026#34;: 0, \u0026#34;max\u0026#34;: 10 }, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;title\u0026#34; : \u0026#34;elasticsearch\u0026#34; } } } Query performance is most efficient when the number of slices is equal to the number of shards in the index. If that number is large (e.g. 500), choose a lower number as too many slices will hurt performance. Setting slices higher than the number of shards generally does not improve efficiency and adds overhead.\nfrom Picking the number of slices\nSearch After #Scroll is not suitable for real-time user requests. After Elasticsearch 5, Search After API is added. It\u0026rsquo;s similar to scroll but provides a live cursor. It uses the results from the previous page to retrieve the next page.\nTo use search after, the query must be sorted, and the following query also contains search_after=previous sort value.\nFor example, if the initial query is this:\nGET twitter/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;title\u0026#34; : \u0026#34;elasticsearch\u0026#34; } }, \u0026#34;sort\u0026#34;: [ {\u0026#34;date\u0026#34;: \u0026#34;asc\u0026#34;}, {\u0026#34;tie_breaker_id\u0026#34;: \u0026#34;asc\u0026#34;} ] } Then you have to extract the sort value of the last document, and pass it to search_after to get the next page result.\nGET twitter/_search { \u0026#34;size\u0026#34;: 10, \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34; : { \u0026#34;title\u0026#34; : \u0026#34;elasticsearch\u0026#34; } }, \u0026#34;search_after\u0026#34;: [1463538857, \u0026#34;654323\u0026#34;], \u0026#34;sort\u0026#34;: [ {\u0026#34;date\u0026#34;: \u0026#34;asc\u0026#34;}, {\u0026#34;tie_breaker_id\u0026#34;: \u0026#34;asc\u0026#34;} ] } Ref # Elasticsearch: The Definitive Guide: Pagination Scalability and resilience: clusters, nodes, and shards ElasticSearch如何支持深度分页 Documentation for scroll API is a bit confusing! Request Body Search: Scroll Optimizing Elasticsearch: How Many Shards per Index? ","date":"21 June 2020","permalink":"https://fromkk.com/posts/retrieve-large-dataset-in-elasticsearch/","section":"Posts","summary":"\u003cp\u003eIt\u0026rsquo;s easy to get small dataset from Elasticsearch by using \u003ccode\u003esize\u003c/code\u003e and \u003ccode\u003efrom\u003c/code\u003e. However, it\u0026rsquo;s impossible to retrieve large dataset in the same way.\u003c/p\u003e\n\u003ch2 id=\"deep-paging-problem\" class=\"relative group\"\u003eDeep Paging Problem \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#deep-paging-problem\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eAs we know it, Elasticsearch data is organised into indexes, which is a logical namespace, and the real data is stored into physical shards. Each shard is an instance of Lucene. There are two kind of shards, primary shards and replica shards. Replica shards is the copy of primary shards in case nodes or shards fail. By distributing documents in an index across multiple shards, and distributing those shards across multiple nodes, Elasticsearch can ensure redundancy and scalability. By default, Elasticsearch create \u003cstrong\u003e5\u003c/strong\u003e primary shards and one replica shard for each primary shards.\u003c/p\u003e","title":"Retrieve Large Dataset in Elasticsearch"},{"content":"It\u0026rsquo;s inevitable to dealing with bugs in coding career. The main part of coding are implementing new features, fixing bugs and improving performance. For me, there are two kinds of bugs that is difficult to tackle: those are hard to reproduce, and those occur in code not wrote by you.\nRecently, I met a bug which has both features mentioned before. I write a Spark program to analyse the log and cluster them. Last week I update the code, use Facebook\u0026rsquo;s faiss library to accelerate the process of find similar vector. After I push the new code to spark, the program crashed. I found this log on Spark driver:\njava.io.EOFException ERROR PythonRunner: Python worker exited unexpectedly (crashed). Because the Python Worker is created by Spark JVM, I can\u0026rsquo;t get the internal state of Python Worker. By inserting log into Code, I get the rough position of crash code. But the code looks good.\nI have tested the code on my develop environment. My develop machine is Using Spark 2.4. but the Spark platform is using Spark 3.0. I guess maybe there is some compatible problem on Spark 3.0. So I use the same docker images as Spark platform to run the code. The code works as expected without crash. That\u0026rsquo;s wired, the docker has isolate the environment, how could same docker image produce different output?\nI search the error from google, some said it\u0026rsquo;s because spark is running out of memory. This doesn\u0026rsquo;t seem correct, this update shouldn\u0026rsquo;t increase the RAM usage. I still gave it a try and no luck.\nAlright, this update add faiss to the code, maybe faiss lead to the crash, as Python doesn\u0026rsquo;t raise any other. If the crash is caused by the C code in faiss, this makes sense. First, I write a code with spark and faiss, the program crashed. Then I wrote a code only contains faiss, it still crashed. So I can confirm that the crash is cause by faiss and Spark is innocent. Even stranger, when running on Spark platform, sometimes the script crashes, sometimes not.\nBut why faiss only crash on the Spark Platform? I ask the colleague to know the detail of the failed job and know that the docker\u0026rsquo;s exit code is 132. 132 means illegal instruction. I search illegal instruction on faiss\u0026rsquo;s GitHub issue. I found this issue: Illegal instruction (core dumped).\nBy compare the host server\u0026rsquo;s CPU instruction. The crashed ones lack of avx2 instruction. avx2 is added after the Intel Fourth generation core (Haswell). The develop server is using sixth generation CPU, and some platform server is too to support this instruction. By adding a parameter to enforce the script scheduling on new server, the crash disappears.\nPS: Running faiss code index.add(xx) will not trigger the crash, but calling faiss.search(xx) does. When I trying to locate the code which cause the crash, the faiss package was imported correctly and the index is built normally. This mislead me to believe that faiss code is working.\n","date":"17 May 2020","permalink":"https://fromkk.com/posts/program-crash-caused-by-cpu-instruction/","section":"Posts","summary":"\u003cp\u003eIt\u0026rsquo;s inevitable to dealing with bugs in coding career. The main part of coding are implementing new features, fixing bugs and improving performance. For me, there are two kinds of bugs that is difficult to tackle: those are hard to reproduce, and those occur in code not wrote by you.\u003c/p\u003e","title":"Program Crash Caused by CPU Instruction"},{"content":"I use Emacs to write blog. In the recent update, I found M-RET no longer behave as leader key in org mode, but behave as org-meta-return. And even more strange is that in other mode, it behave as leader key. And M-RET also works in terminal in org mode. In GUI, pressing C-M-m can trigger leader key.\nSO I opened this issue, with the help of these friends, the issue has been fixed. Here is the cause of the bug.\nIn Emacs, RET is not a key in keyboard, it\u0026rsquo;s a logical key). Emacs bind RET to C-m in source code. In terminal, \u0026lt;Enter\u0026gt; and C-m both send \u0026lt;CR\u0026gt; (ASCII 13) character, so \u0026lt;Enter\u0026gt; / \u0026lt;Return\u0026gt; key is equal to RET. In GUI, pressing \u0026lt;Enter\u0026gt; / \u0026lt;Return\u0026gt; key actually sends \u0026lt;return\u0026gt; to Emacs, and Emacs automatically translate \u0026lt;return\u0026gt; to RET.\nThis can be proved: type SPC h d k \u0026lt;Enter\u0026gt; in spacemacs, it will output RET (translated from \u0026lt;return\u0026gt;) runs the command org-open-at-point, which is an interactive compiled Lisp function in ‘org.el’.\nPressing C-m or \u0026lt;Enter\u0026gt; key usually given the same result, but you can also bind these with two different command. Take M-RET as example. If only \u0026lt;M-return\u0026gt; is bind, the M-RET is unbinded. If only M-RET is binded, then M-return is implicitly also bind to same command as M-RET.\nIn org mode scr:\n(org-defkey org-mode-map (kbd \u0026#34;M-\u0026lt;return\u0026gt;\u0026#34;) #\u0026#39;org-meta-return) (org-defkey org-mode-map (kbd \u0026#34;M-RET\u0026#34;) #\u0026#39;org-meta-return) These two keys were binded to org-meta-return.\nThe unfixed Spacemacs configuration file binds C-M-m as dotspacemacs-major-mode-emacs-leader-key.\nIn GUI, the \u0026lt;Enter\u0026gt; key will send \u0026lt;return\u0026gt; to Emacs. Org mode has explicitly bind M-\u0026lt;return\u0026gt; to org-meta-return, so org-meta-return is triggered. In other mode, the M-\u0026lt;return\u0026gt; key binding is not defined, so \u0026lt;return\u0026gt; will translate to RET, then trigger leader key.\nIn the fixed version, dotspacemacs-major-mode-emacs-leader-key bind to M-\u0026lt;return\u0026gt; in GUI, and this override org mode\u0026rsquo;s binding. Finally meta return becomes leader key again.\nRef # M-RET no longer org mode prefix in GUI Difference between the physical “RET” key and the command \u0026rsquo;newline in the minibuffer Emacs中的 return, RET, Enter, Ctrl-m解析 How to turn off alternative Enter with Ctrl+M in Linux ","date":"11 April 2020","permalink":"https://fromkk.com/posts/c-m-ret-and-return-key-in-emacs/","section":"Posts","summary":"\u003cp\u003eI use Emacs to write blog. In the recent update, I found \u003ccode\u003eM-RET\u003c/code\u003e no longer behave as leader key in org mode, but behave as \u003ccode\u003eorg-meta-return\u003c/code\u003e. And even more strange is that in other mode, it behave as leader key. And \u003ccode\u003eM-RET\u003c/code\u003e also works in terminal in org mode. In GUI, pressing \u003ccode\u003eC-M-m\u003c/code\u003e can trigger leader key.\u003c/p\u003e","title":"C-m, RET and Return Key in Emacs"},{"content":"First zip all of the dependencies into zip file like this. Then you can use one of the following methods to import it.\n|-- kk.zip | |-- kk.py Using \u0026ndash;py-files in spark-submit #When submit spark job, add --py-files=kk.zip parameter. kk.zip will be distributed with the main scrip file, and kk.zip will be inserted at the beginning of PATH environment variable.\nThen you can use import kk in your main script file.\nThis utilize Python\u0026rsquo;s zip import feature. For more information, check this link: zipimport\nUsing addPyFile in main script #You can also upload zip file to hdfs, and using sc.addPyFile('hdfs://kk.zip') after SparkContext is initialized.\nThis has the same effect as --py-files, but your import statement must be after this line.\n","date":"2 April 2020","permalink":"https://fromkk.com/posts/import-custom-package-or-module-in-pyspark/","section":"Posts","summary":"\u003cp\u003eFirst zip all of the dependencies into zip file like this. Then you can use one of the following methods to import it.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-nil\" data-lang=\"nil\"\u003e|-- kk.zip\n|   |-- kk.py\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"using-py-files-in-spark-submit\" class=\"relative group\"\u003eUsing \u0026ndash;py-files in spark-submit \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#using-py-files-in-spark-submit\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eWhen submit spark job, add \u003ccode\u003e--py-files=kk.zip\u003c/code\u003e parameter. \u003ccode\u003ekk.zip\u003c/code\u003e will be distributed with the main scrip file, and \u003ccode\u003ekk.zip\u003c/code\u003e will be inserted at the beginning of \u003ccode\u003ePATH\u003c/code\u003e environment variable.\u003c/p\u003e","title":"Import custom package or module in PySpark"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/influxdb/","section":"Tags","summary":"","title":"InfluxDB"},{"content":"These days I use InfluxDB to save some time series data. I love these features it provides:\nHigh Performance #According to to it\u0026rsquo;s hardware guide, a single node will support more than 750k point write per second, 100 moderate queries per second and 10M series cardinality.\nContinuous Queries #Simple aggregation can be done by InfluxDB\u0026rsquo;s continuous queries.\nOverwrite Duplicated Points #If you submit a new point with same measurements, tag set and timestamp, the new data will overwrite the old one.\nPreset Time Boundary #InfluxDB is well documented, but the group by time section is not very clear. It says it will group data by =preset time boundary. But the example it use is too simple and doesn\u0026rsquo;t explain it very well.\nIn the official example, when using group by time(12m)=, the time boundary is 00:12, 00:24. When using group by time(30m), the time boundary becomes 00:00, 00:30. It seems that the time boundary start from the nearest hour plus x times time interval, that\u0026rsquo;s not correct. If you using group by time(7m), the returned time boundary is not 00:07, 00:14\nHere a example:\nIf the data is:\n{\u0026#39;time\u0026#39;: \u0026#39;2020-01-01T00:02:00Z\u0026#39;, \u0026#39;value\u0026#39;: 10} {\u0026#39;time\u0026#39;: \u0026#39;2020-01-01T00:04:00Z\u0026#39;, \u0026#39;value\u0026#39;: 8} {\u0026#39;time\u0026#39;: \u0026#39;2020-01-01T00:05:00Z\u0026#39;, \u0026#39;value\u0026#39;: 21} {\u0026#39;time\u0026#39;: \u0026#39;2020-01-01T00:07:00Z\u0026#39;, \u0026#39;value\u0026#39;: 33} {\u0026#39;time\u0026#39;: \u0026#39;2020-01-02T00:05:00Z\u0026#39;, \u0026#39;value\u0026#39;: 9} {\u0026#39;time\u0026#39;: \u0026#39;2020-01-03T10:05:00Z\u0026#39;, \u0026#39;value\u0026#39;: 4} Execute select sum(value) from data where time\u0026gt;='2020-01-01 00:00:00' and time\u0026lt;'2020-01-04 00:00:00' group by time(7m) fill(none) will output:\n{\u0026#39;time\u0026#39;: \u0026#39;2019-12-31T23:58:00Z\u0026#39;, \u0026#39;sum\u0026#39;: 18} {\u0026#39;time\u0026#39;: \u0026#39;2020-01-01T00:05:00Z\u0026#39;, \u0026#39;sum\u0026#39;: 54} {\u0026#39;time\u0026#39;: \u0026#39;2020-01-02T00:00:00Z\u0026#39;, \u0026#39;sum\u0026#39;: 9} {\u0026#39;time\u0026#39;: \u0026#39;2020-01-03T10:04:00Z\u0026#39;, \u0026#39;sum\u0026#39;: 4} Note that the time boundary begins at 12-31 23:58, not 01-01 00:00. What cause this?\nInfluxDB using timestamp 0 (1970-01-01T00:00:00Z) as start time, and for each timestamp that is dividable by the group by interval, it create a boundary. So in this sql, the boundary should be timestamp 0, timestamp 420, timestamp 840 etc. 2019-12-31 23:58:00 convert to timestamp 1577836680, it\u0026rsquo;s dividable by 420, so this is the nearest time boundary among the given data.\nWhen you use gourp by time(1w), you will also meet this problem: the result time begins with Thursday rather than Monday. As 1970-01-01 is Thursday.\nSo when you use group by time statement, you\u0026rsquo;d better use 30s, 1m, 5m, 10m as interval, which are factors of 1h, so the result always begin at xx:00.\nSome times you want to calculate the sum of last recent 5m data every minute, by using group by time(5m), you only get 1 result every 5 minute. To achieve this, you can use the offset parameter in group by time statement. For example, group by time(5m,1m) with move the time boundary 1 minute forward, the result will be xx:01, xx:06. you can create 5 continuous queries with offset from 0 to 4.\nMore example can be found in this repo.\nGroup by in Continuous Queries #By reading the official resample document, the resample every \u0026lt;interval\u0026gt; for \u0026lt;interval\u0026gt; can override the continuous queries execute interval and the time range of query statement.\nThe example in official document the interval is always a multiple of group by time(m). I tries different values, here is the result.\nEvery Interval #every interval can be any value regardless of group by time interval. The CQ will execute at the time boundary of every interval.\nFor Interval #for interval can be greater or equal to group by time(xx). If it is less than group by interval, influx will raise an error like this: ERR: error parsing query: FOR duration must be \u0026gt;= GROUP BY time duration: must be a minimum of 20s, got 5s\nStart Time and End Time in CQs #Here is a simple example, every 10 s for 45s group by time(20s)\nexecute time selected start time selected end time real start time real end time 16:00:30 15:59:45 16:00:30 16:00:00 16:00:40 16:00:40 15:59:55 16:00:40 16:00:00 16:00:40 16:00:50 16:00:05 16:00:50 16:00:20 16:01:00 16:01:00 16:00:15 16:01:00 16:00:20 16:01:00 We can see that, the execute interval is always 10s, but the start time and end time in CQ not equals to now()-45s-now(). It still based on group by time\u0026rsquo;s time boundary, but the start time must \u0026gt;= selected start time and end time is also \u0026gt;= selected end time.\nHere is another example, every 5s for 10s group by time(10s)\nexecute time selected start time selected end time real start time real end time 16:00:00 15:59:50 16:00:00 16:59:50 16:00:00 16:00:05 15:59:55 16:00:05 16:00:00 16:00:10 16:00:10 16:00:00 16:00:10 16:00:00 16:00:10 16:00:15 16:00:05 16:00:15 16:00:10 16:00:20 I guess the reason why start time is always \u0026gt;= selected start time is to prevent pollute previous data. If the aggregated data is not enough, it will overwrite the correct data generated before. If there is not enough data in end time clause, it will be correct in the future.\nRef # group by time syntax continuous queries advanced syntax ","date":"29 March 2020","permalink":"https://fromkk.com/posts/time-boundary-in-influxdb-group-by-time-statement/","section":"Posts","summary":"\u003cp\u003eThese days I  use InfluxDB to save some time series data. I love these features it provides:\u003c/p\u003e\n\u003ch4 id=\"high-performance\" class=\"relative group\"\u003eHigh Performance \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#high-performance\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h4\u003e\u003cp\u003eAccording to to it\u0026rsquo;s \u003ca href=\"https://docs.influxdata.com/influxdb/v1.7/guides/hardware_sizing/#single-node-or-cluster\" target=\"_blank\" rel=\"noreferrer\"\u003ehardware guide\u003c/a\u003e, a single node will support more than 750k point write per second, 100 moderate queries per second and 10M series cardinality.\u003c/p\u003e","title":"Time boundary in InfluxDB Group by Time Statement"},{"content":"Python supports multiple inheritance, its class can be derived from more than one base classes. If the specified attribute or methods was not found in current class, how to decide the search sequence from superclasses? In simple scenario, we know left-to right, bottom to up. But when the inheritance hierarchy become complicated, it\u0026rsquo;s not easy to answer by intuition.\nFor instance, what\u0026rsquo;s search sequence of class M?\nclass X:pass class Y: pass class Z:pass class A(X,Y):pass class B(Y,Z):pass class M(B,A,Z):pass The answer is: M, B, A, X, Y, Z, object\nC3 Algorithm #How did Python generate this sequence? After Python 2.3, it use C3 Linearization algorithm.\nC3 follows these two equation:\nL[object] = [object] L[C(B1…BN)] = [C] + merge(L[B1]…L[BN], [B1, … ,BN]) L[C] is the MRO of class C, it will evaluate to a list.\nThe key process is merge, it get a list and generate a list by this way:\nFirst, check the first list\u0026rsquo;s head element(L[B1]) as H. If H is not in the tail of other list, output it, and remove it from all of the list, then go to step 1. Otherwise, check the next list\u0026rsquo;s head as H, go to step 2. (tail means the rest of the list except the first element) If merge\u0026rsquo;s list is empty, end algorithm. If list is not empty but not able to find element to output, raise error. That seems complicated, I\u0026rsquo;ll use the previous example again to explain the calculation of C3.\nLet\u0026rsquo;s begin with the easy ones. Firstly, calculate A\u0026rsquo;s MRO:\nL[A(X,Y)]=[A]+merge(L[X],L[Y],[X,Y]) =[A]+merge([X,obj],[Y,obj],[X,Y]) # X is not tail of other list, use it as H =[A,X]+merge([obj],[Y,obj],[Y]) # obj is in the tail of[Y.obj], use Y as H =[A,X,Y]+merge([obj],[obj]] =[A,X,Y,obj] B\u0026rsquo;s MRO [B,Y,Z,obj] and Z\u0026rsquo;s MRO [z,obj] can also be calculated.\nNow we can get M\u0026rsquo;s MRO:\nL[M(B,A,Z)]=[M]+merge(L[B],L[A],L[Z],[B,A,Z]) =[M]+merge([B,Y,Z,obj],[A,X,Y,obj],[Z,obj],[B,A,Z]) =[M,B]+merge([Y,Z,obj],[A,X,Y,obj],[Z,obj],[A,Z]) # Y is in the tail of [A,X,Y,obj], use A as H =[M,B,A]+merge([Y,Z,obj],[X,Y,obj],[Z,obj],[Z]) # Y is in the tail of [X,Y,obj], use X as H =[M,B,A,X]+merge([Y,Z,obj],[Y,obj],[Z,obj],[Z]) =[M,B,A,X,Y]+merge([Z,obj],[obj],[Z,obj],[Z]) =[M,B,A,X,Y,Z]+merge([obj],[obj],[obj]) =[M,B,A,X,Y,Z,obj] MRO and super() #super also use C3 to find the inherited method to execute.\nFor instance, C\u0026rsquo;s MRO is C,A,B,Base,obj, so after enter A, it will output enter B rather than enter base.\nclass Base: def __init__(self): print(\u0026#39;enter base\u0026#39;) print(\u0026#39;leave base\u0026#39;) class A(Base): def __init__(self): print(\u0026#39;enter A\u0026#39;) super(A, self).__init__() print(\u0026#39;leave A\u0026#39;) class B(Base): def __init__(self): print(\u0026#39;enter B\u0026#39;) super(B, self).__init__() print(\u0026#39;leave B\u0026#39;) class C(A, B): def __init__(self): print(\u0026#39;enter C\u0026#39;) super(C, self).__init__() print(\u0026#39;leave C\u0026#39;) c = C() enter C enter A enter B enter base leave base leave B leave A leave C super works like this, it will get inst\u0026rsquo;s MRO, find cls\u0026rsquo;s index, return next class in MRO. (In python3, super(A,self) can be write as super())\ndef super(cls, inst): mro = inst.__class__.mro() return mro[mro.index(cls) + 1] When running this line super(C, self).__init__(), self is C\u0026rsquo;s instance, mro is:\n[\u0026lt;class \u0026#39;__main__.C\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.A\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.B\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.Base\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;object\u0026#39;\u0026gt;] So it returns A, and A will execute __init__(), then calling super(A, self).__init__(), end enter B\u0026rsquo;s __init__(). (C\u0026rsquo;s instance inst will pass as self in the calling chain.)\nRef # The Python 2.3 Method Resolution Order Python Multiple Inheritance python之理解super及MRO列表 Python的MRO以及C3线性化算法 C3 linearization ","date":"14 March 2020","permalink":"https://fromkk.com/posts/c3-linearization-and-python-mro--method-resolution-order/","section":"Posts","summary":"\u003cp\u003ePython supports multiple inheritance, its class can be derived from more than one base classes. If the specified attribute or methods was not found in current class, how to decide the search sequence from superclasses? In simple scenario, we know left-to right, bottom to up. But when the inheritance hierarchy become complicated, it\u0026rsquo;s not easy to answer by intuition.\u003c/p\u003e","title":"C3 Linearization and Python MRO(Method Resolution Order)"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/mro/","section":"Tags","summary":"","title":"MRO"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/defer/","section":"Tags","summary":"","title":"Defer"},{"content":"defer is a useful function to do cleanup, as it will execute in LIFO order before the surrounding function returns. If you don\u0026rsquo;t know how it works, sometimes the execution result may confuse you.\nHow it Works and Why Value or Pointer Receiver Matters #I found an interesting code on Stack Overflow:\ntype X struct { S string } func (x X) Close() { fmt.Println(\u0026#34;Value-Closing\u0026#34;, x.S) } func (x *X) CloseP() { fmt.Println(\u0026#34;Pointer-Closing\u0026#34;, x.S) } func main() { x := X{\u0026#34;Value-X First\u0026#34;} defer x.Close() x = X{\u0026#34;Value-X Second\u0026#34;} defer x.Close() x2 := X{\u0026#34;Value-X2 First\u0026#34;} defer x2.CloseP() x2 = X{\u0026#34;Value-X2 Second\u0026#34;} defer x2.CloseP() xp := \u0026amp;X{\u0026#34;Pointer-X First\u0026#34;} defer xp.Close() xp = \u0026amp;X{\u0026#34;Pointer-X Second\u0026#34;} defer xp.Close() xp2 := \u0026amp;X{\u0026#34;Pointer-X2 First\u0026#34;} defer xp2.CloseP() xp2 = \u0026amp;X{\u0026#34;Pointer-X2 Second\u0026#34;} defer xp2.CloseP() } The output is:\n1 2 3 4 5 6 7 8 Pointer-Closing Pointer-X2 Second Pointer-Closing Pointer-X2 First Value-Closing Pointer-X Second Value-Closing Pointer-X First Pointer-Closing Value-X2 Second Pointer-Closing Value-X2 Second Value-Closing Value-X Second Value-Closing Value-X First Take a look at line 5-6, why Pointer-Closing Value-X2 Second was printed twice? According to Effective Go, \u0026ldquo;The arguments to the deferred function (which include the receiver if the function is a method) are evaluated when the defer executes, not when the call executes.\u0026rdquo;. And the function\u0026rsquo;s parameters will saved anew when evaluated.\nAs x2 is value and the defer function CloseP\u0026rsquo;s receiver is a pointer, once defer executes, it will create a pointer which points to x2 as function\u0026rsquo;s caller. In the following defer, it will create a pointer which point to x2 again. Although x2.S change to \u0026ldquo;Second\u0026rdquo;, x2\u0026rsquo;s address never changes. Finally, when these two defer is called, the same log was printed again.\nHow to Exit Program and Run all Defer #From Golang Runtime:\nruntime.Goexit() terminates the goroutine that calls it. No other goroutine is affected. Goexit runs all deferred calls before terminating the goroutine. Because Goexit is not a panic, any recover calls in those deferred functions will return nil.\nCalling Goexit from the main goroutine terminates that goroutine without func main returning. Since func main has not returned, the program continues execution of other goroutines. If all other goroutines exit, the program crashes.\nIf you want the program to exit normally, just add defer os.Exit(0) at the top of main function. Here is the example code:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; ) func subGoroutine() { defer fmt.Println(\u0026#34;exit sub routine\u0026#34;) for { fmt.Println(\u0026#34;sub goroutine running\u0026#34;) time.Sleep(1 * time.Second) } } func main() { defer os.Exit(0) defer fmt.Println(\u0026#34;calling os.Exit\u0026#34;) go subGoroutine() time.Sleep(2 * time.Second) runtime.Goexit() } Output:\nsub goroutine running sub goroutine running sub goroutine running calling os.Exit Process finished with exit code 0 The defer code in main goroutine are executed, but those in subGoroutine will not be executed. As os.Exit will\nExit causes the current program to exit with the given status code. Conventionally, code zero indicates success, non-zero an error. The program terminates immediately; deferred functions are not run.\nfrom godoc\nRef # 面向信仰编程 defer Golang defer clarification How to exit a go program honoring deferred calls? Effective Go Golang Runtime Go defer 遇上 os.Exit 時失效 ","date":"19 December 2019","permalink":"https://fromkk.com/posts/difference-between-value-and-pointer-variable-in-defer-in-go/","section":"Posts","summary":"\u003cp\u003e\u003ccode\u003edefer\u003c/code\u003e is a useful function to do cleanup, as it will execute in LIFO order before the surrounding function returns. If you don\u0026rsquo;t know how it works, sometimes the execution result may confuse you.\u003c/p\u003e\n\u003ch2 id=\"how-it-works-and-why-value-or-pointer-receiver-matters\" class=\"relative group\"\u003eHow it Works and Why Value or Pointer Receiver Matters \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#how-it-works-and-why-value-or-pointer-receiver-matters\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eI found an interesting code on \u003ca href=\"https://stackoverflow.com/questions/28893586/golang-defer-clarification\" target=\"_blank\" rel=\"noreferrer\"\u003eStack Overflow\u003c/a\u003e:\u003c/p\u003e","title":"Difference between Value and Pointer variable in Defer in Go"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/go/","section":"Tags","summary":"","title":"Go"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning"},{"content":"Before talking about SimHash, let\u0026rsquo;s review some other methods which can also identify duplication.\nLongest Common Subsequence(LCS) #This is the algorithm used by diff command. It is also edit distance with insertion and deletion as the only two edit operations.\nThis works good for short strings. However, the algorithm\u0026rsquo;s time complexity is \\(O(m*n)\\), if two strings\u0026rsquo; lengths are \\(m\\) and \\(n\\) respectively. So it\u0026rsquo;s not suitable for large corpus. Also, if two corpus consists of same paragraph but the order is not same. LCS treat them as different corpus, and that\u0026rsquo;s not we expected.\nBag of Words(BoW) #Transform document into the words it contains, then using Jaccard Similarity to calculate the similarity.\nFor example, if document A contains {a,b,c} and B contains {a,b,d}, then \\[Similarity = \\frac{A \\cap B}{A \\cap B} = \\frac{\\{a,b\\}}{\\{a,b,c,d\\}}=\\frac{1}{2}\\]\nShingling (n-gram) #BoW drops the word context information. In order to take word context into consideration, we convert sentences into phrases. For instance, roses are red and violets are blue will convert to roses are red, are red and, red and voilets \u0026hellip;\nHashing #Saving shingling result take k times disk space if using k words phrase. To solve this problem, save phrase\u0026rsquo;s hashing value instead of string.\nMinHash #The larger the document is, the more the hashing needs to compare. Is there a way to map documents to constant value? MinHash tackles this problem.\nIt uses \\(k\\) hashing functions to calculate the phrase hashes. Then for each hashing function, using the minimal hashing result as signature. Finally, we get \\(k\\) hashing value as document\u0026rsquo;s signature. The procedure is shown below.\nCompare with Hashing, MinHash successfully reduce the time complexity and storage complexity to \\(O(1)\\), an improvement over \\(O(m+n)\\) and \\(O(n)\\), where n is the phrase number, m is the phrase number to compare.\nSimHash #For a given document, how to find it\u0026rsquo;s most similar document? If using MinHash, we need to travel the whole corpus. Is there any more effective method? SimHash comes to the rescue.\nFor a set of input hashes, SimHash will generate a fingerprint(f-bits vector) for the input And the produced hashes has a property: similar input hashes generate similar fingerprint. So the dissimilarity of two documents can be calculated by the XOR of two fingerprint. In google\u0026rsquo;s Detecting Near-Duplicates for Web Crawling paper, they map 8B web-pages to 64 bits. If two bits differ less than 3 bits, then two web-pages are similar.\nThe calculation of SimHash is quiet simple. Given a set of features extracted from the document and their weights, we\u0026rsquo;ll maintain f-bits vector \\(V\\), and initialize it to zero. Each feature will also hash to f-bit value \\(V_i\\). Then each dimension of \\(V_i\\) will multiply by it\u0026rsquo;s weight \\(W_i\\) and add this new value to \\(V\\). If i-th bits if 1, then \\(V\\) is incremented by the weight of that feature. Otherwise \\(V\\) is decremented by the weight. When all features have been processed, \\(V\\) contains positive and negative dimension. Mapping positive values to 1 and negative numbers to 0 to get the final hash value.\n\\[V = zero\\_or\\_one(\\sum{W_i*inc\\_or\\_dec(V_i)})\\]\nHow to generate features from document #One easy way to do this is to use a window to get sub-string from document. For each sub-string, using the hash value of string as features, and the count of this string as weight.\nFor example, if we has this sentence: kk really rocks!.\nFirst, pre-processing this sentence to kkreallyrocks.\nThen using a window of 4 to generate sub-string from the sentence. We\u0026rsquo;ll get the sub-string and their count: (kkre, 1), (krea, 1), (real, 1) etc.\nSuppose we only get these first 3 sub-string and their hash values are 1001, 0101 and 1101 respectively. Then the final \\(V\\) should be 1101\nHow to find similar document #Iterating over all document and compare with target simhash value is a time consuming operation. Is there any smart way to accomplish this task? In Google\u0026rsquo;s paper, they published a very neat algorithm.\nIf the hash value is a 64-bit vector, and we want to find the document which is 2-bit differs with the target. Then we can divided the vector to 4 part: \\(A\\), \\(B\\), \\(C\\) and \\(D\\). Then we know that at least two part should be the identical.\nSuppose part \\(A\\) and \\(B\\) is identical, if we have sorted the hash by \\(ABCD\\) order, we can easily find all hash that \\(AB\\) part is identical. Then we can compare the rest part \\(B\\) and \\(C\\) and find hash vectors that differs from target at most 2 bit. If you have 8B(\\(2^{34}\\)) document and documents are distributed uniformly at random, on average, you only need to compare \\(2^{34-32}=4\\) fingerprints.\nBesides \\(AB\\), \\(AC\\), \\(AD\\), \\(BC\\), \\(BD\\) and \\(CD\\) may also be identical. So you need to keep \\(C_4^2=6\\) sorted list, and compare 4 fingerprints in each list. You don\u0026rsquo;t need to compare 8B documents anymore, that\u0026rsquo;s a great improvement.\nDepending on the fingerprints\u0026rsquo; bit and documents number, you need to find a optimal number to split the hash value.\nRef # Near-Duplicate Detection Detecting Near-Duplicates for Web Crawling simhash-py ","date":"4 December 2019","permalink":"https://fromkk.com/posts/near-duplicate-with-simhash/","section":"Posts","summary":"\u003cp\u003eBefore talking about \u003cstrong\u003eSimHash\u003c/strong\u003e, let\u0026rsquo;s review some other methods which can also identify duplication.\u003c/p\u003e\n\u003ch2 id=\"longest-common-subsequence--lcs\" class=\"relative group\"\u003eLongest Common Subsequence(LCS) \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#longest-common-subsequence--lcs\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eThis is the algorithm used by \u003ccode\u003ediff\u003c/code\u003e command. It is also \u003cstrong\u003eedit distance\u003c/strong\u003e with insertion and deletion as the only two edit operations.\u003c/p\u003e","title":"Near-duplicate with SimHash"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/simhash/","section":"Tags","summary":"","title":"SimHash"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/jaeger/","section":"Tags","summary":"","title":"Jaeger"},{"content":"Here is the main logic for jaeger agent and jaeger collector. (Based on jaeger 1.13.1)\nJaeger Agent #Collect UDP packet from 6831 port, convert it to model.Span, send to collector by gRPC\nJaeger Collector #Process gRPC or process packet from Zipkin(port 9411).\nJaeger Query #Listen gRPC and HTTP request from 16686.\n","date":"22 September 2019","permalink":"https://fromkk.com/posts/jaeger-code-structure/","section":"Posts","summary":"\u003cp\u003eHere is the main logic for jaeger agent and jaeger collector. (Based on \u003ca href=\"https://github.com/jaegertracing/jaeger\" target=\"_blank\" rel=\"noreferrer\"\u003ejaeger\u003c/a\u003e 1.13.1)\u003c/p\u003e\n\n  \n  \n  \n  \n  \n\n  \n  \n    \n    \n  \n  \u003cfigure class=\"mx-auto my-0 rounded-md\"\u003e\n    \u003cimg src=\"/images/jaeger.svg\" alt=\"\" class=\"mx-auto my-0 rounded-md\"/\u003e\n    \n  \u003c/figure\u003e\n\n\n\u003ch2 id=\"jaeger-agent\" class=\"relative group\"\u003eJaeger Agent \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#jaeger-agent\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eCollect UDP packet from 6831 port, convert it to \u003ccode\u003emodel.Span\u003c/code\u003e, send to collector by gRPC\u003c/p\u003e","title":"Jaeger Code Structure"},{"content":"Thanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.\nFirst, this is the graph that was referenced by almost all of the post related to Transformer.\nTransformer consists of these parts: Input, Encoder*N, Output Input, Decoder*N, Output. I\u0026rsquo;ll explain them step by step.\nInput #The input word will map to 512 dimension vector. Then generate Positional Encoding(PE) and add it to the original embeddings.\nPositional Encoding #The transformer model does not contains recurrence and convolution. In order to let the model capture the sequence of input word, it add PE into embeddings.\nPE will generate a 512 dimension vector for each position:\n\\[\\begin{align*} PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{model}}) \\\\ PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{model}}) \\end{align*}\\] The even and odd dimension use sin and cos function respectively.\nFor example, the second word\u0026rsquo;s PE should be: \\(sin(2 / 10000^{0 / 512}), cos(2 / 10000^{0 / 512}), sin(2 / 10000^{2 / 512}), cos(2 / 10000^{2 / 512})\\text{\u0026hellip;}\\)\nThe value range of PE is (-1,1), and each position\u0026rsquo;s PE is slight different, as cos and sin has different frequency. Also, for any fixed offset k, \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\).\nFor even dimension, let \\(10000^{2i/d_{model}}\\) be \\(\\alpha\\), for even dimension:\n\\[\\begin{aligned} PE_{pos+k}\u0026amp;=sin((pos+k)/\\alpha) \\\\ \u0026amp;=sin(pos/\\alpha)cos(k/\\alpha)+cos(pos/\\alpha)sin(k/\\alpha)\\\\ \u0026amp;=PE_{pos\\_even}K_1+PE_{pos\\_odd}K_2 \\end{aligned}\\]\nThe PE implementation in tensor2tensor use sin in first half of dimension and cos in the rest part of dimension.\nEncoder #There are 6 Encoder layer in Transformer, each layer consists of two sub-layer: Multi-Head Attention and Feed Forward Neural Network.\nMulti-Head Attention #Let\u0026rsquo;s begin with single head attention. In short, it maps word embeddings to q k v and use q k v vector to calculate the attention.\nThe input words map to q k v by multiply the Query, Keys Values matrix. Then for the given Query, the attention for each word in sentence will be calculated by this formula: \\(\\mathrm{attention}=\\mathrm{softmax}(\\frac{qk^T}{\\sqrt{d_k}})v\\), where q k v is a 64 dimension vector.\nMatrix view:\n\\(Attention(Q, K, V) = \\mathrm{softmax}(\\frac{(XW^Q)(XW^K)^T}{\\sqrt{d_k}})(XW^V)\\) where \\(X\\) is the input embedding.\nThe single head attention only output a 64 dimension vector, but the input dimension is 512. How to transform back to 512? That\u0026rsquo;s why transformer has multi-head attention.\nEach head has its own \\(W^Q\\) \\(W^K\\) \\(W^V\\) matrix, and produces \\(Z_0,Z_1\u0026hellip;Z_7\\),(\\(Z_0\\)\u0026rsquo;s shape is (512, 64)) the concat the outputted vectors as \\(O\\). \\(O\\) will multiply a weight matrix \\(W^O\\) (\\(W^O\\)\u0026rsquo;s shape is (512, 512)) and the result is \\(Z\\), which will be sent to Feed Forward Network.\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\nThe whole procedure looks like this:\nAdd \u0026amp; Norm #This layer works like this line of code: norm(x+dropout(sublayer(x))) or x+dropout(sublayer(norm(x))). The sublayer is Multi-Head Attention or FF Network.\nLayer Normalization #Layer Norm is similar to Batch Normalization, but it tries to normalize the whole layer\u0026rsquo;s features rather than each feature.(Scale and Shift also apply for each feature) More details can be found in this paper.\nPosition-wise Feed Forward Network #This layer is a Neural Network whose size is (512, 2048, 512). The exact same feed-forward network is independently applied to each position.\nOutput Input #Same as Input.\nDecoder #The decoder is pretty similar to Encoder. It also has 6 layers, but has 3 sublayers in each Decoder. It add a masked multi-head-attention at the beginning of Decoder.\nMasked Multi-Head Attention #This layer is used to block future words during training. For example, if the output is \u0026lt;bos\u0026gt; hello world \u0026lt;eos\u0026gt;. First, we should use \u0026lt;bos\u0026gt; as input to predict hello, hello world \u0026lt;eos\u0026gt; will be masked to 0.\nKey and Value in Decoder Multi-Head Attention Layer #In Encoder, the q k v vector is generated by \\(XW^Q\\), \\(XW^K\\) and \\(XW^V\\). In the second sub-layer of Decoder, q k v was generated by \\(XW^Q\\), \\(YW^K\\) and \\(YW^V\\), where \\(Y\\) is the Encoder\u0026rsquo;s output, \\(X\\) is the \u0026lt;init of sentence\u0026gt; or previous output.\nThe animation below illustrates how to apply the Transformer to machine translation.\nOutput #Using a linear layer to predict the output.\nRef # The Annotated Transformer The Illustrated Transformer The Transformer – Attention is all you need Seq2seq pay Attention to Self Attention: Part 2 Transformer模型的PyTorch实现 How to code The Transformer in Pytorch Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention Transformer: A Novel Neural Network Architecture for Language Understanding Dive into Deep Learning - 10.3 Transformer 10分钟带你深入理解Transformer原理及实现 ","date":"1 September 2019","permalink":"https://fromkk.com/posts/the-annotated-the-annotated-transformer/","section":"Posts","summary":"\u003cp\u003eThanks for the articles I list at the end of this post, I understand how transformers works. These posts are comprehensive, but there are some points that confused me.\u003c/p\u003e\n\u003cp\u003eFirst, this is the graph that was referenced by almost all of the post related to Transformer.\u003c/p\u003e","title":"The Annotated The Annotated Transformer"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/transformer/","section":"Tags","summary":"","title":"Transformer"},{"content":"\\(s_t\\) and \\(h_i\\) are source hidden states and target hidden state, the shape is (n,1). \\(c_t\\) is the final context vector, and \\(\\alpha_{t,s}\\) is alignment score.\n\\[\\begin{aligned} c_t\u0026amp;=\\sum_{i=1}^n \\alpha_{t,s}h_i \\\\ \\alpha_{t,s}\u0026amp;= \\frac{\\exp(score(s_t,h_i))}{\\sum_{i=1}^n \\exp(score(s_t,h_i))} \\end{aligned}\\]\nGlobal(Soft) VS Local(Hard) #Global Attention takes all source hidden states into account, and local attention only use part of the source hidden states.\nContent-based VS Location-based #Content-based Attention uses both source hidden states and target hidden states, but location-based attention only use source hidden states.\nHere are several popular attention mechanisms:\nDot-Product #\\[score(s_t,h_i)=s_t^Th_i\\]\nScaled Dot-Product #\\[score(s_t,h_i)=\\frac{s_t^Th_i}{\\sqrt{n}}\\] where n is the vectors dimension. Google\u0026rsquo;s Transformer model has similar scaling factor when calculate self-attention: \\(score=\\frac{KQ^T}{\\sqrt{n}}\\)\nLocation-Base #\\[socre(s_t,h_i)=softmax(W_as_t)\\]\nGeneral #\\[score(s_t,h_i)=s_t^TW_ah_i\\]\n\\(Wa\\)\u0026rsquo;s shape is (n,n)\nConcat #\\[score(s_t,h_i)=v_a^Ttanh(W_a[s_t,h_i])\\]\n\\(v_a\\)\u0026rsquo;s shape is (x,1), and \\(Wa\\) \u0026rsquo;s shape is (x,x). This is similar to a neural network with one hidden layer.\nWhen I doing a slot filling project, I compare these mechanisms. Concat attention produce the best result.\nRef # Attention Variants Attention? Attention! Attention Seq2Seq with PyTorch: learning to invert a sequence ","date":"15 July 2019","permalink":"https://fromkk.com/posts/different-types-of-attention/","section":"Posts","summary":"\u003cp\u003e\\(s_t\\) and \\(h_i\\) are source hidden states and target hidden state, the shape is \u003ccode\u003e(n,1)\u003c/code\u003e. \\(c_t\\) is the final context vector, and \\(\\alpha_{t,s}\\) is alignment score.\u003c/p\u003e\n\u003cp\u003e\\[\\begin{aligned}\nc_t\u0026amp;=\\sum_{i=1}^n \\alpha_{t,s}h_i \\\\\n\\alpha_{t,s}\u0026amp;= \\frac{\\exp(score(s_t,h_i))}{\\sum_{i=1}^n \\exp(score(s_t,h_i))}\n\\end{aligned}\\]\u003c/p\u003e\n\u003ch2 id=\"global--soft--vs-local--hard\" class=\"relative group\"\u003eGlobal(Soft) VS Local(Hard) \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#global--soft--vs-local--hard\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eGlobal Attention takes all source hidden states into account, and local attention only use part of the source hidden states.\u003c/p\u003e","title":"Different types of Attention"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/pytorch/","section":"Tags","summary":"","title":"PyTorch"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/torchtext/","section":"Tags","summary":"","title":"Torchtext"},{"content":"Load separate files #data.Field parameters is here.\nWhen calling build_vocab, torchtext will add \u0026lt;unk\u0026gt; in vocabulary list. Set unk_token=None if you want to remove it. If sequential=True (default), it will add \u0026lt;pad\u0026gt; in vocab. \u0026lt;unk\u0026gt; and \u0026lt;pad\u0026gt; will add at the beginning of vocabulary list by default.\nLabelField is similar to Field, but it will set sequential=False, unk_token=None and is_target=Ture\nINPUT = data.Field(lower=True, batch_first=True) TAG = data.LabelField() train, val, test = data.TabularDataset.splits(path=base_dir.as_posix(), train=\u0026#39;train_data.csv\u0026#39;, validation=\u0026#39;val_data.csv\u0026#39;, test=\u0026#39;test_data.csv\u0026#39;, format=\u0026#39;tsv\u0026#39;, fields=[(None, None), (\u0026#39;input\u0026#39;, INPUT), (\u0026#39;tag\u0026#39;, TAG)]) Load single file #all_data = data.TabularDataset(path=base_dir / \u0026#39;gossip_train_data.csv\u0026#39;, format=\u0026#39;tsv\u0026#39;, fields=[(\u0026#39;text\u0026#39;, TEXT), (\u0026#39;category\u0026#39;, CATEGORY)]) train, val, test = all_data.split([0.7, 0.2, 0.1]) Create iterator #train_iter, val_iter, test_iter = data.BucketIterator.splits( (train, val, test), batch_sizes=(32, 256, 256), shuffle=True, sort_key=lambda x: x.input) Load pretrained vector #vectors = Vectors(name=\u0026#39;cc.zh.300.vec\u0026#39;, cache=\u0026#39;./\u0026#39;) INPUT.build_vocab(train, vectors=vectors) TAG.build_vocab(train, val, test) Check vocab sizes #You can view vocab index by vocab.itos.\ntag_size = len(TAG.vocab) Use field vector in model #vec = INPUT.vocab.vectors class Model: nn.Embedding.from_pretrained(vec, freeze=False) Convert text to vector #s = \u0026#39; \u0026#39;.join(segmentize(s)) s = INPUT.preprocess(s) vec = INPUT.process([s]) ","date":"1 July 2019","permalink":"https://fromkk.com/posts/torchtext-snippets/","section":"Posts","summary":"\u003ch2 id=\"load-separate-files\" class=\"relative group\"\u003eLoad separate files \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#load-separate-files\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003e\u003ccode\u003edata.Field\u003c/code\u003e parameters is \u003ca href=\"https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field\" target=\"_blank\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhen calling \u003ccode\u003ebuild_vocab\u003c/code\u003e, torchtext will add \u003ccode\u003e\u0026lt;unk\u0026gt;\u003c/code\u003e in vocabulary list. Set \u003ccode\u003eunk_token=None\u003c/code\u003e if you want to remove it. If \u003ccode\u003esequential=True\u003c/code\u003e (default), it will add \u003ccode\u003e\u0026lt;pad\u0026gt;\u003c/code\u003e in vocab. \u003ccode\u003e\u0026lt;unk\u0026gt;\u003c/code\u003e and \u003ccode\u003e\u0026lt;pad\u0026gt;\u003c/code\u003e will add at the beginning of vocabulary list by default.\u003c/p\u003e","title":"Torchtext snippets"},{"content":"After Inoreader change the free plan, which limit the max subscription to 150, I begin to find an alternative. Finally, I found Tiny Tiny RSS. It has a nice website and has the fever API Plugin which was supported by most of the RSS reader app, so you can read RSS on all of you devices.\nThis post will tell you how to deploy it on your server.\nPrerequisite #You need to install Docker and Docker Compose before using docker-compose.yml\nInstall docker #Make a new ttrss folder, create docker-compose.yml with this content:\nversion: \u0026#34;3\u0026#34; services: database.postgres: image: sameersbn/postgresql:latest container_name: postgres environment: - PG_PASSWORD=PWD # please change the password - DB_EXTENSION=pg_trgm volumes: - ~/postgres/data/:/var/lib/postgresql/ # persist postgres data to ~/postgres/data/ on the host ports: - 5433:5432 restart: always service.rss: image: wangqiru/ttrss:latest container_name: ttrss ports: - 181:80 environment: - SELF_URL_PATH=https://RSS.com/ # please change to your own domain - DB_HOST=database.postgres - DB_PORT=5432 - DB_NAME=ttrss - DB_USER=postgres - DB_PASS=PWD # please change the password - ENABLE_PLUGINS=auth_internal,fever,api_newsplus # auth_internal is required. Plugins enabled here will be enabled for all users as system plugins - SESSION_COOKIE_LIFETIME = 8760 stdin_open: true tty: true restart: always command: sh -c \u0026#39;sh /wait-for.sh database.postgres:5432 -- php /configure-db.php \u0026amp;\u0026amp; exec s6-svscan /etc/s6/\u0026#39; service.mercury: # set Mercury Parser API endpoint to =service.mercury:3000= on TTRSS plugin setting page image: wangqiru/mercury-parser-api:latest container_name: mercury expose: - 3000 ports: - 3000:3000 restart: always Run this command to deploy: docker-compose up -d. After it finished, the TTRSS service is running on port 181, the default account is admin with password password.\nI made minor modification on the yml file, you can find the latest file here.\nNginx Configuration #If you have a domain and you can use Nginx as reverse proxy to redirect TTRSS to the domain.\nupstream ttrssdev { server 127.0.0.1:181; } server { listen 80; server_name RSS.com; return 301 https://RSS.com/$request_uri; } server { listen 443 ssl; gzip on; server_name RSS.com; access_log /var/log/nginx/ttrssdev_access.log combined; error_log /var/log/nginx/ttrssdev_error.log; location / { proxy_redirect off; proxy_pass http://ttrssdev; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-Ssl on; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Frame-Options SAMEORIGIN; client_max_body_size 100m; client_body_buffer_size 128k; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; } ssl_certificate /etc/letsencrypt/live/rss.fromkk.com/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/rss.fromkk.com/privkey.pem; # managed by Certbot } To enable HTTPS on your website, you can use certbot.\nCaddy Configuration #Update in 22/12/2021\nI found Caddy2 is much easier to use than Nginx, all you need to do is add 3 lines in `/etc/caddy/Caddyfile`\nrss.com { encode gzip zstd reverse_proxy 127.0.0.1:181 } Voila, a HTTPS enabled website is deployed.\nFever API and Mercury # Fever Check Enable API: Allows accessing this account through the API in preference Enter a new password for fever in Plugins - Fever Emulation Mecury Fulltext Extraction Check mecury-fulltext plugin in Preference - Plugins Set Mercury Parser API address to service.mercury:3000 in Feeds - Mercury Fulltext settings Update #Simply run this command to update TTRSS code.\ndocker-compose pull docker-compose up -d App recommendation #Reeder 4 works great on my iPad. It\u0026rsquo;s smooth and fast, and is worth every penny.\nIf you want a free app, I suggest Fiery Feeds. I stopped using it after ver 2.2, as it\u0026rsquo;s so lagging. If this issue was fixed, I thought it was the biggest competitor for Reeder 4. For more alternative, read this article: The Best RSS App for iPhone and iPad.\nupdate 25-03-20: You can find the latest document here.\nRef # A ttrss setup guide - Start your own RSS aggregator today ","date":"10 June 2019","permalink":"https://fromkk.com/posts/build-your-own-tiny-tiny-rss-service/","section":"Posts","summary":"\u003cp\u003eAfter Inoreader change the free plan, which limit the max subscription to 150, I begin to find an alternative. Finally, I found Tiny Tiny RSS. It has a nice website and has the fever API Plugin which was supported by most of the RSS reader app, so you can read RSS on all of you devices.\u003c/p\u003e","title":"Build Your Own Tiny Tiny RSS Service"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/latex/","section":"Tags","summary":"","title":"LaTeX"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/org-mode/","section":"Tags","summary":"","title":"Org Mode"},{"content":"Using the right Emacs Version #I failed to preview LaTeX with emacs-plus. If you have installed d12frosted/emacs-plus, uninstall it and use emacs-mac.\nbrew tap railwaycat/emacsmacport brew install emacs-mac If you like the fancy spacemacs icon, install it with cask: brew cask install emacs-mac-spacemacs-icon\nInstall Tex # Download and install BasicTeX.pkg here. Add /Library/TeX/texbin to PATH. Install dvisvgm by sudo tlmgr update --self \u0026amp;\u0026amp; sudo tlmgr install dvisvgm collection-fontsrecommended Emacs settings # Add TeX related bin to path: (setenv \u0026quot;PATH\u0026quot; (concat (getenv \u0026quot;PATH\u0026quot;) \u0026quot;:/Library/TeX/texbin\u0026quot;)) Tell Org Mode to create svg images: (setq org-latex-create-formula-image-program 'dvisvgm) Now you can see the rendered LaTeX equation by calling org-preview-latex-fragment or using shortcut ,Tx.\nIf you want to load LaTeX previews automatically at startup, add this at the beginning of org file: #+STARTUP: latexpreview.\nupdate 31-07-19\n_ and ... are not displayed in Emacs, as some fonts are missing. tlmgr install collection-fontsrecommended should fix this.\nIf Org Preview Latex buffer output warn processing of PostScript specials is disabled (Ghostscript not found), run brew install ghostscript.\n","date":"12 May 2019","permalink":"https://fromkk.com/posts/preview-latex-in-org-mode-with-emacs-in-macos/","section":"Posts","summary":"\u003ch2 id=\"using-the-right-emacs-version\" class=\"relative group\"\u003eUsing the right Emacs Version \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#using-the-right-emacs-version\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eI failed to preview LaTeX with \u003ccode\u003eemacs-plus\u003c/code\u003e. If you have installed \u003ccode\u003ed12frosted/emacs-plus\u003c/code\u003e, uninstall it and use \u003ccode\u003eemacs-mac\u003c/code\u003e.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-nil\" data-lang=\"nil\"\u003ebrew tap railwaycat/emacsmacport\nbrew install emacs-mac\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf you like the fancy spacemacs icon, install it with cask: \u003ccode\u003ebrew cask install emacs-mac-spacemacs-icon\u003c/code\u003e\u003c/p\u003e","title":"Preview LaTeX in Org Mode with Emacs in MacOS"},{"content":"PyTorch provide a simple DQN implementation to solve the cartpole game. However, the code is incorrect, it diverges after training (It has been discussed here).\nThe official code\u0026rsquo;s training data is below, it\u0026rsquo;s high score is about 50 and finally diverges.\nThere are many reason that lead to divergence.\nFirst it use the difference of two frame as input in the tutorial, not only it loss the cart\u0026rsquo;s absolute information(This information is useful, as game will terminate if cart moves too far from centre), but also confused the agent when difference is the same but the state is varied.\nSecond, small replay memory. If the memory is too small, the agent will forget the strategy it has token in some state. I\u0026rsquo;m not sure whether 10000 memory is big enough, but I suggest using a higher value.\nThird, the parameters. learning_rate, target_update_interval may cause fluctuation. Here is a example on Stack Overflow. I also met this problem when training cartpole agent. The reward stops growing after 1000 episode.\nAfter doing some research on the cartpole DNQ code, I managed to made a model to play the flappy bird. Here are the changes from the original cartpole code. Most of the technology can be found in these two papers: Playing Atari with Deep Reinforcement Learning and Rainbow: Combining Improvements in Deep Reinforcement Learning.\nHere is the model architecture:\nHere is a trained result:\n{{\u0026lt; youtube NV82ZUQynuQ \u0026gt;}}\nDueling DQN\nThe vanilla DQN has the overestimate problem. As the max function will accumulate the noise when training. This leads to converging at suboptimal point. Two following architectures are submitted to solve this problem.\n\\[ Q(s, a) = r + \\gamma \\max_{a\u0026rsquo;}[Q(s\u0026rsquo;, a\u0026rsquo;)] \\]\nDouble DQN was published two year later DQN. It has two value function, one is used to choose the action with max Q value, another one is used to calculate the Q value of this action.\n\\[ a^{max}(S\u0026rsquo;_j, w) = \\arg\\max_{a\u0026rsquo;}Q(\\phi(S\u0026rsquo;_j),a,w) \\]\n\\[ Q(s,a) = r + \\gamma Q\u0026rsquo;(\\phi(S\u0026rsquo;_j),a^{max}(S\u0026rsquo;_j, w),w\u0026rsquo;) \\]\nDueling DQN is another solution. It has two estimator, one estimates the score of current state, another estimates the action score.\n\\[Q(s, a) = r + \\gamma( \\max_{a’}[A(s\u0026rsquo;,a\u0026rsquo;)+V(s\u0026rsquo;)]\\]\nIn order to distinguish the score of the actions, the return the Q-value will minus the mean action score:\nx=val+adv-adv.mean(1,keepdim=True)\nIn this project, I use dueling DQN.\nImage processing\nI grayscale and crop the image.\nStack frames\nI use the last 4 frame as the input. This should help the agent to know the change of environment.\nExtra FC before last layer\nI add a FC between the image features and the FC for calculate Q-Value.\nFrame Skipping\nFrame-skipping means agent sees and selects actions on every k frame instead of every frame, the last action is repeated on skipped frames. This method will accelerate the training procedure. In this project, I use frame_skipping=2, as the more the frame skipping is, the more the bird is likely to hit the pipe. And this method did help the agent to converge faster. More details can be found in this post.\nPrioritized Experience Replay\nThis idea was published here. It\u0026rsquo;s a very simple idea: replay high TD error experience more frequently. My code implementation is not efficient. But in cartpole game, this technology help the agent converge faster.\nColab and Kaggle Kernel\nMy MacBook doesn\u0026rsquo;t support CUDA, so I use these two website to train the model. Here are the comparison of them. During training, Kaggle seems more stable, Colab usually disconnected after 1h.\nColab Kaggle Kernel GPU Tesla T4(16G) Tesla P100(16G) RAM 13G 13G Max training time 12h 9h Export trained model Google Drive - The lesson I learnt from this project is patience. It takes a long time(maybe hundreds of thousand steps) to see whether this model works, and there are so many parameters can effect the final performance. It takes me about 3 weeks to build the final model. So if you want to build your own model, be patient and good luck. Here are two articles talking about the debugging and hyperparameter tuning in DQN:\nDQN debugging using Open AI gym Cartpole DDQN hyperparameter tuning using Open AI gym Cartpole Here are something may help with this task.\nTensorBoard\nIt\u0026rsquo;s a visualization tool made by TensorFlow Team. It\u0026rsquo;s more convenient to use it rather than generate graph manually by matplotlib. Besides reward and mean_q, these variable are also useful when debugging: TD-error, loss and action_distribution, avg_priority.\nAdvanced image pre-processing\nIn this project, I just grayscalize the image. A more advance technology such as binarize should help agent to filter unimportant detail of game output.\nIn Flappy Bird RL, the author extract the vertical distance from lower pipe and horizontal distance from next pair of pipes as state. The trained agent can achieve 3000 score.\nOther Improvements\nRainbow introduce many other extensions to enhance DQN, some of them have been discussed in this post.\nI\u0026rsquo;ve uploaded code to this repo.\nRef # PyTorch REINFORCEMENT LEARNING (DQN) TUTORIAL 强化学习 (A series of Chinese post about reinforcement learning) Deep Reinforcement Learning for Flappy Bird Flappy-Bird-Double-DQN-Pytorch DeepRL-Tutorials Speeding up DQN on PyTorch: how to solve Pong in 30 minutes Frame Skipping and Pre-Processing for Deep Q-Networks on Atari 2600 Games OpenAI Baselines: DQN Deep-Reinforcement-Learning-Hands-On DQN solution results peak at ~35 reward Update 26-04-19\nColab\u0026rsquo;s GPU has upgrade to Tesla T4 from K80, now it becomes my best bet.\nUpdate 07-05-19\nTensorBoard is now natively supported in PyTorch after version 1.1\nUpdate 26-07-19\nIf you run out of RAM in Colab, it will show up an option to double the RAM.\nUpdate 13-08-19\nUpload video, update code.\n","date":"14 April 2019","permalink":"https://fromkk.com/posts/using-ddqn-to-play-flappy-bird/","section":"Posts","summary":"\u003cp\u003ePyTorch provide a simple DQN implementation to solve the cartpole game. However, the code is incorrect, it diverges after training (It has been discussed \u003ca href=\"https://discuss.pytorch.org/t/dqn-example-from-pytorch-diverged/4123\" target=\"_blank\" rel=\"noreferrer\"\u003ehere\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eThe official code\u0026rsquo;s training data is below, it\u0026rsquo;s high score is about 50 and finally diverges.\u003c/p\u003e","title":"Using Dueling DQN to Play Flappy Bird"},{"content":"Recently, I found a really good example code for Python circular import, and I\u0026rsquo;d like to record it here.\nHere is the code:\n1 2 3 4 5 6 7 8 # X.py def X1(): return \u0026#34;x1\u0026#34; from Y import Y2 def X2(): return \u0026#34;x2\u0026#34; 1 2 3 4 5 6 7 8 # Y.py def Y1(): return \u0026#34;y1\u0026#34; from X import X1 def Y2(): return \u0026#34;y2\u0026#34; Guess what will happen if you run python X.py and python Y.py?\nHere is the answer, the first one outputs this:\nTraceback (most recent call last): File \u0026#34;X.py\u0026#34;, line 4, in \u0026lt;module\u0026gt; from Y import Y2 File \u0026#34;/Users/kk/Y.py\u0026#34;, line 4, in \u0026lt;module\u0026gt; from X import X1 File \u0026#34;/Users/kk/X.py\u0026#34;, line 4, in \u0026lt;module\u0026gt; from Y import Y2 ImportError: cannot import name Y2 The second one runs normally.\nIf this is the same as you thought, you already know how python import works. You don\u0026rsquo;t need to read this post.\nPython import machinery #When Python imports a module for the first time, it create a new module object and set sys.modules[module_name]=module object , then executes execute in module object to define its content. If you import that module again, Python will just return the object save in sys.modules.\nIn X.py line 5, Python add Y into sys.modules and start execute code in Y.py. In Y.xy line5, it pause import Y, add X into sys.modules, and execute code X.py. Back to X.py line5, Python find Y in sys.modules and try to import Y2 in Y. But Y2 is not yet defined, so the ImportError was raised.\nHow to fix # Change import order. Wrap function call related to other module into configure function, call it manually. Dynamic import(use import within a function). Ref # Python Circular Imports Python Cirluar Importing Circular imports in Python Effective Python: 59 Specific Ways to Write Better Python Python doc: The import system ","date":"10 March 2019","permalink":"https://fromkk.com/posts/circular-import-in-python/","section":"Posts","summary":"\u003cp\u003eRecently, I found a really good example code for Python circular import, and I\u0026rsquo;d like to record it here.\u003c/p\u003e\n\u003cp\u003eHere is the code:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e8\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# X.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eX1\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;x1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eY\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eY2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eX2\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;x2\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e8\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python3\" data-lang=\"python3\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Y.py\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eY1\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;y1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003eX\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eX1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eY2\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;y2\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eGuess what will happen if you run \u003ccode\u003epython X.py\u003c/code\u003e and \u003ccode\u003epython Y.py\u003c/code\u003e?\u003c/p\u003e","title":"Circular Import in Python"},{"content":"Overview # CPython allocation memory to save dictionary, the initial table size is 8, entries are saved as \u0026lt;hash,key,value\u0026gt; in each slot(The slot content changed after Python 3.6). When a new key is added, python use i = hash(key) \u0026amp; mask where mask=table_size-1 to calculate which slot it should be placed. If the slot is occupied, CPython using a probing algorithm to find the empty slot to store new item. When 2/3 of the table is full, the table will be resized. When getting item from dictionary, both hash and key must be equal. Resizing #When elements size is below 50000, the table size will increase by a factor of 4 based on used slots. Otherwise, it will increase by a factor of 2. The dictionary size is always \\(2^{n}\\).\ndict size resize when elements in dict new table size 8 6 32 32 22 128 128 86 512 Removing item from dictionary doesn\u0026rsquo;t lead to shrink table. The value of the item will marks as null but not empty. When looking up element in dictionary, it will keep probing once find this special mark. So deleting element from Python will not decrease the memory using. If you really want to do so, you can the items in the old dictionary to create a new one.\nProbing #CPython used a modified random probing algorithm to choose the empty slot. This algorithm can traval all of the slots in a pseudo random order.\nThe travel order can be calculated by this formula: j = ((5*j) + 1) mod 2**i, where j is slot index.\nFor example, if table size is 8, and the calculate slot index is 2, then the traversal order should be:\n2 -\u0026gt; (5*2+1) mod 8 = 3 -\u0026gt; (5*3+1) mod 8 = 0 -\u0026gt; (5*0+1) mod 8 = 1 -\u0026gt; 6 -\u0026gt; 7 -\u0026gt; 4 -\u0026gt; 5 -\u0026gt; 2\nCPython changed this formula by adding perturb and PERTURB_SHIFT variables, where perturb is hash value and PERTURB_SHIFT is 5. By adding PERTURB_SHIFT, the probe sequence depends on every bit in the hash code, and the collision probability is decreased. And perturb will eventually becomes to 0, this ensures that all of the slots will be checked.\nj = (5*j) + 1 + perturb; perturb \u0026gt;\u0026gt;= PERTURB_SHIFT; j = j % 2**i Dictionary improvement after 3.6 #CPython 3.6 use a compact representation to save entries, and \u0026ldquo;The memory usage of the new dict() is between 20% and 25% smaller compared to Python 3.5\u0026rdquo;.\nCompact Hash Table #As mentioned before, entries saved in the form of \u0026lt;hash,key,value\u0026gt;. This will takes 3B on 64 bit machine. And no matter how much item is added into the dictionary, the memory usage is the same(3B*table_size).\nAfter 3.6, CPython use two structure to save data. One is index, another is the real data.\nFor example, if the table size is 8, and there is an item in slot 1, the index looks like this:\n[null, 0, null, null, null, null, null, null]\nAnd the real data is:\n| hash | key | value | | xxx1 | yyy1 | zzz1 | 0 represents the items index on real data. If another item is added in slot 3, the new index become this:\n[null, 0, null, 1, null, null, null, null]\nThe real data become this:\n| hash | key | value | | xxx1 | yyy1 | zzz1 | | xxx2 | yyy2 | zzz2 | This saves memory, especially when table load factor is low.\nRef # How are Python\u0026rsquo;s Built In Dictionaries Implemented cpython source code Is it possible to give a python dict an initial capacity (and is it useful) Python dictionary implementation ","date":"17 February 2019","permalink":"https://fromkk.com/posts/python-dictionary-implementation/","section":"Posts","summary":"\u003ch2 id=\"overview\" class=\"relative group\"\u003eOverview \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#overview\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003col\u003e\n\u003cli\u003eCPython allocation memory to save dictionary, the initial table size is 8, entries are saved as \u003ccode\u003e\u0026lt;hash,key,value\u0026gt;\u003c/code\u003e in each slot(The slot content changed after Python 3.6).\u003c/li\u003e\n\u003cli\u003eWhen a new key is added, python use \u003ccode\u003ei = hash(key) \u0026amp; mask\u003c/code\u003e where \u003ccode\u003emask=table_size-1\u003c/code\u003e to calculate which slot it should be placed. If the slot is occupied, CPython using a probing algorithm to find the empty slot to store new item.\u003c/li\u003e\n\u003cli\u003eWhen 2/3 of the table is full, the table will be resized.\u003c/li\u003e\n\u003cli\u003eWhen getting item from dictionary, both \u003ccode\u003ehash\u003c/code\u003e and \u003ccode\u003ekey\u003c/code\u003e must be equal.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"resizing\" class=\"relative group\"\u003eResizing \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#resizing\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eWhen elements size is below 50000, the table size will increase by a factor of 4 based on used slots. Otherwise, it will increase by a factor of 2. The dictionary size is always \\(2^{n}\\).\u003c/p\u003e","title":"Python Dictionary Implementation"},{"content":"This blog is powered by hugo, and the theme is congo.\nI use Spacemacs and org-mode to write post. Then use ox-hugo to export org file to markdown. Finally GitHub Action will build the blog automatically after I push commit to GitHub.\n","date":"1 February 2019","permalink":"https://fromkk.com/about/","section":"KK's Blog (fromkk)","summary":"\u003cp\u003eThis blog is powered by \u003ca href=\"https://gohugo.io\" target=\"_blank\" rel=\"noreferrer\"\u003ehugo\u003c/a\u003e, and the theme is \u003ca href=\"https://github.com/jpanther/congo\" target=\"_blank\" rel=\"noreferrer\"\u003econgo\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI use \u003ca href=\"https://spacemacs.org\" target=\"_blank\" rel=\"noreferrer\"\u003eSpacemacs\u003c/a\u003e and \u003ca href=\"https://orgmode.org\" target=\"_blank\" rel=\"noreferrer\"\u003eorg-mode\u003c/a\u003e to write post. Then use \u003ca href=\"https://ox-hugo.scripter.co\" target=\"_blank\" rel=\"noreferrer\"\u003eox-hugo\u003c/a\u003e to export org file to markdown. Finally \u003ca href=\"https://gohugo.io/hosting-and-deployment/hosting-on-github/\" target=\"_blank\" rel=\"noreferrer\"\u003eGitHub Action\u003c/a\u003e will build the blog automatically after I push commit to GitHub.\u003c/p\u003e","title":"About"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/textcnn/","section":"Tags","summary":"","title":"TextCNN"},{"content":"PyTorch is a really powerful framework to build the machine learning models. Although some features is missing when compared with TensorFlow (For example, the early stop function, History to draw plot), its code style is more intuitive.\nTorchtext is a NLP package which is also made by pytorch team. It provide a way to read text, processing and iterate the texts.\nGoogle Colab is a Jupyter notebook environment host by Google, you can use free GPU and TPU to run your modal.\nHere is a simple tutorial to build a TextCNN modal and run it on Colab.\nThe TextCNN paper was published by Kim in 2014. The model\u0026rsquo;s idea is pretty simple, but the performance is impressive. If you trying to solve the text classification problem, this model is a good choice to start with.\nThe main architecture is shown below:\nIt uses different kernels to extract text features, then use the softmax regression to classify text base on the features.\nNow we can build this model step by step.\nFirst build the model. The model I use is CNN-multichannel, which contains two sets of word embedding. Both of them is the copy of word embedding generate from corpus, but only one set will update embedding during training.\nThe code is below:\nclass textCNNMulti(nn.Module): def __init__(self,args): super().__init__() dim = args[\u0026#39;dim\u0026#39;] n_class = args[\u0026#39;n_class\u0026#39;] embedding_matrix=args[\u0026#39;embedding_matrix\u0026#39;] kernels=[3,4,5] kernel_number=[150,150,150] self.static_embed = nn.Embedding.from_pretrained(embedding_matrix) self.non_static_embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=False) self.convs = nn.ModuleList([nn.Conv2d(2, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)]) self.dropout=nn.Dropout() self.out = nn.Linear(sum(kernel_number), n_class) def forward(self, x): non_static_input = self.non_static_embed(x) static_input = self.static_embed(x) x = torch.stack([non_static_input, static_input], dim=1) x = [F.relu(conv(x)).squeeze(3) for conv in self.convs] x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] x = torch.cat(x, 1) x = self.dropout(x) x = self.out(x) return x Second, convert text into word index, so each sentence become a vector for training.\nTEXT = data.Field(lower=True,batch_first=True) LABEL = data.LabelField() train, val, test = datasets.SST.splits(TEXT, LABEL, \u0026#39;data/\u0026#39;,fine_grained=True) TEXT.build_vocab(train, vectors=\u0026#34;glove.840B.300d\u0026#34;) LABEL.build_vocab(train,val,test) train_iter, val_iter, test_iter = data.BucketIterator.splits( (train, val, test), batch_sizes=(128, 256, 256),shuffle=True) Field defines how to process text, here is the most common parameters:\nsequential – Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.\nuse_vocab – Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.\npreprocessing – The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor. Default: None.\nbatch_first – Whether to produce tensors with the batch dimension first. Default: False.\ndatasets.SST.splits will load the SST datasets, and split into train, validation, and test Dataset objects.\nbuild_vocab will create the Vocab object for Field, which contains the information to convert word into word index and vice versa. Also, the word embedding will save as Field.Vocab.vectors. vectors contains all of the word embedding. Torchtext can download some pretrained vectors automatically, such as glove.840B.300d, fasttext.en.300d. You can also load your vectors in this way, xxx.vec should be the standard word2vec format.\nfrom torchtext.vocab import Vectors vectors = Vectors(name=\u0026#39;xxx.vec\u0026#39;, cache=\u0026#39;./\u0026#39;) TEXT.build_vocab(train, val, test, vectors=vectors) data.BucketIterator.splits will returns iterators that loads batches of data from datasets, and the text in same batch has similar lengths.\nNow, we can start to train the model. First we wrap some parameters into args, it contains settings like output class, learning rate, log interval and so on.\nargs={} args[\u0026#39;vocb_size\u0026#39;]=len(TEXT.vocab) args[\u0026#39;dim\u0026#39;]=300 args[\u0026#39;n_class\u0026#39;]=len(LABEL.vocab) args[\u0026#39;embedding_matrix\u0026#39;]=TEXT.vocab.vectors args[\u0026#39;lr\u0026#39;]=0.001 args[\u0026#39;momentum\u0026#39;]=0.8 args[\u0026#39;epochs\u0026#39;]=180 args[\u0026#39;log_interval\u0026#39;]=100 args[\u0026#39;test_interval\u0026#39;]=500 args[\u0026#39;save_dir\u0026#39;]=\u0026#39;./\u0026#39; Finally, we can train the model.\nmodel=textCNNMulti(args) model.cuda() optimizer = torch.optim.SGD(model.parameters(), lr=args[\u0026#39;lr\u0026#39;],momentum=args[\u0026#39;momentum\u0026#39;]) criterion = nn.CrossEntropyLoss() steps=0 for epoch in range(1, args[\u0026#39;epochs\u0026#39;]+1): for i,data in enumerate(train_iter): steps+=1 x, target = data.text, data.label x=x.cuda() target.sub_(1) target=target.cuda() output = model(x) loss = criterion(output, target) optimizer.zero_grad() loss.backward() optimizer.step() You can found textcnn.ipynb on GitHub or Colab.\nRef # Convolutional Neural Networks for Sentence Classiﬁcation Understanding Convolutional Neural Networks for NLP Torchtext Docs Castor ","date":"3 December 2018","permalink":"https://fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/","section":"Posts","summary":"\u003cp\u003e\u003ca href=\"https://pytorch.org\" target=\"_blank\" rel=\"noreferrer\"\u003ePyTorch\u003c/a\u003e is a really powerful framework to build the machine learning models. Although some features is missing when compared with TensorFlow (For example, the early stop function, History to draw plot), its code style is more intuitive.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/pytorch/text\" target=\"_blank\" rel=\"noreferrer\"\u003eTorchtext\u003c/a\u003e is a NLP package which is also made by \u003ccode\u003epytorch\u003c/code\u003e team. It provide a way to read text, processing and iterate the texts.\u003c/p\u003e","title":"TextCNN with PyTorch and Torchtext on Colab"},{"content":"CSRF(Cross-site request forgery) is a way to generate fake user request to target website. For example, on a malicious website A, there is a button, click it will send request to www.B.com/logout. When the user click this button, he will logout from website B unconsciously. Logout is not a big problem, but malicious website can generate more dangerous request like money transfer.\nDjango CSRF protection #Each web framework has different approach to do CSRF protection. In Django, the validation process is below:\nWhen user login for the first time, Django generate a csrf_secret, add random salt and encrypt it as A, save A to cookie csrftoken. When Django processing tag {{ csrf_token }} or {% csrf_token %}, it read csrftoken cookie A, reverse it to csrf_secret, add random salt and encrypt it as B, return corresponding HTML. When Django receive POST request, it will retrive cookie csrftoken as A, and tries to get csrfmiddlewaretoken value B from POST data, if it does not exist, it will get header X-CSRFToken value as B. Then A and B will be reversed to csrf_secret. If the values are identical, the validation is passed. Otherwise, a 403 error will raise. Django CSRF Usage #Form #\u0026lt;form\u0026gt; {% csrf_token %} \u0026lt;/form\u0026gt; Single AJAX request #$.ajax({ data: { csrfmiddlewaretoken: \u0026#39;{{ csrf_token }}\u0026#39; }, Multiple AJAX request #Extracting csrftoken from cookie and add it to header for each ajax request.\nfunction getCookie(name) { var cookieValue = null; if (document.cookie \u0026amp;\u0026amp; document.cookie !== \u0026#39;\u0026#39;) { var cookies = document.cookie.split(\u0026#39;;\u0026#39;); for (var i = 0; i \u0026lt; cookies.length; i++) { var cookie = jQuery.trim(cookies[i]); // Does this cookie string begin with the name we want? if (cookie.substring(0, name.length + 1) === (name + \u0026#39;=\u0026#39;)) { cookieValue = decodeURIComponent(cookie.substring(name.length + 1)); break; } } } return cookieValue; } var csrftoken = getCookie(\u0026#39;csrftoken\u0026#39;); function csrfSafeMethod(method) { // these HTTP methods do not require CSRF protection return (/^(GET|HEAD|OPTIONS|TRACE)$/.test(method)); } $.ajaxSetup({ beforeSend: function(xhr, settings) { if (!csrfSafeMethod(settings.type) \u0026amp;\u0026amp; !this.crossDomain) { xhr.setRequestHeader(\u0026#34;X-CSRFToken\u0026#34;, csrftoken); } } }); Ref # Cross Site Request Forgery protection csrf.py What\u0026rsquo;s the relationship between csrfmiddlewaretoken and csrftoken? ","date":"7 November 2018","permalink":"https://fromkk.com/posts/csrf-in-django/","section":"Posts","summary":"\u003cp\u003eCSRF(Cross-site request forgery) is a way to generate fake user request to target website. For example, on a malicious website A, there is a button, click it will send request to \u003ca href=\"https://www.B.com/logout\" target=\"_blank\" rel=\"noreferrer\"\u003ewww.B.com/logout\u003c/a\u003e. When the user click this button, he will logout from website B unconsciously. Logout is not a big problem, but malicious website can generate more dangerous request like money transfer.\u003c/p\u003e","title":"CSRF in Django"},{"content":"Recently, I\u0026rsquo;m working on a neo4j project. I use Py2neo to interact with graph db. Although Py2neo is a very Pythonic and easy to use, its performance is really poor. Sometimes I have to manually write cypher statement by myself if I can\u0026rsquo;t bear with the slow execution. Here is a small script which I use to compare the performance of 4 different ways to insert nodes.\nimport time from graph_db import graph from py2neo.data import Node, Subgraph def delete_label(label): graph.run(\u0026#39;MATCH (n:{}) DETACH DELETE n\u0026#39;.format(label)) def delete_all(): print(\u0026#39;delete all\u0026#39;) graph.run(\u0026#39;match (n) detach delete n\u0026#39;) def count_label(label): return len(graph.nodes.match(label)) def bench_create1(): print(\u0026#39;Using py2neo one by one\u0026#39;) delete_label(\u0026#39;test\u0026#39;) start = time.time() tx = graph.begin() for i in range(100000): n = Node(\u0026#39;test\u0026#39;, id=i) tx.create(n) tx.commit() print(time.time() - start) print(count_label(\u0026#39;test\u0026#39;)) delete_label(\u0026#39;test\u0026#39;) def bench_create2(): print(\u0026#39;Using cypher one by one\u0026#39;) delete_label(\u0026#39;test\u0026#39;) start = time.time() tx = graph.begin() for i in range(100000): tx.run(\u0026#39;create (n:test {id: $id})\u0026#39;, id=i) if i and i % 1000 == 0: tx.commit() tx = graph.begin() tx.commit() print(time.time() - start) print(count_label(\u0026#39;test\u0026#39;)) delete_label(\u0026#39;test\u0026#39;) def bench_create3(): print(\u0026#39;Using Subgraph\u0026#39;) delete_label(\u0026#39;test\u0026#39;) start = time.time() tx = graph.begin() nodes = [] for i in range(100000): nodes.append(Node(\u0026#39;test\u0026#39;, id=i)) s = Subgraph(nodes=nodes) tx.create(s) tx.commit() print(time.time() - start) print(count_label(\u0026#39;test\u0026#39;)) delete_label(\u0026#39;test\u0026#39;) def bench_create4(): print(\u0026#39;Using unwind\u0026#39;) delete_label(\u0026#39;test\u0026#39;) start = time.time() tx = graph.begin() ids = list(range(100000)) tx.run(\u0026#39;unwind $ids as id create (n:test {id: id})\u0026#39;, ids=ids) tx.commit() print(time.time() - start) print(count_label(\u0026#39;test\u0026#39;)) delete_label(\u0026#39;test\u0026#39;) def bench_create(): create_tests = [bench_create1, bench_create2, bench_create3, bench_create4] print(\u0026#39;testing create\u0026#39;) for i in create_tests: i() if __name__ == \u0026#39;__main__\u0026#39;: bench_create() Apparently, using cypher with unwind keyword is the fastest way to batch insert nodes.\ntesting create Using py2neo one by one 96.09799289703369 100000 Using cypher one by one 9.493892192840576 100000 Using Subgraph 7.638832092285156 100000 Using unwind 2.511630058288574 100000 The above result is based on http protocol. A very interesting result is that, bolt protocol will decrease the time of the first method, but double the time of second method. That\u0026rsquo;s wired, maybe py2neo has some special optimization when doing batch insert on bolt protocol? But I have no idea why insert one by one with cypher is 2x slower. Here is the result of bolt protocol.\ntesting create Using py2neo one by one 51.73185706138611 100000 Using cypher one by one 22.051995992660522 100000 Using Subgraph 8.81674599647522 100000 Using unwind 2.8623900413513184 100000 ","date":"5 November 2018","permalink":"https://fromkk.com/posts/create-node-benchmark-in-py2neo/","section":"Posts","summary":"\u003cp\u003eRecently, I\u0026rsquo;m working on a neo4j project. I use \u003ccode\u003ePy2neo\u003c/code\u003e to interact with graph db. Although \u003ccode\u003ePy2neo\u003c/code\u003e is a very Pythonic and easy to use, its performance is really poor. Sometimes I have to manually write cypher statement by myself if I can\u0026rsquo;t bear with the slow execution. Here is a small script which I use to compare the performance of 4 different ways to insert nodes.\u003c/p\u003e","title":"Create Node Benchmark in Py2neo"},{"content":"Recently, I enjoy using Spacemacs, so I decided to switch to org file from Markdown for writing blog. After several attempts, I managed to let Travis convert org file to HTML. Here are the steps.\nInstall Org Mode plugin #First you need to install Org Mode plugin on your computer following the official guide: Nikola orgmode plugin.\nEdit conf.el #Org Mode will convert to HTML to display on Nikola. Org Mode plugin will call Emacs to do this job. When I run nikola build, it shows this message: Please install htmlize from https://github.com/hniksic/emacs-htmlize. I\u0026rsquo;m using Spacemacs, the htmlize package is already downloaded if the org layer is enabled. I just need to add htmlize folder to load-path. So here is the code:\n(setq dir \u0026#34;~/.emacs.d/elpa/27.0/develop/\u0026#34;) (if(file-directory-p dir) (let ((default-directory dir)) (normal-top-level-add-subdirs-to-load-path))) (require \u0026#39;htmlize) This package is also needed on Travis, the similar approach is required.\nModify .travis.yml #Travis is using ubuntu 14.04, and the default Emacs version is 24, and the Org Mode version is below 8.0, which not match the requirements. The easiest solution is to update Emacs to 25. So in the before_install section, add these code:\n- sudo add-apt-repository ppa:kelleyk/emacs -y - sudo apt-get update In the install section, add these code:\n- sudo apt-get remove emacs - sudo apt autoremove - sudo apt-get install emacs25 The default emacs doesn\u0026rsquo;t contains htmlize package. So add git clone https://github.com/hniksic/emacs-htmlize ~/emacs-htmlize into before_install section.\nFinally, modify conf.el for Travis Emacs, add GitHub repo to load-path: (add-to-list 'load-path \u0026quot;~/emacs-htmlize/\u0026quot;)\nVoila, the org file should show up.\nThe full .travis.yml is below:\nlanguage: python cache: apt sudo: false addons: apt: packages: - language-pack-en-base branches: only: - src python: - 3.6 before_install: - sudo add-apt-repository ppa:kelleyk/emacs -y - sudo apt-get update - openssl aes-256-cbc -K $encrypted_a5c638e4bedc_key -iv $encrypted_a5c638e4bedc_iv -in travis.enc -out travis -d - git config --global user.name \u0026#39;bebound\u0026#39; - git config --global user.email \u0026#39;bebound@gmail.com\u0026#39; - git config --global push.default \u0026#39;simple\u0026#39; - pip install --upgrade pip wheel - echo -e \u0026#39;Host github.com\\n StrictHostKeyChecking no\u0026#39; \u0026gt;\u0026gt; ~/.ssh/config - eval \u0026#34;$(ssh-agent -s)\u0026#34; - chmod 600 travis - ssh-add travis - git remote rm origin - git remote add origin git@github.com:bebound/bebound.github.io - git fetch origin master - git branch master FETCH_HEAD - git clone https://github.com/hniksic/emacs-htmlize ~/emacs-htmlize install: - pip install \u0026#39;Nikola[extras]\u0026#39;==7.8.15 - sudo apt-get remove emacs - sudo apt autoremove - sudo apt-get install emacs25 script: - nikola build \u0026amp;\u0026amp; nikola github_deploy -m \u0026#39;Nikola auto deploy [ci skip]\u0026#39; notifications: email: on_success: change on_failure: always And here is the conf.el:\n(setq dir \u0026#34;~/.emacs.d/elpa/27.0/develop/\u0026#34;) (if(file-directory-p dir) (let ((default-directory dir)) (normal-top-level-add-subdirs-to-load-path))) (add-to-list \u0026#39;load-path \u0026#34;~/emacs-htmlize/\u0026#34;) (require \u0026#39;htmlize) ","date":"3 November 2018","permalink":"https://fromkk.com/posts/deploy-nikola-org-mode-on-travis/","section":"Posts","summary":"\u003cp\u003eRecently, I enjoy using \u003ccode\u003eSpacemacs\u003c/code\u003e, so I decided to switch to org file from Markdown for writing blog. After several attempts, I managed to let Travis convert org file to HTML. Here are the steps.\u003c/p\u003e\n\u003ch2 id=\"install-org-mode-plugin\" class=\"relative group\"\u003eInstall Org Mode plugin \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#install-org-mode-plugin\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eFirst you need to install Org Mode plugin on your computer following the official guide: \u003ca href=\"https://plugins.getnikola.com/v8/orgmode/\" target=\"_blank\" rel=\"noreferrer\"\u003eNikola orgmode plugin\u003c/a\u003e.\u003c/p\u003e","title":"Deploy Nikola Org Mode on Travis"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/nikola/","section":"Tags","summary":"","title":"Nikola"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/matplotlib/","section":"Tags","summary":"","title":"Matplotlib"},{"content":"After searching from Google, here is easiest solution. This should also works on other languages:\nimport matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = \u0026#39;retina\u0026#39; import matplotlib.font_manager as fm f = \u0026#34;/System/Library/Fonts/PingFang.ttc\u0026#34; prop = fm.FontProperties(fname=f) plt.title(\u0026#34;你好\u0026#34;,fontproperties=prop) plt.show() Output:\n","date":"4 October 2018","permalink":"https://fromkk.com/posts/using-chinese-characters-in-matplotlib/","section":"Posts","summary":"\u003cp\u003eAfter searching from Google, here is easiest solution. This should also works on other languages:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ematplotlib.pyplot\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nn\"\u003eplt\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e%\u003c/span\u003e\u003cspan class=\"n\"\u003ematplotlib\u003c/span\u003e \u003cspan class=\"n\"\u003einline\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e%\u003c/span\u003e\u003cspan class=\"n\"\u003econfig\u003c/span\u003e \u003cspan class=\"n\"\u003eInlineBackend\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efigure_format\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;retina\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ematplotlib.font_manager\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nn\"\u003efm\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ef\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;/System/Library/Fonts/PingFang.ttc\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eprop\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003efm\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eFontProperties\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efname\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ef\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etitle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;你好\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003efontproperties\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eprop\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshow\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eOutput:\u003c/p\u003e\n\n  \n  \n  \n  \n  \n\n  \n  \n    \n    \n  \n  \u003cfigure class=\"mx-auto my-0 rounded-md\"\u003e\n    \u003cimg src=\"/images/matplot_chinese.png\" alt=\"\" class=\"mx-auto my-0 rounded-md\"/\u003e\n    \n  \u003c/figure\u003e","title":"Using Chinese Characters in Matplotlib"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/gru/","section":"Tags","summary":"","title":"GRU"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/lstm/","section":"Tags","summary":"","title":"LSTM"},{"content":"LSTM #The avoid the problem of vanishing gradient and exploding gradient in vanilla RNN, LSTM was published, which can remember information for longer periods of time.\nHere is the structure of LSTM:\nThe calculate procedure are:\n\\[\\begin{aligned} f_t\u0026amp;=\\sigma(W_f\\cdot[h_{t-1},x_t]+b_f)\\\\ i_t\u0026amp;=\\sigma(W_i\\cdot[h_{t-1},x_t]+b_i)\\\\ o_t\u0026amp;=\\sigma(W_o\\cdot[h_{t-1},x_t]+b_o)\\\\ \\tilde{C_t}\u0026amp;=tanh(W_C\\cdot[h_{t-1},x_t]+b_C)\\\\ C_t\u0026amp;=f_t\\ast C_{t-1}+i_t\\ast \\tilde{C_t}\\\\ h_t\u0026amp;=o_t \\ast tanh(C_t) \\end{aligned}\\]\n\\(f_t\\),\\(i_t\\),\\(o_t\\) are forget gate, input gate and output gate respectively. \\(\\tilde{C_t}\\) is the new memory content. \\(C_t\\) is cell state. \\(h_t\\) is the output.\nUse \\(f_t\\) and \\(i_t\\) to update \\(C_t\\), use \\(o_t\\) to decide which part of hidden state should be outputted.\nGRU # \\[\\begin{aligned} z_t\u0026amp;=\\sigma(W_z\\cdot[h_{t-1},x_t])\\\\ r_t\u0026amp;=\\sigma(W_r\\cdot[h_{t-1},x_t])\\\\ \\tilde{h_t}\u0026amp;=tanh(W\\cdot[r_t \\ast h_{t-1},x_t])\\\\ h_t\u0026amp;=(1-z_t)\\ast h_{t-1}+z_t \\ast \\tilde{h_t} \\end{aligned}\\]\n\\(z_t\\) is update gate, \\(r_t\\) is reset gate, \\(\\tilde{h_t}\\) is candidate activation, \\(h_t\\) is activation.\nCompare with LSTM, GRU merge cell state and hidden state to one hidden state, and use \\(z_t\\) to decide how to update the state rather than \\(f_t\\) and \\(i_t\\).\nRef # Understanding LSTM Networks ","date":"22 April 2018","permalink":"https://fromkk.com/posts/lstm-and-gru/","section":"Posts","summary":"\u003ch2 id=\"lstm\" class=\"relative group\"\u003eLSTM \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#lstm\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eThe avoid the problem of vanishing gradient and exploding gradient in vanilla RNN, LSTM was published, which can remember information for longer periods of time.\u003c/p\u003e\n\u003cp\u003eHere is the structure of LSTM:\u003c/p\u003e","title":"LSTM and GRU"},{"content":"Generally, word2vec is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.\nModels #CBOW (Continuous Bag of Words) #Use the context to predict the probability of current word. (In the picture, the word is encoded with one-hot encoding, \\(W_{V*N}\\) is word embedding, and \\(W_{V*N}^{\u0026rsquo;}\\), the output weight matrix in hidden layer, is same as \\(\\hat{\\upsilon}\\) in following equations)\nContext words\u0026rsquo; vectors are \\(\\upsilon_{c-n} \u0026hellip; \\upsilon_{c+m}\\) (\\(m\\) is the window size) Context vector \\(\\hat{\\upsilon}=\\frac{\\upsilon_{c-m}+\\upsilon_{c-m+1}+\u0026hellip;+\\upsilon_{c+m}}{2m}\\) Score vector \\(z_i = u_i\\hat{\\upsilon}\\), where \\(u_i\\) is the output vector representation of word \\(\\omega_i\\) Turn scores into probabilities \\(\\hat{y}=softmax(z)\\) We desire probabilities \\(\\hat{y}\\) match the true probabilities \\(y\\). We use cross entropy \\(H(\\hat{y},y)\\) to measure the distance between these two distributions. \\[H(\\hat{y},y)=-\\sum_{j=1}^{\\lvert V \\rvert}{y_j\\log(\\hat{y}_j)}\\]\n\\(y\\) and \\(\\hat{y}\\) is accurate, so the loss simplifies to: \\[H(\\hat{y},y)=-y_j\\log(\\hat{y})\\]\nFor perfect prediction, \\(H(\\hat{y},y)=-1\\log(1)=0\\)\nAccording to this, we can create this loss function:\n\\[\\begin{aligned} minimize\\ J \u0026amp;=-\\log P(\\omega_c\\lvert \\omega_{c-m},\u0026hellip;,\\omega_{c-1},\u0026hellip;,\\omega_{c+m}) \\\\ \u0026amp;= -\\log P(u_c \\lvert \\hat{\\upsilon}) \\\\ \u0026amp;= -\\log \\frac{\\exp(u_c^T\\hat{\\upsilon})}{\\sum_{j=1}^{\\lvert V \\rvert}\\exp (u_j^T\\hat{\\upsilon})} \\\\ \u0026amp;= -u_c^T\\hat{\\upsilon}+\\log \\sum_{j=1}^{\\lvert V \\rvert}\\exp (u_j^T\\hat{\\upsilon}) \\end{aligned}\\]\nSkip-Gram #Use current word to predict its context.\nWe get the input word\u0026rsquo;s vector \\(\\upsilon_c\\) Generate \\(2m\\) score vectors, \\(u_{c-m},\u0026hellip;,u_{c-1},\u0026hellip;,u_{c+m}\\). Turn scores into probabilities \\(\\hat{y}=softmax(u)\\) We desire probabilities \\(\\hat{y}\\) match the true probabilities \\(y\\). \\[\\begin{aligned} minimize J \u0026amp;=-\\log P(\\omega_{c-m},\u0026hellip;,\\omega_{c-1},\\omega_{c+1},\u0026hellip;\\omega_{c+m}\\lvert \\omega_c)\\\\ \u0026amp;=-\\log \\prod_{j=0,j\\ne m}^{2m}P(\\omega_{c-m+j}\\lvert \\omega_c)\\\\ \u0026amp;=-\\log \\prod_{j=0,j\\ne m}^{2m}P(u_{c-m+j}\\lvert \\upsilon_c)\\\\ \u0026amp;=-\\log \\prod_{j=0,j\\ne m}^{2m}\\frac{\\exp (u^T_{c-m+j}\\upsilon_c)}{\\sum_{k=1}^{\\lvert V \\rvert}{\\exp (u^T_k \\upsilon_c)}}\\\\ \u0026amp;=-\\sum_{j=0,j\\ne m}^{2m}{u^T_{c-m+j}\\upsilon_c+2m\\log \\sum_{k=1}^{\\lvert V \\rvert} \\exp(u^T_k \\upsilon_c)} \\end{aligned}\\]\nArchitectures #Minimize \\(J\\) is expensive, you need to calculate the probability of each word in vocabulary list. There are two ways to reduce the computation. Hierarchical Softmax and Negative Sampling.\nHierarchical Softmax #Encode words into a huffman tree, then each word has a Huffman code. The probability of it\u0026rsquo;s probability \\(P(w\\lvert Context(\\omega))\\) can change to choose the path from root to the leaf node, each node is a binary classification. Suppose code \\(0\\) is a positive label, \\(1\\) is negative label. If the probability of a positive classification is \\[\\sigma(X^T_\\omega \\theta)=\\frac{1}{1+e^{-X^T_\\omega}}\\]\nThen the probability of negative classification is \\[1-\\sigma(X^T_\\omega \\theta)\\]\n`足球`'s Huffman code is \\\\(1001\\\\), then it's probability in each node are \\[\\begin{aligned} p(d_2^\\omega\\lvert X_\\omega,\\theta^\\omega_1\u0026amp;=1-\\sigma(X^T_\\omega \\theta^\\omega_1))\\\\ p(d^\\omega_3\\lvert X_\\omega,\\theta^\\omega_2\u0026amp;=\\sigma(X^T_\\omega \\theta^\\omega_2))\\\\ p(d^\\omega_4\\lvert X_\\omega,\\theta^\\omega_3\u0026amp;=\\sigma(X^T_\\omega \\theta^\\omega_3))\\\\ p(d^\\omega_5\\lvert X_\\omega,\\theta^\\omega_4\u0026amp;=1-\\sigma(X^T_\\omega \\theta^\\omega_4))\\\\ \\end{aligned}\\]\nwhere \\(\\theta\\) is parameter in the node.\nThe probability of the 足球 is the production of these equation.\nGenerally,\n\\[p(\\omega\\lvert Context(\\omega))=\\prod_{j=2}^{l\\omega}p(d^\\omega_j\\lvert X_\\omega,\\theta^\\omega_{j-1})\\]\nThis reduce the calculation complexity to \\(log(n)\\) instead of \\(n\\)\nNegative Sampling #This method will choose some negative sample, then add the probability of the negative word into loss function. The optimisation target becomes maximise the positive words\u0026rsquo; probability and minimise the negative words\u0026rsquo; probability.\nLet \\(P(D=0 \\lvert \\omega,c)\\) be the probability that \\((\\omega,c)\\) did not come from the corpus data. Then the objective function will be\n\\[\\theta = \\text{argmax} \\prod_{(\\omega,c)\\in D} P(D=1\\lvert \\omega,c,\\theta) \\prod_{(\\omega,c)\\in \\tilde{D}} P(D=0\\lvert \\omega,c,\\theta)\\]\nwhere \\(\\theta\\) is the parameters of the model(\\(\\upsilon\\) and \\(u\\)).\nupdate 04-04-20 I found this two articles pretty useful: Language Models, Word2Vec, and Efficient Softmax Approximations and Word2vec from Scratch with NumPy.\nRef # word2vec 原理推导与代码分析 CS 224D: Deep Learning for NLP Lecture Notes: Part I word2vec 中的数学原理详解（一）目录和前言 ","date":"5 January 2018","permalink":"https://fromkk.com/posts/models-and-architechtures-in-word2vec/","section":"Posts","summary":"\u003cp\u003eGenerally, \u003ccode\u003eword2vec\u003c/code\u003e is a language model to predict the words probability based on the context. When build the model, it create word embedding for each word, and word embedding is widely used in many NLP tasks.\u003c/p\u003e\n\u003ch2 id=\"models\" class=\"relative group\"\u003eModels \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#models\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003ch3 id=\"cbow--continuous-bag-of-words\" class=\"relative group\"\u003eCBOW (Continuous Bag of Words) \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#cbow--continuous-bag-of-words\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h3\u003e\u003cp\u003eUse the context to predict the probability of current word. (In the picture, the word is encoded with one-hot encoding, \\(W_{V*N}\\) is word embedding, and \\(W_{V*N}^{\u0026rsquo;}\\), the output weight matrix in hidden layer, is same as \\(\\hat{\\upsilon}\\) in following equations)\u003c/p\u003e","title":"Models and Architectures in Word2vec"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/word2vec/","section":"Tags","summary":"","title":"Word2vec"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/doc2vec/","section":"Tags","summary":"","title":"Doc2vec"},{"content":"Here is a simple way to classify text without much human effort and get a impressive performance.\nIt can be divided into two steps:\nGet train data by using keyword classification Generate a more accurate classification model by using doc2vec and label spreading Keyword-based Classification #Keyword based classification is a simple but effective method. Extracting the target keyword is a monotonous work. I use this method to automatic extract keyword candidate.\nFind some most common words to classify the text. Use this equation to calculate the score of each word appears in the text. \\[ score(i) = \\frac{count(i)}{all\\_count(i)^{0.3}}\\] where \\(all\\_count(i)\\) is the word \\(i\\)\u0026rsquo;s word count in all corpus, and \\(count(i)\\) is the word \\(i\\)\u0026rsquo;s word count in positive corpus. Check the top words, add it to the final keyword list. Repeat this process. Finally, we can use the keywords to classify the text and get the train data.\nClassification by doc2vec and Label Spreading #Keyword-based classification sometimes produces the wrong result, as it can\u0026rsquo;t using the semantic information in the text. Fortunately, Google has open sourced word2vec, which can be used to produce semantically meaningful word embeddings. Furthermore, sentences can also be converted to vectors by using doc2vec. Sentences which has closed meaning also have short vector distance.\nSo the problem is how to classify these vectors.\nUsing corpus to train the doc2vec model. Using doc2vec model to convert sentence into vector. Using label spreading algorithm to train a classify model to classify the vectors. ","date":"10 September 2017","permalink":"https://fromkk.com/posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/","section":"Posts","summary":"\u003cp\u003eHere is a simple way to classify text without much human effort and get a impressive performance.\u003c/p\u003e\n\u003cp\u003eIt can be divided into two steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGet train data by using keyword classification\u003c/li\u003e\n\u003cli\u003eGenerate a more accurate classification model by using doc2vec and label spreading\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"keyword-based-classification\" class=\"relative group\"\u003eKeyword-based Classification \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#keyword-based-classification\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eKeyword based classification is a simple but effective method. Extracting the target keyword is a monotonous work. I use this method to automatic extract keyword candidate.\u003c/p\u003e","title":"Semi-supervised text classification using doc2vec and label spreading"},{"content":"Here are some parameter in gensim\u0026rsquo;s doc2vec class.\nwindow #window is the maximum distance between the predicted word and context words used for prediction within a document. It will look behind and ahead.\nIn skip-gram model, if the window size is 2, the training samples will be this:(the blue word is the input word)\nmin_count #If the word appears less than this value, it will be skipped\nsample #High frequency word like the is useless for training. sample is a threshold for deleting these higher-frequency words. The probability of keeping the word \\(w_i\\) is:\n\\[P(w_i) = (\\sqrt{\\frac{z(\\omega_i)}{s}} + 1) \\cdot \\frac{s}{z(\\omega_i)}\\]\nwhere \\(z(w_i)\\) is the frequency of the word and \\(s\\) is the sample rate.\nThis is the plot when sample is 1e-3.\nnegative #Usually, when training a neural network, for each training sample, all of the weights in the neural network need to be tweaked. For example, if the word pair is (\u0026lsquo;fox\u0026rsquo;, \u0026lsquo;quick\u0026rsquo;), then only the word quick\u0026rsquo;s neurons should output 1, and all of the other word neurons should output 0.\nBut it would takes a lot of time to do this when we have billions of training samples. So, instead of update all of the weight, we random choose a small number of \u0026ldquo;negative\u0026rdquo; words (default value is 5) to update the weight.(Update their wight to output 0).\nSo when dealing with word pair (\u0026lsquo;fox\u0026rsquo;,\u0026lsquo;quick\u0026rsquo;), we update quick\u0026rsquo;s weight to output 1, and other 5 random words\u0026rsquo; weight to output 1.\nThe probability of selecting word \\(\\omega_i\\) is \\(P(\\omega_i)\\):\n\\[P(\\omega_i)=\\frac{{f(\\omega_i)}^{{3}/{4}}}{\\sum_{j=0}^{n}\\left({f(\\omega_j)}^{{3}/{4}}\\right)}\\]\n\\(f(\\omega_j)\\) is the frequency of word \\(\\omega_j\\).\nRef # Word2Vec Tutorial - The Skip-Gram Model Word2Vec Tutorial Part 2 - Negative Sampling ","date":"3 August 2017","permalink":"https://fromkk.com/posts/parameters-in-dov2vec/","section":"Posts","summary":"\u003cp\u003eHere are some parameter in \u003ccode\u003egensim\u003c/code\u003e\u0026rsquo;s \u003ccode\u003edoc2vec\u003c/code\u003e class.\u003c/p\u003e\n\u003ch3 id=\"window\" class=\"relative group\"\u003ewindow \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#window\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h3\u003e\u003cp\u003ewindow is the maximum distance between the predicted word and context words used for prediction within a document. It will look behind and ahead.\u003c/p\u003e","title":"Parameters in doc2vec"},{"content":"As I said before, I\u0026rsquo;m working on a text classification project. I use doc2vec to convert text into vectors, then I use LPA to classify the vectors.\nLPA is a simple, effective semi-supervised algorithm. It can use the density of unlabeled data to find a hyperplane to split the data.\nHere are the main stop of the algorithm:\nLet $ (x_1,y1)\u0026hellip;(x_l,y_l)$ be labeled data, $Y_L = \\{y_1\u0026hellip;y_l\\} $ are the class labels. Let \\((x_{l+1},y_{l+u})\\) be unlabeled data where \\(Y_U = \\{y_{l+1}\u0026hellip;y_{l+u}\\}\\) are unobserved, usually \\(l \\ll u\\). Let \\(X=\\{x_1\u0026hellip;x_{l+u}\\}\\) where \\(x_i\\in R^D\\). The problem is to estimate \\(Y_U\\) for \\(X\\) and \\(Y_L\\). Calculate the similarity of the data points. The most simple metric is Euclidean distance. Use a parameter \\(\\sigma\\) to control the weights. \\[w_{ij}= exp(-\\frac{d^2_{ij}}{\\sigma^2})=exp(-\\frac{\\sum^D_{d=1}{(x^d_i-x^d_j})^2}{\\sigma^2})\\]\nLarger weight allow labels to travel through easier.\nDefine a \\((l+u)*(l+u)\\) probabilistic transition matrix \\(T\\) \\[T_{ij}=P(j \\rightarrow i)=\\frac{w_{ij}}{\\sum^{l+u}_{k=1}w_{kj}}\\]\n\\(T_{ij}\\) is the probability to jump from node \\(j\\) to \\(i\\). If there are \\(C\\) classes, we can define a \\((l+u)*C\\) label matrix \\(Y\\), to represent the probability of a label belong to class \\(c\\). The initialization of unlabeled data points is not important.\nPropagate \\(Y \\leftarrow TY\\) Row-normalize Y. Reset labeled data\u0026rsquo;s Y. Repeat 3 until Y converges. In short, let the nearest label has larger weight, then calculate each label\u0026rsquo;s new label, reset labeled data\u0026rsquo;s label, repeat.\nRef # Learning from Labeled and Unlabeled Data with Label Propagation 标签传播算法（Label Propagation）及Python实现 ","date":"16 July 2017","permalink":"https://fromkk.com/posts/brief-introduction-of-label-propagation-algorithm/","section":"Posts","summary":"\u003cp\u003eAs I said before, I\u0026rsquo;m working on a text classification project. I use \u003ccode\u003edoc2vec\u003c/code\u003e to convert text into vectors, then I use LPA to classify the vectors.\u003c/p\u003e\n\u003cp\u003eLPA is a simple, effective semi-supervised algorithm. It can use the density of unlabeled data to find a hyperplane to split the data.\u003c/p\u003e","title":"Brief Introduction of Label Propagation Algorithm"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/label-propagation/","section":"Tags","summary":"","title":"Label Propagation"},{"content":"These days, I’m working on some text classification works, and I use gensim ’s doc2vec function.\nWhen using gensim, it shows this warning message:\nC extension not loaded for Word2Vec, training will be slow.\nI search this on Internet and found that gensim has rewrite some part of the code using cython rather than numpy to get better performance. A compiler is required to enable this feature.\nI tried to install mingw and add it into the path, but it\u0026rsquo;s not working.\nFinally, I tried to install Visual C++ Build Tools and it works.\nIf this output is not -1, then it\u0026rsquo;s fine.\nfrom gensim.models import word2vec print(word2vec.FAST_VERSION) ","date":"10 June 2017","permalink":"https://fromkk.com/posts/enable-c-extension-for-gensim-on-windows/","section":"Posts","summary":"\u003cp\u003eThese days, I’m working on some text classification works, and I use \u003ccode\u003egensim\u003c/code\u003e ’s \u003ccode\u003edoc2vec\u003c/code\u003e function.\u003c/p\u003e\n\u003cp\u003eWhen using gensim, it shows this warning message:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eC extension not loaded for Word2Vec, training will be slow.\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eI search this on Internet and found that gensim has rewrite some part of the code using \u003ccode\u003ecython\u003c/code\u003e rather than \u003ccode\u003enumpy\u003c/code\u003e to get better performance. A compiler is required to enable this feature.\u003c/p\u003e","title":"Enable C Extension for gensim on Windows"},{"content":"","date":null,"permalink":"https://fromkk.com/tags/shell/","section":"Tags","summary":"","title":"Shell"},{"content":"Here are some shell tools I use, which can boost your productivity. Mordern-unix is a great repo that list lots of modern unix tools.\nPrezto #A zsh configuration framework. Provides auto completion, prompt theme and lots of modules to work with other useful tools. I extremely love the agnoster theme.\nFasd #Help you to navigate between folders and launch application.\nHere are the official usage example:\nv def conf =\u0026gt; vim /some/awkward/path/to/type/default.conf j abc =\u0026gt; cd /hell/of/a/awkward/path/to/get/to/abcdef m movie =\u0026gt; mplayer /whatever/whatever/whatever/awesome_movie.mp4 o eng paper =\u0026gt; xdg-open /you/dont/remember/where/english_paper.pdf vim `f rc lo` =\u0026gt; vim /etc/rc.local vim `f rc conf` =\u0026gt; vim /etc/rc.conf pt #A fast code search tool similar to ack.\nfzf #A great fuzzy finder, it can also integrate with vim by fzf.vim\nthefuck #Magnificent app which corrects your previous console command.\ntldr #More concise and user-friendly man pages. (This screenshot uses powerlevel10k theme)\nripgrep #Another search tool. Use rg -. to include hidden files.\nfd #A user-friendly alternative to find. Ignore hidden files and gitignore file by default.\nFor example: fd -H 'flac$' search all files ends with flac.\nbat #Similar to cat with syntax highlighting and git integration.\nZim #A fast Zsh framework. You can use OMZ plugin like this:\nexport ZSH_CACHE_DIR=~/.cache zmodule ohmyzsh/ohmyzsh --use degit --source \u0026#39;plugins/fasd/fasd.plugin.zsh\u0026#39; update 18/11/19 Add tldr powerlevel10k theme is a fancy ZSH theme\nupdate 29/12/21 Add rg, bat, fd\nupdate 06/01/22 Add Zim\nupdate 01/04/24 Add maintained-modern-unix repo\n","date":"7 May 2017","permalink":"https://fromkk.com/posts/some-useful-shell-tools/","section":"Posts","summary":"\u003cp\u003eHere are some shell tools I use, which can boost your productivity. \u003ca href=\"https://github.com/johnalanwoods/maintained-modern-unix\" target=\"_blank\" rel=\"noreferrer\"\u003eMordern-unix\u003c/a\u003e is a great repo that list lots of modern unix tools.\u003c/p\u003e\n\u003ch2 id=\"prezto\" class=\"relative group\"\u003e\u003ca href=\"https://github.com/sorin-ionescu/prezto\" target=\"_blank\" rel=\"noreferrer\"\u003ePrezto\u003c/a\u003e \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#prezto\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003eA zsh configuration framework. Provides auto completion, prompt theme and lots of modules to work with other useful tools. I extremely love the \u003ccode\u003eagnoster\u003c/code\u003e theme.\u003c/p\u003e","title":"Some Useful Shell Tools"},{"content":"Over the years, I have read so many programmers’ blogs, which has helped me a lot. Now I think it’s the time to start my own blog.\nI hope this can enforce myself to review what I have learned, and it would even be better if someone can benefit from it.\n","date":"18 April 2017","permalink":"https://fromkk.com/posts/start/","section":"Posts","summary":"\u003cp\u003eOver the years, I have read so many programmers’ blogs, which has helped me a lot. Now I think it’s the time to start my own blog.\u003c/p\u003e\n\u003cp\u003eI hope this can enforce myself to review what I have learned, and it would even be better if someone can benefit from it.\u003c/p\u003e","title":"Start"},{"content":"","date":null,"permalink":"https://fromkk.com/categories/","section":"Categories","summary":"","title":"Categories"}]